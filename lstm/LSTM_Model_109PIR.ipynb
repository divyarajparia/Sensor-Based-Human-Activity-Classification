{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Libraries/Modules required"
      ],
      "metadata": {
        "id": "GdWt_ZwDu_6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uV4XKyhhWx4V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#from IPython.display import display\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Concatenate\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "#from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading dataset"
      ],
      "metadata": {
        "id": "X3nVTuYpvQ6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the files (you’ll get a file picker)\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "4dwbyynJW6aa",
        "outputId": "9a3f3203-adec-492f-ef75-6835cb01b033"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c3a61178-f040-45b8-b652-1c3c8f4a2acc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c3a61178-f040-45b8-b652-1c3c8f4a2acc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pirvision_office_dataset2.csv to pirvision_office_dataset2.csv\n",
            "Saving pirvision_office_dataset1.csv to pirvision_office_dataset1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We first read the 2 data files\n",
        "df1 = pd.read_csv('pirvision_office_dataset1.csv')\n",
        "df2 = pd.read_csv('pirvision_office_dataset2.csv')"
      ],
      "metadata": {
        "id": "V4V1Cli-W9aC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We now merge these 2 dataframes\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "#We print the shapes of all datafmrames\n",
        "print(df1.shape, df2.shape, df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd4M2E31W_kV",
        "outputId": "2e2e05ff-ae40-432b-fbe2-e62a497018b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7651, 59) (7651, 59) (15302, 59)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis & Data Pre-Processing"
      ],
      "metadata": {
        "id": "7JmUjqTjvUxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "Cfp0H6FQ1yn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primary anlysis based on original dataset\n",
        "\n",
        "def Data_Analysis(df):\n",
        "    # Display the first few rows of the dataset\n",
        "    print(\"First few rows of the dataset:\")\n",
        "    display(df.head())\n",
        "    # Dataset overview\n",
        "    print(\"Dataset Overview:\")\n",
        "    print(f\"Number of samples: {df.shape[0]}\")\n",
        "    print(f\"Number of features: {df.shape[1]}\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(df.info())\n",
        "\n",
        "    # Label distribution visualization\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.countplot(x='Label', data=df, palette='viridis', legend = False)\n",
        "    plt.title('Label Distribution')\n",
        "    plt.xlabel('Label')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n",
        "\n",
        "    # Display class distribution details\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    label_counts = df['Label'].value_counts().sort_index()\n",
        "    for label, count in label_counts.items():\n",
        "        print(f\"Label {label}: {count} samples ({(count / len(df)) * 100:.2f}%)\")\n",
        "\n",
        "    # Correlation matrix for PIR sensors\n",
        "    pir_columns = [col for col in df.columns if col.startswith('PIR_')]\n",
        "\n",
        "    if pir_columns:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        correlation_matrix = df[pir_columns].corr()\n",
        "        sns.heatmap(correlation_matrix, cmap='coolwarm', center=0)\n",
        "        plt.title('Correlation Matrix of PIR Sensors')\n",
        "        plt.show()\n",
        "\n",
        "    #Correlation matrix for Temperature and Label\n",
        "    print('\\nPearson Correlation matrix between Temperature and Label:\\n')\n",
        "    corr = df[['Temperature_F', 'Label']].corr(method='pearson')\n",
        "    sns.heatmap(corr, annot=True)\n",
        "    plt.show()\n",
        "\n",
        "# Load and analyze the dataset\n",
        "Data_Analysis(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bjVX2TRKj1aI",
        "outputId": "7e07c81e-bf4c-45c3-f9d7-89107ee417a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         Date      Time  Label  Temperature_F  PIR_1  PIR_2  PIR_3  PIR_4  \\\n",
              "0  2024-08-08  19:19:56      0             86  10269  10721  11156  11170   \n",
              "1  2024-08-08  19:20:12      1             86  10364  10907  11299  11238   \n",
              "2  2024-08-08  19:20:28      0             86  10329  10793  11197  11242   \n",
              "3  2024-08-08  19:20:44      0             86  10169  10425  10822  11133   \n",
              "4  2024-08-08  19:21:00      0             86  10320  10667  11104  11234   \n",
              "\n",
              "   PIR_5  PIR_6  PIR_7  PIR_8  PIR_9  PIR_10  PIR_11  PIR_12  PIR_13  PIR_14  \\\n",
              "0  10931  10671  10395  10133   9885    9705    9538    9418    9469    9599   \n",
              "1  10867  10535  10173   9950   9856    9795    9714    9702    9792    9789   \n",
              "2  11052  10658  10288   9988   9819    9711    9659    9626    9726    9752   \n",
              "3  11136  10834  10520  10228   9986    9848    9643    9562    9591    9618   \n",
              "4  11129  10814  10453  10040   9733    9630    9578    9476    9596    9748   \n",
              "\n",
              "   PIR_15  PIR_16  PIR_17  PIR_18  PIR_19  PIR_20  PIR_21  PIR_22  PIR_23  \\\n",
              "0    9817    9910    9890   10075   10231   10247   10271   10229   10272   \n",
              "1    9915    9900    9944    9964    9971   10059   10161   10234   10285   \n",
              "2    9835    9942    9925    9965   10110   10174   10140   10235   10303   \n",
              "3    9718    9849    9857   10026   10150   10198   10261   10351   10425   \n",
              "4    9755    9823   10004   10048   10202   10234   10255   10282   10298   \n",
              "\n",
              "   PIR_24  PIR_25  PIR_26  PIR_27  PIR_28  PIR_29  PIR_30  PIR_31  PIR_32  \\\n",
              "0   10354   10449   10451   10419   10409   10336   10306   10356   10461   \n",
              "1   10309   10384   10464   10450   10427   10366   10361   10452   10502   \n",
              "2   10365   10366   10379   10375   10287   10310   10345   10373   10328   \n",
              "3   10469   10374   10344   10303   10293   10294   10333   10353   10345   \n",
              "4   10319   10315   10270   10334   10400   10428   10514   10529   10453   \n",
              "\n",
              "   PIR_33  PIR_34  PIR_35  PIR_36  PIR_37  PIR_38  PIR_39  PIR_40  PIR_41  \\\n",
              "0   10456   10460   10467   10422   10303    9877    9308    9061    9299   \n",
              "1   10444   10337   10250   10313   10211    9718    9236    9193    9609   \n",
              "2   10387   10415   10491   10421   10432    9964    9368    9135    9287   \n",
              "3   10354   10362   10375   10369   10319   10115    9603    9182    9125   \n",
              "4   10374   10303   10298   10238   10246    9918    9399    9198    9422   \n",
              "\n",
              "   PIR_42  PIR_43  PIR_44  PIR_45  PIR_46  PIR_47  PIR_48  PIR_49  PIR_50  \\\n",
              "0    9748   10209   10615   10975   11178   11197   11161   11096   10957   \n",
              "1   10022   10431   10798   11055   11122   11145   11136   11108   11041   \n",
              "2    9643   10184   10663   11016   11168   11204   11162   11109   11007   \n",
              "3    9560   10161   10560   10883   11116   11273   11186   10984   10910   \n",
              "4    9848   10225   10615   10860   11006   11257   11370   11173   10924   \n",
              "\n",
              "   PIR_51  PIR_52  PIR_53  PIR_54  PIR_55  \n",
              "0   10839   10735   10590   10411   10329  \n",
              "1   10824   10645   10493   10398   10357  \n",
              "2   10867   10700   10533   10427   10265  \n",
              "3   10807   10714   10651   10562   10463  \n",
              "4   10816   10754   10588   10428   10407  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5b63288b-694f-4977-a575-dde0ec2ae505\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>Label</th>\n",
              "      <th>Temperature_F</th>\n",
              "      <th>PIR_1</th>\n",
              "      <th>PIR_2</th>\n",
              "      <th>PIR_3</th>\n",
              "      <th>PIR_4</th>\n",
              "      <th>PIR_5</th>\n",
              "      <th>PIR_6</th>\n",
              "      <th>PIR_7</th>\n",
              "      <th>PIR_8</th>\n",
              "      <th>PIR_9</th>\n",
              "      <th>PIR_10</th>\n",
              "      <th>PIR_11</th>\n",
              "      <th>PIR_12</th>\n",
              "      <th>PIR_13</th>\n",
              "      <th>PIR_14</th>\n",
              "      <th>PIR_15</th>\n",
              "      <th>PIR_16</th>\n",
              "      <th>PIR_17</th>\n",
              "      <th>PIR_18</th>\n",
              "      <th>PIR_19</th>\n",
              "      <th>PIR_20</th>\n",
              "      <th>PIR_21</th>\n",
              "      <th>PIR_22</th>\n",
              "      <th>PIR_23</th>\n",
              "      <th>PIR_24</th>\n",
              "      <th>PIR_25</th>\n",
              "      <th>PIR_26</th>\n",
              "      <th>PIR_27</th>\n",
              "      <th>PIR_28</th>\n",
              "      <th>PIR_29</th>\n",
              "      <th>PIR_30</th>\n",
              "      <th>PIR_31</th>\n",
              "      <th>PIR_32</th>\n",
              "      <th>PIR_33</th>\n",
              "      <th>PIR_34</th>\n",
              "      <th>PIR_35</th>\n",
              "      <th>PIR_36</th>\n",
              "      <th>PIR_37</th>\n",
              "      <th>PIR_38</th>\n",
              "      <th>PIR_39</th>\n",
              "      <th>PIR_40</th>\n",
              "      <th>PIR_41</th>\n",
              "      <th>PIR_42</th>\n",
              "      <th>PIR_43</th>\n",
              "      <th>PIR_44</th>\n",
              "      <th>PIR_45</th>\n",
              "      <th>PIR_46</th>\n",
              "      <th>PIR_47</th>\n",
              "      <th>PIR_48</th>\n",
              "      <th>PIR_49</th>\n",
              "      <th>PIR_50</th>\n",
              "      <th>PIR_51</th>\n",
              "      <th>PIR_52</th>\n",
              "      <th>PIR_53</th>\n",
              "      <th>PIR_54</th>\n",
              "      <th>PIR_55</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-08-08</td>\n",
              "      <td>19:19:56</td>\n",
              "      <td>0</td>\n",
              "      <td>86</td>\n",
              "      <td>10269</td>\n",
              "      <td>10721</td>\n",
              "      <td>11156</td>\n",
              "      <td>11170</td>\n",
              "      <td>10931</td>\n",
              "      <td>10671</td>\n",
              "      <td>10395</td>\n",
              "      <td>10133</td>\n",
              "      <td>9885</td>\n",
              "      <td>9705</td>\n",
              "      <td>9538</td>\n",
              "      <td>9418</td>\n",
              "      <td>9469</td>\n",
              "      <td>9599</td>\n",
              "      <td>9817</td>\n",
              "      <td>9910</td>\n",
              "      <td>9890</td>\n",
              "      <td>10075</td>\n",
              "      <td>10231</td>\n",
              "      <td>10247</td>\n",
              "      <td>10271</td>\n",
              "      <td>10229</td>\n",
              "      <td>10272</td>\n",
              "      <td>10354</td>\n",
              "      <td>10449</td>\n",
              "      <td>10451</td>\n",
              "      <td>10419</td>\n",
              "      <td>10409</td>\n",
              "      <td>10336</td>\n",
              "      <td>10306</td>\n",
              "      <td>10356</td>\n",
              "      <td>10461</td>\n",
              "      <td>10456</td>\n",
              "      <td>10460</td>\n",
              "      <td>10467</td>\n",
              "      <td>10422</td>\n",
              "      <td>10303</td>\n",
              "      <td>9877</td>\n",
              "      <td>9308</td>\n",
              "      <td>9061</td>\n",
              "      <td>9299</td>\n",
              "      <td>9748</td>\n",
              "      <td>10209</td>\n",
              "      <td>10615</td>\n",
              "      <td>10975</td>\n",
              "      <td>11178</td>\n",
              "      <td>11197</td>\n",
              "      <td>11161</td>\n",
              "      <td>11096</td>\n",
              "      <td>10957</td>\n",
              "      <td>10839</td>\n",
              "      <td>10735</td>\n",
              "      <td>10590</td>\n",
              "      <td>10411</td>\n",
              "      <td>10329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-08-08</td>\n",
              "      <td>19:20:12</td>\n",
              "      <td>1</td>\n",
              "      <td>86</td>\n",
              "      <td>10364</td>\n",
              "      <td>10907</td>\n",
              "      <td>11299</td>\n",
              "      <td>11238</td>\n",
              "      <td>10867</td>\n",
              "      <td>10535</td>\n",
              "      <td>10173</td>\n",
              "      <td>9950</td>\n",
              "      <td>9856</td>\n",
              "      <td>9795</td>\n",
              "      <td>9714</td>\n",
              "      <td>9702</td>\n",
              "      <td>9792</td>\n",
              "      <td>9789</td>\n",
              "      <td>9915</td>\n",
              "      <td>9900</td>\n",
              "      <td>9944</td>\n",
              "      <td>9964</td>\n",
              "      <td>9971</td>\n",
              "      <td>10059</td>\n",
              "      <td>10161</td>\n",
              "      <td>10234</td>\n",
              "      <td>10285</td>\n",
              "      <td>10309</td>\n",
              "      <td>10384</td>\n",
              "      <td>10464</td>\n",
              "      <td>10450</td>\n",
              "      <td>10427</td>\n",
              "      <td>10366</td>\n",
              "      <td>10361</td>\n",
              "      <td>10452</td>\n",
              "      <td>10502</td>\n",
              "      <td>10444</td>\n",
              "      <td>10337</td>\n",
              "      <td>10250</td>\n",
              "      <td>10313</td>\n",
              "      <td>10211</td>\n",
              "      <td>9718</td>\n",
              "      <td>9236</td>\n",
              "      <td>9193</td>\n",
              "      <td>9609</td>\n",
              "      <td>10022</td>\n",
              "      <td>10431</td>\n",
              "      <td>10798</td>\n",
              "      <td>11055</td>\n",
              "      <td>11122</td>\n",
              "      <td>11145</td>\n",
              "      <td>11136</td>\n",
              "      <td>11108</td>\n",
              "      <td>11041</td>\n",
              "      <td>10824</td>\n",
              "      <td>10645</td>\n",
              "      <td>10493</td>\n",
              "      <td>10398</td>\n",
              "      <td>10357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-08-08</td>\n",
              "      <td>19:20:28</td>\n",
              "      <td>0</td>\n",
              "      <td>86</td>\n",
              "      <td>10329</td>\n",
              "      <td>10793</td>\n",
              "      <td>11197</td>\n",
              "      <td>11242</td>\n",
              "      <td>11052</td>\n",
              "      <td>10658</td>\n",
              "      <td>10288</td>\n",
              "      <td>9988</td>\n",
              "      <td>9819</td>\n",
              "      <td>9711</td>\n",
              "      <td>9659</td>\n",
              "      <td>9626</td>\n",
              "      <td>9726</td>\n",
              "      <td>9752</td>\n",
              "      <td>9835</td>\n",
              "      <td>9942</td>\n",
              "      <td>9925</td>\n",
              "      <td>9965</td>\n",
              "      <td>10110</td>\n",
              "      <td>10174</td>\n",
              "      <td>10140</td>\n",
              "      <td>10235</td>\n",
              "      <td>10303</td>\n",
              "      <td>10365</td>\n",
              "      <td>10366</td>\n",
              "      <td>10379</td>\n",
              "      <td>10375</td>\n",
              "      <td>10287</td>\n",
              "      <td>10310</td>\n",
              "      <td>10345</td>\n",
              "      <td>10373</td>\n",
              "      <td>10328</td>\n",
              "      <td>10387</td>\n",
              "      <td>10415</td>\n",
              "      <td>10491</td>\n",
              "      <td>10421</td>\n",
              "      <td>10432</td>\n",
              "      <td>9964</td>\n",
              "      <td>9368</td>\n",
              "      <td>9135</td>\n",
              "      <td>9287</td>\n",
              "      <td>9643</td>\n",
              "      <td>10184</td>\n",
              "      <td>10663</td>\n",
              "      <td>11016</td>\n",
              "      <td>11168</td>\n",
              "      <td>11204</td>\n",
              "      <td>11162</td>\n",
              "      <td>11109</td>\n",
              "      <td>11007</td>\n",
              "      <td>10867</td>\n",
              "      <td>10700</td>\n",
              "      <td>10533</td>\n",
              "      <td>10427</td>\n",
              "      <td>10265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-08-08</td>\n",
              "      <td>19:20:44</td>\n",
              "      <td>0</td>\n",
              "      <td>86</td>\n",
              "      <td>10169</td>\n",
              "      <td>10425</td>\n",
              "      <td>10822</td>\n",
              "      <td>11133</td>\n",
              "      <td>11136</td>\n",
              "      <td>10834</td>\n",
              "      <td>10520</td>\n",
              "      <td>10228</td>\n",
              "      <td>9986</td>\n",
              "      <td>9848</td>\n",
              "      <td>9643</td>\n",
              "      <td>9562</td>\n",
              "      <td>9591</td>\n",
              "      <td>9618</td>\n",
              "      <td>9718</td>\n",
              "      <td>9849</td>\n",
              "      <td>9857</td>\n",
              "      <td>10026</td>\n",
              "      <td>10150</td>\n",
              "      <td>10198</td>\n",
              "      <td>10261</td>\n",
              "      <td>10351</td>\n",
              "      <td>10425</td>\n",
              "      <td>10469</td>\n",
              "      <td>10374</td>\n",
              "      <td>10344</td>\n",
              "      <td>10303</td>\n",
              "      <td>10293</td>\n",
              "      <td>10294</td>\n",
              "      <td>10333</td>\n",
              "      <td>10353</td>\n",
              "      <td>10345</td>\n",
              "      <td>10354</td>\n",
              "      <td>10362</td>\n",
              "      <td>10375</td>\n",
              "      <td>10369</td>\n",
              "      <td>10319</td>\n",
              "      <td>10115</td>\n",
              "      <td>9603</td>\n",
              "      <td>9182</td>\n",
              "      <td>9125</td>\n",
              "      <td>9560</td>\n",
              "      <td>10161</td>\n",
              "      <td>10560</td>\n",
              "      <td>10883</td>\n",
              "      <td>11116</td>\n",
              "      <td>11273</td>\n",
              "      <td>11186</td>\n",
              "      <td>10984</td>\n",
              "      <td>10910</td>\n",
              "      <td>10807</td>\n",
              "      <td>10714</td>\n",
              "      <td>10651</td>\n",
              "      <td>10562</td>\n",
              "      <td>10463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024-08-08</td>\n",
              "      <td>19:21:00</td>\n",
              "      <td>0</td>\n",
              "      <td>86</td>\n",
              "      <td>10320</td>\n",
              "      <td>10667</td>\n",
              "      <td>11104</td>\n",
              "      <td>11234</td>\n",
              "      <td>11129</td>\n",
              "      <td>10814</td>\n",
              "      <td>10453</td>\n",
              "      <td>10040</td>\n",
              "      <td>9733</td>\n",
              "      <td>9630</td>\n",
              "      <td>9578</td>\n",
              "      <td>9476</td>\n",
              "      <td>9596</td>\n",
              "      <td>9748</td>\n",
              "      <td>9755</td>\n",
              "      <td>9823</td>\n",
              "      <td>10004</td>\n",
              "      <td>10048</td>\n",
              "      <td>10202</td>\n",
              "      <td>10234</td>\n",
              "      <td>10255</td>\n",
              "      <td>10282</td>\n",
              "      <td>10298</td>\n",
              "      <td>10319</td>\n",
              "      <td>10315</td>\n",
              "      <td>10270</td>\n",
              "      <td>10334</td>\n",
              "      <td>10400</td>\n",
              "      <td>10428</td>\n",
              "      <td>10514</td>\n",
              "      <td>10529</td>\n",
              "      <td>10453</td>\n",
              "      <td>10374</td>\n",
              "      <td>10303</td>\n",
              "      <td>10298</td>\n",
              "      <td>10238</td>\n",
              "      <td>10246</td>\n",
              "      <td>9918</td>\n",
              "      <td>9399</td>\n",
              "      <td>9198</td>\n",
              "      <td>9422</td>\n",
              "      <td>9848</td>\n",
              "      <td>10225</td>\n",
              "      <td>10615</td>\n",
              "      <td>10860</td>\n",
              "      <td>11006</td>\n",
              "      <td>11257</td>\n",
              "      <td>11370</td>\n",
              "      <td>11173</td>\n",
              "      <td>10924</td>\n",
              "      <td>10816</td>\n",
              "      <td>10754</td>\n",
              "      <td>10588</td>\n",
              "      <td>10428</td>\n",
              "      <td>10407</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b63288b-694f-4977-a575-dde0ec2ae505')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5b63288b-694f-4977-a575-dde0ec2ae505 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5b63288b-694f-4977-a575-dde0ec2ae505');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dc00c310-a09b-424e-a172-823c5530fedc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dc00c310-a09b-424e-a172-823c5530fedc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dc00c310-a09b-424e-a172-823c5530fedc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Overview:\n",
            "Number of samples: 15302\n",
            "Number of features: 59\n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15302 entries, 0 to 15301\n",
            "Data columns (total 59 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Date           15302 non-null  object\n",
            " 1   Time           15302 non-null  object\n",
            " 2   Label          15302 non-null  int64 \n",
            " 3   Temperature_F  15302 non-null  int64 \n",
            " 4   PIR_1          15302 non-null  int64 \n",
            " 5   PIR_2          15302 non-null  int64 \n",
            " 6   PIR_3          15302 non-null  int64 \n",
            " 7   PIR_4          15302 non-null  int64 \n",
            " 8   PIR_5          15302 non-null  int64 \n",
            " 9   PIR_6          15302 non-null  int64 \n",
            " 10  PIR_7          15302 non-null  int64 \n",
            " 11  PIR_8          15302 non-null  int64 \n",
            " 12  PIR_9          15302 non-null  int64 \n",
            " 13  PIR_10         15302 non-null  int64 \n",
            " 14  PIR_11         15302 non-null  int64 \n",
            " 15  PIR_12         15302 non-null  int64 \n",
            " 16  PIR_13         15302 non-null  int64 \n",
            " 17  PIR_14         15302 non-null  int64 \n",
            " 18  PIR_15         15302 non-null  int64 \n",
            " 19  PIR_16         15302 non-null  int64 \n",
            " 20  PIR_17         15302 non-null  int64 \n",
            " 21  PIR_18         15302 non-null  int64 \n",
            " 22  PIR_19         15302 non-null  int64 \n",
            " 23  PIR_20         15302 non-null  int64 \n",
            " 24  PIR_21         15302 non-null  int64 \n",
            " 25  PIR_22         15302 non-null  int64 \n",
            " 26  PIR_23         15302 non-null  int64 \n",
            " 27  PIR_24         15302 non-null  int64 \n",
            " 28  PIR_25         15302 non-null  int64 \n",
            " 29  PIR_26         15302 non-null  int64 \n",
            " 30  PIR_27         15302 non-null  int64 \n",
            " 31  PIR_28         15302 non-null  int64 \n",
            " 32  PIR_29         15302 non-null  int64 \n",
            " 33  PIR_30         15302 non-null  int64 \n",
            " 34  PIR_31         15302 non-null  int64 \n",
            " 35  PIR_32         15302 non-null  int64 \n",
            " 36  PIR_33         15302 non-null  int64 \n",
            " 37  PIR_34         15302 non-null  int64 \n",
            " 38  PIR_35         15302 non-null  int64 \n",
            " 39  PIR_36         15302 non-null  int64 \n",
            " 40  PIR_37         15302 non-null  int64 \n",
            " 41  PIR_38         15302 non-null  int64 \n",
            " 42  PIR_39         15302 non-null  int64 \n",
            " 43  PIR_40         15302 non-null  int64 \n",
            " 44  PIR_41         15302 non-null  int64 \n",
            " 45  PIR_42         15302 non-null  int64 \n",
            " 46  PIR_43         15302 non-null  int64 \n",
            " 47  PIR_44         15302 non-null  int64 \n",
            " 48  PIR_45         15302 non-null  int64 \n",
            " 49  PIR_46         15302 non-null  int64 \n",
            " 50  PIR_47         15302 non-null  int64 \n",
            " 51  PIR_48         15302 non-null  int64 \n",
            " 52  PIR_49         15302 non-null  int64 \n",
            " 53  PIR_50         15302 non-null  int64 \n",
            " 54  PIR_51         15302 non-null  int64 \n",
            " 55  PIR_52         15302 non-null  int64 \n",
            " 56  PIR_53         15302 non-null  int64 \n",
            " 57  PIR_54         15302 non-null  int64 \n",
            " 58  PIR_55         15302 non-null  int64 \n",
            "dtypes: int64(57), object(2)\n",
            "memory usage: 6.9+ MB\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-a3faba68860f>:17: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.countplot(x='Label', data=df, palette='viridis', legend = False)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class Distribution:\n",
            "Label 0: 12494 samples (81.65%)\n",
            "Label 1: 1666 samples (10.89%)\n",
            "Label 3: 1142 samples (7.46%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pearson Correlation matrix between Temperature and Label:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "NIHP-ZiJ17ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering - PART 1\n",
        "\n",
        "# Ensure 'Date' and 'Time' are parsed correctly\n",
        "df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
        "\n",
        "# Extract temporal features\n",
        "df['Hour'] = df['Datetime'].dt.hour\n",
        "df['Minute'] = df['Datetime'].dt.minute\n",
        "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
        "df['Month'] = df['Datetime'].dt.month\n",
        "\n",
        "df = df.drop(columns=['Date', 'Time', 'Datetime'])\n",
        "\n",
        "#Question - Do we consider cyclic nature of minute, hour??\n",
        "\n",
        "meta_features = ['Hour', 'Minute', 'DayOfWeek', 'Month', 'Temperature_F']"
      ],
      "metadata": {
        "id": "uSxCfUDNypW8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering - PART 2\n",
        "\n",
        "# Interpolating pairwise PIR sensor values to generate more features per time stamp\n",
        "x = np.zeros((df.shape[0], 55+54+6))\n",
        "\n",
        "for i in range(df.shape[0]):\n",
        "  for j in range (0, 2):\n",
        "    x[i,j] = df.iloc[i,j]\n",
        "  for j in range(2, 55+54+2):\n",
        "    if(j%2 == 0):\n",
        "      x[i,j] = df.iloc[i,j//2]\n",
        "    else:\n",
        "      x[i,j] = (df.iloc[i,j//2] + df.iloc[i,j//2+1])/2\n",
        "  for j in range(55+54+2, 55+54+6):\n",
        "    x[i, j] = df.iloc[i, j-54]\n",
        "\n",
        "x_df = pd.DataFrame(x)\n",
        "\n",
        "x_df.columns = ['Label', 'Temperature_F'] + [f'PIR_{i}' for i in range(109)] + ['Hour', 'Minute', 'DayOfWeek', 'Month']\n",
        "\n",
        "df = x_df"
      ],
      "metadata": {
        "id": "dCMTUiiZytcj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Normalisation/Standardisation"
      ],
      "metadata": {
        "id": "Nl-Gk65f2BtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing - Data standardisation\n",
        "scaler = StandardScaler()\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "numeric_cols = numeric_cols.drop(\"Label\")  # Exclude the Label column\n",
        "\n",
        "# Question - saw somewhere that for meta columsn(fixed range values) MInMax scaler is much better than StandardScaler?\n",
        "\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])"
      ],
      "metadata": {
        "id": "lgzp_xDGyvQC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()   #Final preprocessed dataset"
      ],
      "metadata": {
        "id": "2VOjKemTywl_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "0911afac-44d9-4a5d-89c4-469547707268"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Label  Temperature_F     PIR_0     PIR_1     PIR_2     PIR_3     PIR_4  \\\n",
              "0    0.0       0.245676  0.245676 -0.055874 -0.055875 -0.055944 -0.494950   \n",
              "1    1.0       0.245676  0.245676 -0.055852 -0.055853 -0.055878 -0.180633   \n",
              "2    0.0       0.245676  0.245676 -0.055860 -0.055861 -0.055913 -0.373279   \n",
              "3    0.0       0.245676  0.245676 -0.055897 -0.055899 -0.056036 -0.995152   \n",
              "4    0.0       0.245676  0.245676 -0.055862 -0.055863 -0.055944 -0.586203   \n",
              "\n",
              "      PIR_5     PIR_6     PIR_7     PIR_8     PIR_9    PIR_10    PIR_11  \\\n",
              "0 -0.535025 -0.551045 -0.591841 -0.616061 -0.595164 -0.558023 -0.411659   \n",
              "1 -0.266177 -0.333270 -0.429731 -0.513508 -0.591975 -0.663652 -0.589812   \n",
              "2 -0.442685 -0.488606 -0.505024 -0.507475 -0.441310 -0.358319 -0.315456   \n",
              "3 -1.049842 -1.059694 -0.876879 -0.671862 -0.461239 -0.219681 -0.083858   \n",
              "4 -0.621645 -0.630236 -0.582622 -0.519540 -0.386305 -0.231234 -0.107908   \n",
              "\n",
              "     PIR_12    PIR_13    PIR_14    PIR_15    PIR_16    PIR_17    PIR_18  \\\n",
              "0 -0.230890 -0.040599  0.165129  0.314171  0.450680  0.498168  0.525723   \n",
              "1 -0.484048 -0.392987 -0.277654 -0.099009  0.084387  0.286769  0.470167   \n",
              "2 -0.255089 -0.158718 -0.048284  0.057081  0.160448  0.287766  0.399285   \n",
              "3  0.072528  0.242886  0.414444  0.538615  0.640832  0.693612  0.719211   \n",
              "4  0.035299  0.157250  0.280811  0.278464  0.264531  0.253863  0.234533   \n",
              "\n",
              "     PIR_19    PIR_20    PIR_21    PIR_22    PIR_23    PIR_24    PIR_25  \\\n",
              "0  0.528330  0.515734  0.447967  0.370196  0.279563  0.184010  0.194594   \n",
              "1  0.586064  0.679292  0.689126  0.679981  0.685867  0.676146  0.731050   \n",
              "2  0.471542  0.526638  0.563107  0.583173  0.570159  0.544447  0.605553   \n",
              "3  0.759267  0.775610  0.672807  0.555011  0.499497  0.433544  0.429681   \n",
              "4  0.313482  0.379435  0.416236  0.440602  0.366124  0.284517  0.358094   \n",
              "\n",
              "     PIR_26    PIR_27    PIR_28    PIR_29    PIR_30    PIR_31    PIR_32  \\\n",
              "0  0.201863  0.243975  0.280221  0.381752  0.477645  0.453421  0.416393   \n",
              "1  0.773828  0.707510  0.621484  0.646532  0.658225  0.535910  0.397845   \n",
              "2  0.656956  0.614442  0.555027  0.538965  0.510813  0.500290  0.475745   \n",
              "3  0.417899  0.371379  0.314347  0.308202  0.295222  0.303442  0.303253   \n",
              "4  0.426753  0.493362  0.547843  0.461737  0.363400  0.313753  0.255029   \n",
              "\n",
              "     PIR_33    PIR_34    PIR_35    PIR_36    PIR_37    PIR_38    PIR_39  \\\n",
              "0  0.281833  0.137928  0.199226  0.257101  0.307502  0.352263  0.280735   \n",
              "1  0.323367  0.238492  0.144941  0.045904 -0.056683 -0.161902 -0.176954   \n",
              "2  0.345078  0.203108  0.127798  0.047807  0.080746  0.112979  0.082539   \n",
              "3  0.193100  0.076472  0.121131  0.163870  0.179890  0.192081  0.147924   \n",
              "4  0.307320  0.350229  0.282082  0.205729  0.252531  0.294914  0.237827   \n",
              "\n",
              "     PIR_40    PIR_41    PIR_42    PIR_43    PIR_44    PIR_45    PIR_46  \\\n",
              "0  0.197236  0.140089  0.078255 -0.035004 -0.148685 -0.162059 -0.170861   \n",
              "1 -0.186373 -0.169357 -0.146818 -0.144674 -0.138325 -0.143058 -0.143695   \n",
              "2  0.048281 -0.071746 -0.189786 -0.165564 -0.136253 -0.123001 -0.106080   \n",
              "3  0.097252  0.078823  0.057794  0.081978  0.104101  0.128234  0.148866   \n",
              "4  0.170709  0.109975  0.045517  0.003642 -0.038868 -0.078666 -0.116529   \n",
              "\n",
              "     PIR_47    PIR_48    PIR_49    PIR_50    PIR_51    PIR_52    PIR_53  \\\n",
              "0 -0.131657 -0.087877 -0.019885  0.050418  0.032361  0.013493 -0.033058   \n",
              "1 -0.165733 -0.182316 -0.138881 -0.089082 -0.023935  0.040904  0.013455   \n",
              "2 -0.086932 -0.064792 -0.097773 -0.127713 -0.135444 -0.138317 -0.155684   \n",
              "3  0.153729  0.153465  0.023387 -0.110544 -0.164674 -0.212114 -0.268795   \n",
              "4 -0.141241 -0.161329 -0.202707 -0.237168 -0.308661 -0.368142 -0.314251   \n",
              "\n",
              "     PIR_54    PIR_55    PIR_56    PIR_57    PIR_58    PIR_59    PIR_60  \\\n",
              "0 -0.077806 -0.092583 -0.103976 -0.173862 -0.238827 -0.266700 -0.288882   \n",
              "1 -0.013779 -0.041444 -0.067306 -0.124524 -0.178165 -0.179000 -0.175699   \n",
              "2 -0.168683 -0.265832 -0.352515 -0.325986 -0.291400 -0.253287 -0.208625   \n",
              "3 -0.317390 -0.334714 -0.340292 -0.336264 -0.323754 -0.282177 -0.233319   \n",
              "4 -0.253363 -0.190688 -0.122311 -0.088549 -0.052796  0.042830  0.139156   \n",
              "\n",
              "     PIR_61    PIR_62    PIR_63    PIR_64    PIR_65    PIR_66    PIR_67  \\\n",
              "0 -0.233162 -0.170143 -0.050141  0.069416  0.075027  0.078371  0.093931   \n",
              "1 -0.074587  0.029915  0.093617  0.153468  0.104805  0.054383 -0.042301   \n",
              "2 -0.174353 -0.134716 -0.171863 -0.203240 -0.132395 -0.059561 -0.021109   \n",
              "3 -0.207958 -0.176395 -0.175011 -0.168389 -0.148824 -0.125529 -0.107894   \n",
              "4  0.166951  0.190379  0.122998  0.053016 -0.017389 -0.085548 -0.147250   \n",
              "\n",
              "     PIR_68    PIR_69    PIR_70    PIR_71    PIR_72    PIR_73    PIR_74  \\\n",
              "0  0.107238  0.127637  0.145402  0.116187  0.082729  0.108872  0.129407   \n",
              "1 -0.137586 -0.217401 -0.293455 -0.223551 -0.145031 -0.104631 -0.059855   \n",
              "2  0.017668  0.106326  0.193939  0.140156  0.080640  0.244834  0.394786   \n",
              "3 -0.087825 -0.065179 -0.040657 -0.034924 -0.028016  0.069571  0.162322   \n",
              "4 -0.205261 -0.203194 -0.196380 -0.251689 -0.301747 -0.147119  0.012147   \n",
              "\n",
              "     PIR_75    PIR_76    PIR_77    PIR_78    PIR_79    PIR_80    PIR_81  \\\n",
              "0  0.287074  0.399382  0.472744  0.512783  0.502623  0.475682  0.477421   \n",
              "1  0.043706  0.127043  0.285027  0.406865  0.546212  0.658589  0.799215   \n",
              "2  0.496506  0.548397  0.592200  0.601048  0.599971  0.578221  0.522559   \n",
              "3  0.533351  0.807033  0.905874  0.946753  0.804837  0.643346  0.438834   \n",
              "4  0.271560  0.469608  0.580011  0.646652  0.668259  0.665517  0.666712   \n",
              "\n",
              "     PIR_82    PIR_83    PIR_84    PIR_85    PIR_86    PIR_87    PIR_88  \\\n",
              "0  0.467011  0.412986  0.333056  0.177566 -0.012290 -0.189309 -0.350064   \n",
              "1  0.930333  0.896097  0.821685  0.664868  0.440365  0.232227  0.014222   \n",
              "2  0.449076  0.316198  0.145808  0.049845 -0.063265 -0.165370 -0.254513   \n",
              "3  0.206953  0.113523 -0.002207 -0.054296 -0.110161 -0.296514 -0.459548   \n",
              "4  0.650845  0.597461  0.511388  0.291532  0.020334 -0.172656 -0.350064   \n",
              "\n",
              "     PIR_89    PIR_90    PIR_91    PIR_92    PIR_93    PIR_94    PIR_95  \\\n",
              "0 -0.398756 -0.426881 -0.457676 -0.472273 -0.531099 -0.577291 -0.599922   \n",
              "1 -0.144127 -0.282181 -0.436510 -0.565744 -0.619964 -0.660100 -0.662097   \n",
              "2 -0.312589 -0.352722 -0.430337 -0.488964 -0.533568 -0.566144 -0.593462   \n",
              "3 -0.541077 -0.593285 -0.593491 -0.575759 -0.519580 -0.456263 -0.518367   \n",
              "4 -0.510096 -0.634887 -0.710786 -0.759362 -0.623256 -0.481743 -0.382711   \n",
              "\n",
              "     PIR_96    PIR_97    PIR_98    PIR_99   PIR_100   PIR_101   PIR_102  \\\n",
              "0 -0.614463 -0.599804 -0.574078 -0.578531 -0.572173 -0.534485 -0.483428   \n",
              "1 -0.654840 -0.610573 -0.554029 -0.495710 -0.425187 -0.472090 -0.510887   \n",
              "2 -0.612848 -0.588206 -0.552358 -0.524180 -0.484681 -0.463952 -0.432172   \n",
              "3 -0.574085 -0.671875 -0.761205 -0.715704 -0.654415 -0.605921 -0.542007   \n",
              "4 -0.276907 -0.362880 -0.445428 -0.540571 -0.629918 -0.585123 -0.525531   \n",
              "\n",
              "    PIR_103   PIR_104   PIR_105   PIR_106   PIR_107   PIR_108      Hour  \\\n",
              "0 -0.424869 -0.355541 -0.338631 -0.314113 -0.347892 -0.376149  0.954478   \n",
              "1 -0.524366 -0.529183 -0.526364 -0.513220 -0.464623 -0.404123  0.954478   \n",
              "2 -0.431502 -0.423068 -0.430992 -0.431114 -0.391401 -0.341720  0.954478   \n",
              "3 -0.475091 -0.396057 -0.298474 -0.188902 -0.122920 -0.051221  0.954478   \n",
              "4 -0.428659 -0.318883 -0.321564 -0.318218 -0.331974 -0.339568  0.954478   \n",
              "\n",
              "     Minute  DayOfWeek     Month  \n",
              "0 -0.613620  -1.023796 -1.887176  \n",
              "1 -0.555507  -1.023796 -1.887176  \n",
              "2 -0.555507  -1.023796 -1.887176  \n",
              "3 -0.555507  -1.023796 -1.887176  \n",
              "4 -0.497395  -1.023796 -1.887176  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29e6cb15-b399-42f5-b146-d2da89243a10\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Temperature_F</th>\n",
              "      <th>PIR_0</th>\n",
              "      <th>PIR_1</th>\n",
              "      <th>PIR_2</th>\n",
              "      <th>PIR_3</th>\n",
              "      <th>PIR_4</th>\n",
              "      <th>PIR_5</th>\n",
              "      <th>PIR_6</th>\n",
              "      <th>PIR_7</th>\n",
              "      <th>PIR_8</th>\n",
              "      <th>PIR_9</th>\n",
              "      <th>PIR_10</th>\n",
              "      <th>PIR_11</th>\n",
              "      <th>PIR_12</th>\n",
              "      <th>PIR_13</th>\n",
              "      <th>PIR_14</th>\n",
              "      <th>PIR_15</th>\n",
              "      <th>PIR_16</th>\n",
              "      <th>PIR_17</th>\n",
              "      <th>PIR_18</th>\n",
              "      <th>PIR_19</th>\n",
              "      <th>PIR_20</th>\n",
              "      <th>PIR_21</th>\n",
              "      <th>PIR_22</th>\n",
              "      <th>PIR_23</th>\n",
              "      <th>PIR_24</th>\n",
              "      <th>PIR_25</th>\n",
              "      <th>PIR_26</th>\n",
              "      <th>PIR_27</th>\n",
              "      <th>PIR_28</th>\n",
              "      <th>PIR_29</th>\n",
              "      <th>PIR_30</th>\n",
              "      <th>PIR_31</th>\n",
              "      <th>PIR_32</th>\n",
              "      <th>PIR_33</th>\n",
              "      <th>PIR_34</th>\n",
              "      <th>PIR_35</th>\n",
              "      <th>PIR_36</th>\n",
              "      <th>PIR_37</th>\n",
              "      <th>PIR_38</th>\n",
              "      <th>PIR_39</th>\n",
              "      <th>PIR_40</th>\n",
              "      <th>PIR_41</th>\n",
              "      <th>PIR_42</th>\n",
              "      <th>PIR_43</th>\n",
              "      <th>PIR_44</th>\n",
              "      <th>PIR_45</th>\n",
              "      <th>PIR_46</th>\n",
              "      <th>PIR_47</th>\n",
              "      <th>PIR_48</th>\n",
              "      <th>PIR_49</th>\n",
              "      <th>PIR_50</th>\n",
              "      <th>PIR_51</th>\n",
              "      <th>PIR_52</th>\n",
              "      <th>PIR_53</th>\n",
              "      <th>PIR_54</th>\n",
              "      <th>PIR_55</th>\n",
              "      <th>PIR_56</th>\n",
              "      <th>PIR_57</th>\n",
              "      <th>PIR_58</th>\n",
              "      <th>PIR_59</th>\n",
              "      <th>PIR_60</th>\n",
              "      <th>PIR_61</th>\n",
              "      <th>PIR_62</th>\n",
              "      <th>PIR_63</th>\n",
              "      <th>PIR_64</th>\n",
              "      <th>PIR_65</th>\n",
              "      <th>PIR_66</th>\n",
              "      <th>PIR_67</th>\n",
              "      <th>PIR_68</th>\n",
              "      <th>PIR_69</th>\n",
              "      <th>PIR_70</th>\n",
              "      <th>PIR_71</th>\n",
              "      <th>PIR_72</th>\n",
              "      <th>PIR_73</th>\n",
              "      <th>PIR_74</th>\n",
              "      <th>PIR_75</th>\n",
              "      <th>PIR_76</th>\n",
              "      <th>PIR_77</th>\n",
              "      <th>PIR_78</th>\n",
              "      <th>PIR_79</th>\n",
              "      <th>PIR_80</th>\n",
              "      <th>PIR_81</th>\n",
              "      <th>PIR_82</th>\n",
              "      <th>PIR_83</th>\n",
              "      <th>PIR_84</th>\n",
              "      <th>PIR_85</th>\n",
              "      <th>PIR_86</th>\n",
              "      <th>PIR_87</th>\n",
              "      <th>PIR_88</th>\n",
              "      <th>PIR_89</th>\n",
              "      <th>PIR_90</th>\n",
              "      <th>PIR_91</th>\n",
              "      <th>PIR_92</th>\n",
              "      <th>PIR_93</th>\n",
              "      <th>PIR_94</th>\n",
              "      <th>PIR_95</th>\n",
              "      <th>PIR_96</th>\n",
              "      <th>PIR_97</th>\n",
              "      <th>PIR_98</th>\n",
              "      <th>PIR_99</th>\n",
              "      <th>PIR_100</th>\n",
              "      <th>PIR_101</th>\n",
              "      <th>PIR_102</th>\n",
              "      <th>PIR_103</th>\n",
              "      <th>PIR_104</th>\n",
              "      <th>PIR_105</th>\n",
              "      <th>PIR_106</th>\n",
              "      <th>PIR_107</th>\n",
              "      <th>PIR_108</th>\n",
              "      <th>Hour</th>\n",
              "      <th>Minute</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055874</td>\n",
              "      <td>-0.055875</td>\n",
              "      <td>-0.055944</td>\n",
              "      <td>-0.494950</td>\n",
              "      <td>-0.535025</td>\n",
              "      <td>-0.551045</td>\n",
              "      <td>-0.591841</td>\n",
              "      <td>-0.616061</td>\n",
              "      <td>-0.595164</td>\n",
              "      <td>-0.558023</td>\n",
              "      <td>-0.411659</td>\n",
              "      <td>-0.230890</td>\n",
              "      <td>-0.040599</td>\n",
              "      <td>0.165129</td>\n",
              "      <td>0.314171</td>\n",
              "      <td>0.450680</td>\n",
              "      <td>0.498168</td>\n",
              "      <td>0.525723</td>\n",
              "      <td>0.528330</td>\n",
              "      <td>0.515734</td>\n",
              "      <td>0.447967</td>\n",
              "      <td>0.370196</td>\n",
              "      <td>0.279563</td>\n",
              "      <td>0.184010</td>\n",
              "      <td>0.194594</td>\n",
              "      <td>0.201863</td>\n",
              "      <td>0.243975</td>\n",
              "      <td>0.280221</td>\n",
              "      <td>0.381752</td>\n",
              "      <td>0.477645</td>\n",
              "      <td>0.453421</td>\n",
              "      <td>0.416393</td>\n",
              "      <td>0.281833</td>\n",
              "      <td>0.137928</td>\n",
              "      <td>0.199226</td>\n",
              "      <td>0.257101</td>\n",
              "      <td>0.307502</td>\n",
              "      <td>0.352263</td>\n",
              "      <td>0.280735</td>\n",
              "      <td>0.197236</td>\n",
              "      <td>0.140089</td>\n",
              "      <td>0.078255</td>\n",
              "      <td>-0.035004</td>\n",
              "      <td>-0.148685</td>\n",
              "      <td>-0.162059</td>\n",
              "      <td>-0.170861</td>\n",
              "      <td>-0.131657</td>\n",
              "      <td>-0.087877</td>\n",
              "      <td>-0.019885</td>\n",
              "      <td>0.050418</td>\n",
              "      <td>0.032361</td>\n",
              "      <td>0.013493</td>\n",
              "      <td>-0.033058</td>\n",
              "      <td>-0.077806</td>\n",
              "      <td>-0.092583</td>\n",
              "      <td>-0.103976</td>\n",
              "      <td>-0.173862</td>\n",
              "      <td>-0.238827</td>\n",
              "      <td>-0.266700</td>\n",
              "      <td>-0.288882</td>\n",
              "      <td>-0.233162</td>\n",
              "      <td>-0.170143</td>\n",
              "      <td>-0.050141</td>\n",
              "      <td>0.069416</td>\n",
              "      <td>0.075027</td>\n",
              "      <td>0.078371</td>\n",
              "      <td>0.093931</td>\n",
              "      <td>0.107238</td>\n",
              "      <td>0.127637</td>\n",
              "      <td>0.145402</td>\n",
              "      <td>0.116187</td>\n",
              "      <td>0.082729</td>\n",
              "      <td>0.108872</td>\n",
              "      <td>0.129407</td>\n",
              "      <td>0.287074</td>\n",
              "      <td>0.399382</td>\n",
              "      <td>0.472744</td>\n",
              "      <td>0.512783</td>\n",
              "      <td>0.502623</td>\n",
              "      <td>0.475682</td>\n",
              "      <td>0.477421</td>\n",
              "      <td>0.467011</td>\n",
              "      <td>0.412986</td>\n",
              "      <td>0.333056</td>\n",
              "      <td>0.177566</td>\n",
              "      <td>-0.012290</td>\n",
              "      <td>-0.189309</td>\n",
              "      <td>-0.350064</td>\n",
              "      <td>-0.398756</td>\n",
              "      <td>-0.426881</td>\n",
              "      <td>-0.457676</td>\n",
              "      <td>-0.472273</td>\n",
              "      <td>-0.531099</td>\n",
              "      <td>-0.577291</td>\n",
              "      <td>-0.599922</td>\n",
              "      <td>-0.614463</td>\n",
              "      <td>-0.599804</td>\n",
              "      <td>-0.574078</td>\n",
              "      <td>-0.578531</td>\n",
              "      <td>-0.572173</td>\n",
              "      <td>-0.534485</td>\n",
              "      <td>-0.483428</td>\n",
              "      <td>-0.424869</td>\n",
              "      <td>-0.355541</td>\n",
              "      <td>-0.338631</td>\n",
              "      <td>-0.314113</td>\n",
              "      <td>-0.347892</td>\n",
              "      <td>-0.376149</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.613620</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055852</td>\n",
              "      <td>-0.055853</td>\n",
              "      <td>-0.055878</td>\n",
              "      <td>-0.180633</td>\n",
              "      <td>-0.266177</td>\n",
              "      <td>-0.333270</td>\n",
              "      <td>-0.429731</td>\n",
              "      <td>-0.513508</td>\n",
              "      <td>-0.591975</td>\n",
              "      <td>-0.663652</td>\n",
              "      <td>-0.589812</td>\n",
              "      <td>-0.484048</td>\n",
              "      <td>-0.392987</td>\n",
              "      <td>-0.277654</td>\n",
              "      <td>-0.099009</td>\n",
              "      <td>0.084387</td>\n",
              "      <td>0.286769</td>\n",
              "      <td>0.470167</td>\n",
              "      <td>0.586064</td>\n",
              "      <td>0.679292</td>\n",
              "      <td>0.689126</td>\n",
              "      <td>0.679981</td>\n",
              "      <td>0.685867</td>\n",
              "      <td>0.676146</td>\n",
              "      <td>0.731050</td>\n",
              "      <td>0.773828</td>\n",
              "      <td>0.707510</td>\n",
              "      <td>0.621484</td>\n",
              "      <td>0.646532</td>\n",
              "      <td>0.658225</td>\n",
              "      <td>0.535910</td>\n",
              "      <td>0.397845</td>\n",
              "      <td>0.323367</td>\n",
              "      <td>0.238492</td>\n",
              "      <td>0.144941</td>\n",
              "      <td>0.045904</td>\n",
              "      <td>-0.056683</td>\n",
              "      <td>-0.161902</td>\n",
              "      <td>-0.176954</td>\n",
              "      <td>-0.186373</td>\n",
              "      <td>-0.169357</td>\n",
              "      <td>-0.146818</td>\n",
              "      <td>-0.144674</td>\n",
              "      <td>-0.138325</td>\n",
              "      <td>-0.143058</td>\n",
              "      <td>-0.143695</td>\n",
              "      <td>-0.165733</td>\n",
              "      <td>-0.182316</td>\n",
              "      <td>-0.138881</td>\n",
              "      <td>-0.089082</td>\n",
              "      <td>-0.023935</td>\n",
              "      <td>0.040904</td>\n",
              "      <td>0.013455</td>\n",
              "      <td>-0.013779</td>\n",
              "      <td>-0.041444</td>\n",
              "      <td>-0.067306</td>\n",
              "      <td>-0.124524</td>\n",
              "      <td>-0.178165</td>\n",
              "      <td>-0.179000</td>\n",
              "      <td>-0.175699</td>\n",
              "      <td>-0.074587</td>\n",
              "      <td>0.029915</td>\n",
              "      <td>0.093617</td>\n",
              "      <td>0.153468</td>\n",
              "      <td>0.104805</td>\n",
              "      <td>0.054383</td>\n",
              "      <td>-0.042301</td>\n",
              "      <td>-0.137586</td>\n",
              "      <td>-0.217401</td>\n",
              "      <td>-0.293455</td>\n",
              "      <td>-0.223551</td>\n",
              "      <td>-0.145031</td>\n",
              "      <td>-0.104631</td>\n",
              "      <td>-0.059855</td>\n",
              "      <td>0.043706</td>\n",
              "      <td>0.127043</td>\n",
              "      <td>0.285027</td>\n",
              "      <td>0.406865</td>\n",
              "      <td>0.546212</td>\n",
              "      <td>0.658589</td>\n",
              "      <td>0.799215</td>\n",
              "      <td>0.930333</td>\n",
              "      <td>0.896097</td>\n",
              "      <td>0.821685</td>\n",
              "      <td>0.664868</td>\n",
              "      <td>0.440365</td>\n",
              "      <td>0.232227</td>\n",
              "      <td>0.014222</td>\n",
              "      <td>-0.144127</td>\n",
              "      <td>-0.282181</td>\n",
              "      <td>-0.436510</td>\n",
              "      <td>-0.565744</td>\n",
              "      <td>-0.619964</td>\n",
              "      <td>-0.660100</td>\n",
              "      <td>-0.662097</td>\n",
              "      <td>-0.654840</td>\n",
              "      <td>-0.610573</td>\n",
              "      <td>-0.554029</td>\n",
              "      <td>-0.495710</td>\n",
              "      <td>-0.425187</td>\n",
              "      <td>-0.472090</td>\n",
              "      <td>-0.510887</td>\n",
              "      <td>-0.524366</td>\n",
              "      <td>-0.529183</td>\n",
              "      <td>-0.526364</td>\n",
              "      <td>-0.513220</td>\n",
              "      <td>-0.464623</td>\n",
              "      <td>-0.404123</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.555507</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055860</td>\n",
              "      <td>-0.055861</td>\n",
              "      <td>-0.055913</td>\n",
              "      <td>-0.373279</td>\n",
              "      <td>-0.442685</td>\n",
              "      <td>-0.488606</td>\n",
              "      <td>-0.505024</td>\n",
              "      <td>-0.507475</td>\n",
              "      <td>-0.441310</td>\n",
              "      <td>-0.358319</td>\n",
              "      <td>-0.315456</td>\n",
              "      <td>-0.255089</td>\n",
              "      <td>-0.158718</td>\n",
              "      <td>-0.048284</td>\n",
              "      <td>0.057081</td>\n",
              "      <td>0.160448</td>\n",
              "      <td>0.287766</td>\n",
              "      <td>0.399285</td>\n",
              "      <td>0.471542</td>\n",
              "      <td>0.526638</td>\n",
              "      <td>0.563107</td>\n",
              "      <td>0.583173</td>\n",
              "      <td>0.570159</td>\n",
              "      <td>0.544447</td>\n",
              "      <td>0.605553</td>\n",
              "      <td>0.656956</td>\n",
              "      <td>0.614442</td>\n",
              "      <td>0.555027</td>\n",
              "      <td>0.538965</td>\n",
              "      <td>0.510813</td>\n",
              "      <td>0.500290</td>\n",
              "      <td>0.475745</td>\n",
              "      <td>0.345078</td>\n",
              "      <td>0.203108</td>\n",
              "      <td>0.127798</td>\n",
              "      <td>0.047807</td>\n",
              "      <td>0.080746</td>\n",
              "      <td>0.112979</td>\n",
              "      <td>0.082539</td>\n",
              "      <td>0.048281</td>\n",
              "      <td>-0.071746</td>\n",
              "      <td>-0.189786</td>\n",
              "      <td>-0.165564</td>\n",
              "      <td>-0.136253</td>\n",
              "      <td>-0.123001</td>\n",
              "      <td>-0.106080</td>\n",
              "      <td>-0.086932</td>\n",
              "      <td>-0.064792</td>\n",
              "      <td>-0.097773</td>\n",
              "      <td>-0.127713</td>\n",
              "      <td>-0.135444</td>\n",
              "      <td>-0.138317</td>\n",
              "      <td>-0.155684</td>\n",
              "      <td>-0.168683</td>\n",
              "      <td>-0.265832</td>\n",
              "      <td>-0.352515</td>\n",
              "      <td>-0.325986</td>\n",
              "      <td>-0.291400</td>\n",
              "      <td>-0.253287</td>\n",
              "      <td>-0.208625</td>\n",
              "      <td>-0.174353</td>\n",
              "      <td>-0.134716</td>\n",
              "      <td>-0.171863</td>\n",
              "      <td>-0.203240</td>\n",
              "      <td>-0.132395</td>\n",
              "      <td>-0.059561</td>\n",
              "      <td>-0.021109</td>\n",
              "      <td>0.017668</td>\n",
              "      <td>0.106326</td>\n",
              "      <td>0.193939</td>\n",
              "      <td>0.140156</td>\n",
              "      <td>0.080640</td>\n",
              "      <td>0.244834</td>\n",
              "      <td>0.394786</td>\n",
              "      <td>0.496506</td>\n",
              "      <td>0.548397</td>\n",
              "      <td>0.592200</td>\n",
              "      <td>0.601048</td>\n",
              "      <td>0.599971</td>\n",
              "      <td>0.578221</td>\n",
              "      <td>0.522559</td>\n",
              "      <td>0.449076</td>\n",
              "      <td>0.316198</td>\n",
              "      <td>0.145808</td>\n",
              "      <td>0.049845</td>\n",
              "      <td>-0.063265</td>\n",
              "      <td>-0.165370</td>\n",
              "      <td>-0.254513</td>\n",
              "      <td>-0.312589</td>\n",
              "      <td>-0.352722</td>\n",
              "      <td>-0.430337</td>\n",
              "      <td>-0.488964</td>\n",
              "      <td>-0.533568</td>\n",
              "      <td>-0.566144</td>\n",
              "      <td>-0.593462</td>\n",
              "      <td>-0.612848</td>\n",
              "      <td>-0.588206</td>\n",
              "      <td>-0.552358</td>\n",
              "      <td>-0.524180</td>\n",
              "      <td>-0.484681</td>\n",
              "      <td>-0.463952</td>\n",
              "      <td>-0.432172</td>\n",
              "      <td>-0.431502</td>\n",
              "      <td>-0.423068</td>\n",
              "      <td>-0.430992</td>\n",
              "      <td>-0.431114</td>\n",
              "      <td>-0.391401</td>\n",
              "      <td>-0.341720</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.555507</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055897</td>\n",
              "      <td>-0.055899</td>\n",
              "      <td>-0.056036</td>\n",
              "      <td>-0.995152</td>\n",
              "      <td>-1.049842</td>\n",
              "      <td>-1.059694</td>\n",
              "      <td>-0.876879</td>\n",
              "      <td>-0.671862</td>\n",
              "      <td>-0.461239</td>\n",
              "      <td>-0.219681</td>\n",
              "      <td>-0.083858</td>\n",
              "      <td>0.072528</td>\n",
              "      <td>0.242886</td>\n",
              "      <td>0.414444</td>\n",
              "      <td>0.538615</td>\n",
              "      <td>0.640832</td>\n",
              "      <td>0.693612</td>\n",
              "      <td>0.719211</td>\n",
              "      <td>0.759267</td>\n",
              "      <td>0.775610</td>\n",
              "      <td>0.672807</td>\n",
              "      <td>0.555011</td>\n",
              "      <td>0.499497</td>\n",
              "      <td>0.433544</td>\n",
              "      <td>0.429681</td>\n",
              "      <td>0.417899</td>\n",
              "      <td>0.371379</td>\n",
              "      <td>0.314347</td>\n",
              "      <td>0.308202</td>\n",
              "      <td>0.295222</td>\n",
              "      <td>0.303442</td>\n",
              "      <td>0.303253</td>\n",
              "      <td>0.193100</td>\n",
              "      <td>0.076472</td>\n",
              "      <td>0.121131</td>\n",
              "      <td>0.163870</td>\n",
              "      <td>0.179890</td>\n",
              "      <td>0.192081</td>\n",
              "      <td>0.147924</td>\n",
              "      <td>0.097252</td>\n",
              "      <td>0.078823</td>\n",
              "      <td>0.057794</td>\n",
              "      <td>0.081978</td>\n",
              "      <td>0.104101</td>\n",
              "      <td>0.128234</td>\n",
              "      <td>0.148866</td>\n",
              "      <td>0.153729</td>\n",
              "      <td>0.153465</td>\n",
              "      <td>0.023387</td>\n",
              "      <td>-0.110544</td>\n",
              "      <td>-0.164674</td>\n",
              "      <td>-0.212114</td>\n",
              "      <td>-0.268795</td>\n",
              "      <td>-0.317390</td>\n",
              "      <td>-0.334714</td>\n",
              "      <td>-0.340292</td>\n",
              "      <td>-0.336264</td>\n",
              "      <td>-0.323754</td>\n",
              "      <td>-0.282177</td>\n",
              "      <td>-0.233319</td>\n",
              "      <td>-0.207958</td>\n",
              "      <td>-0.176395</td>\n",
              "      <td>-0.175011</td>\n",
              "      <td>-0.168389</td>\n",
              "      <td>-0.148824</td>\n",
              "      <td>-0.125529</td>\n",
              "      <td>-0.107894</td>\n",
              "      <td>-0.087825</td>\n",
              "      <td>-0.065179</td>\n",
              "      <td>-0.040657</td>\n",
              "      <td>-0.034924</td>\n",
              "      <td>-0.028016</td>\n",
              "      <td>0.069571</td>\n",
              "      <td>0.162322</td>\n",
              "      <td>0.533351</td>\n",
              "      <td>0.807033</td>\n",
              "      <td>0.905874</td>\n",
              "      <td>0.946753</td>\n",
              "      <td>0.804837</td>\n",
              "      <td>0.643346</td>\n",
              "      <td>0.438834</td>\n",
              "      <td>0.206953</td>\n",
              "      <td>0.113523</td>\n",
              "      <td>-0.002207</td>\n",
              "      <td>-0.054296</td>\n",
              "      <td>-0.110161</td>\n",
              "      <td>-0.296514</td>\n",
              "      <td>-0.459548</td>\n",
              "      <td>-0.541077</td>\n",
              "      <td>-0.593285</td>\n",
              "      <td>-0.593491</td>\n",
              "      <td>-0.575759</td>\n",
              "      <td>-0.519580</td>\n",
              "      <td>-0.456263</td>\n",
              "      <td>-0.518367</td>\n",
              "      <td>-0.574085</td>\n",
              "      <td>-0.671875</td>\n",
              "      <td>-0.761205</td>\n",
              "      <td>-0.715704</td>\n",
              "      <td>-0.654415</td>\n",
              "      <td>-0.605921</td>\n",
              "      <td>-0.542007</td>\n",
              "      <td>-0.475091</td>\n",
              "      <td>-0.396057</td>\n",
              "      <td>-0.298474</td>\n",
              "      <td>-0.188902</td>\n",
              "      <td>-0.122920</td>\n",
              "      <td>-0.051221</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.555507</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055862</td>\n",
              "      <td>-0.055863</td>\n",
              "      <td>-0.055944</td>\n",
              "      <td>-0.586203</td>\n",
              "      <td>-0.621645</td>\n",
              "      <td>-0.630236</td>\n",
              "      <td>-0.582622</td>\n",
              "      <td>-0.519540</td>\n",
              "      <td>-0.386305</td>\n",
              "      <td>-0.231234</td>\n",
              "      <td>-0.107908</td>\n",
              "      <td>0.035299</td>\n",
              "      <td>0.157250</td>\n",
              "      <td>0.280811</td>\n",
              "      <td>0.278464</td>\n",
              "      <td>0.264531</td>\n",
              "      <td>0.253863</td>\n",
              "      <td>0.234533</td>\n",
              "      <td>0.313482</td>\n",
              "      <td>0.379435</td>\n",
              "      <td>0.416236</td>\n",
              "      <td>0.440602</td>\n",
              "      <td>0.366124</td>\n",
              "      <td>0.284517</td>\n",
              "      <td>0.358094</td>\n",
              "      <td>0.426753</td>\n",
              "      <td>0.493362</td>\n",
              "      <td>0.547843</td>\n",
              "      <td>0.461737</td>\n",
              "      <td>0.363400</td>\n",
              "      <td>0.313753</td>\n",
              "      <td>0.255029</td>\n",
              "      <td>0.307320</td>\n",
              "      <td>0.350229</td>\n",
              "      <td>0.282082</td>\n",
              "      <td>0.205729</td>\n",
              "      <td>0.252531</td>\n",
              "      <td>0.294914</td>\n",
              "      <td>0.237827</td>\n",
              "      <td>0.170709</td>\n",
              "      <td>0.109975</td>\n",
              "      <td>0.045517</td>\n",
              "      <td>0.003642</td>\n",
              "      <td>-0.038868</td>\n",
              "      <td>-0.078666</td>\n",
              "      <td>-0.116529</td>\n",
              "      <td>-0.141241</td>\n",
              "      <td>-0.161329</td>\n",
              "      <td>-0.202707</td>\n",
              "      <td>-0.237168</td>\n",
              "      <td>-0.308661</td>\n",
              "      <td>-0.368142</td>\n",
              "      <td>-0.314251</td>\n",
              "      <td>-0.253363</td>\n",
              "      <td>-0.190688</td>\n",
              "      <td>-0.122311</td>\n",
              "      <td>-0.088549</td>\n",
              "      <td>-0.052796</td>\n",
              "      <td>0.042830</td>\n",
              "      <td>0.139156</td>\n",
              "      <td>0.166951</td>\n",
              "      <td>0.190379</td>\n",
              "      <td>0.122998</td>\n",
              "      <td>0.053016</td>\n",
              "      <td>-0.017389</td>\n",
              "      <td>-0.085548</td>\n",
              "      <td>-0.147250</td>\n",
              "      <td>-0.205261</td>\n",
              "      <td>-0.203194</td>\n",
              "      <td>-0.196380</td>\n",
              "      <td>-0.251689</td>\n",
              "      <td>-0.301747</td>\n",
              "      <td>-0.147119</td>\n",
              "      <td>0.012147</td>\n",
              "      <td>0.271560</td>\n",
              "      <td>0.469608</td>\n",
              "      <td>0.580011</td>\n",
              "      <td>0.646652</td>\n",
              "      <td>0.668259</td>\n",
              "      <td>0.665517</td>\n",
              "      <td>0.666712</td>\n",
              "      <td>0.650845</td>\n",
              "      <td>0.597461</td>\n",
              "      <td>0.511388</td>\n",
              "      <td>0.291532</td>\n",
              "      <td>0.020334</td>\n",
              "      <td>-0.172656</td>\n",
              "      <td>-0.350064</td>\n",
              "      <td>-0.510096</td>\n",
              "      <td>-0.634887</td>\n",
              "      <td>-0.710786</td>\n",
              "      <td>-0.759362</td>\n",
              "      <td>-0.623256</td>\n",
              "      <td>-0.481743</td>\n",
              "      <td>-0.382711</td>\n",
              "      <td>-0.276907</td>\n",
              "      <td>-0.362880</td>\n",
              "      <td>-0.445428</td>\n",
              "      <td>-0.540571</td>\n",
              "      <td>-0.629918</td>\n",
              "      <td>-0.585123</td>\n",
              "      <td>-0.525531</td>\n",
              "      <td>-0.428659</td>\n",
              "      <td>-0.318883</td>\n",
              "      <td>-0.321564</td>\n",
              "      <td>-0.318218</td>\n",
              "      <td>-0.331974</td>\n",
              "      <td>-0.339568</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.497395</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29e6cb15-b399-42f5-b146-d2da89243a10')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29e6cb15-b399-42f5-b146-d2da89243a10 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29e6cb15-b399-42f5-b146-d2da89243a10');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6d4326dc-b06c-4ec1-a784-17721872461b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6d4326dc-b06c-4ec1-a784-17721872461b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6d4326dc-b06c-4ec1-a784-17721872461b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PIR_columns = [f'PIR_{i}' for i in range(109)] + ['Label']\n",
        "PIR_data = df[PIR_columns]"
      ],
      "metadata": {
        "id": "imt_OJz6y4sh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Meta_columns = ['Temperature_F', 'Hour', 'Minute', 'DayOfWeek', 'Month']\n",
        "Meta_data = df[Meta_columns]"
      ],
      "metadata": {
        "id": "aqxGtYQaz1A6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Meta_data.head(10)\n"
      ],
      "metadata": {
        "id": "QyL1DE451J8V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "07fa7669-eb7d-4222-a2ea-7c59342e2988"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Temperature_F      Hour    Minute  DayOfWeek     Month\n",
              "0       0.245676  0.954478 -0.613620  -1.023796 -1.887176\n",
              "1       0.245676  0.954478 -0.555507  -1.023796 -1.887176\n",
              "2       0.245676  0.954478 -0.555507  -1.023796 -1.887176\n",
              "3       0.245676  0.954478 -0.555507  -1.023796 -1.887176\n",
              "4       0.245676  0.954478 -0.497395  -1.023796 -1.887176\n",
              "5       0.245676  0.954478 -0.497395  -1.023796 -1.887176\n",
              "6       0.245676  0.954478 -0.497395  -1.023796 -1.887176\n",
              "7       0.245676  0.954478 -0.497395  -1.023796 -1.887176\n",
              "8       0.245676  0.954478 -0.439282  -1.023796 -1.887176\n",
              "9       0.245676  0.954478 -0.439282  -1.023796 -1.887176"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ec02a63-f82a-4638-a17a-580498d9a250\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Temperature_F</th>\n",
              "      <th>Hour</th>\n",
              "      <th>Minute</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.613620</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.555507</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.555507</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.555507</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.497395</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.497395</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.497395</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.497395</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.439282</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.954478</td>\n",
              "      <td>-0.439282</td>\n",
              "      <td>-1.023796</td>\n",
              "      <td>-1.887176</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ec02a63-f82a-4638-a17a-580498d9a250')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1ec02a63-f82a-4638-a17a-580498d9a250 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1ec02a63-f82a-4638-a17a-580498d9a250');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-80bc42a2-a447-4a25-82b3-428d251448d5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-80bc42a2-a447-4a25-82b3-428d251448d5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-80bc42a2-a447-4a25-82b3-428d251448d5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "Meta_data",
              "summary": "{\n  \"name\": \"Meta_data\",\n  \"rows\": 15302,\n  \"fields\": [\n    {\n      \"column\": \"Temperature_F\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000326770688825,\n        \"min\": -3.516366679819658,\n        \"max\": 0.37690978341585946,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.24567574532926897,\n          0.28942042469146584,\n          0.20193106596707217\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hour\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000326770689076,\n        \"min\": -1.4955332389369969,\n        \"max\": 1.4702694866248962,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          -1.1086894051680543,\n          -0.07710584845087409,\n          0.9544777082663061\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Minute\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000326770688792,\n        \"min\": -1.7177603559847685,\n        \"max\": 1.7108864890597288,\n        \"num_unique_values\": 60,\n        \"samples\": [\n          -0.6136198465636592,\n          -0.3230565546107357,\n          1.47843585549739\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DayOfWeek\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000326770688857,\n        \"min\": -2.101986753858741,\n        \"max\": 0.5934913659074875,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.0237955059522497,\n          0.5934913659074875,\n          -2.101986753858741\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000326770686987,\n        \"min\": -1.88717570291626,\n        \"max\": 1.9135037661577865,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.88717570291626,\n          0.01316403162076321,\n          1.9135037661577865\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PIR_data.head(10)"
      ],
      "metadata": {
        "id": "niE9n5P4y5xy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "f93ae93e-32bb-442e-a270-c1a95eb3ab4e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      PIR_0     PIR_1     PIR_2     PIR_3     PIR_4     PIR_5     PIR_6  \\\n",
              "0  0.245676 -0.055874 -0.055875 -0.055944 -0.494950 -0.535025 -0.551045   \n",
              "1  0.245676 -0.055852 -0.055853 -0.055878 -0.180633 -0.266177 -0.333270   \n",
              "2  0.245676 -0.055860 -0.055861 -0.055913 -0.373279 -0.442685 -0.488606   \n",
              "3  0.245676 -0.055897 -0.055899 -0.056036 -0.995152 -1.049842 -1.059694   \n",
              "4  0.245676 -0.055862 -0.055863 -0.055944 -0.586203 -0.621645 -0.630236   \n",
              "5  0.245676 -0.055874 -0.055875 -0.055998 -0.888690 -0.898666 -0.873900   \n",
              "6  0.245676 -0.055864 -0.055865 -0.055938 -0.527057 -0.570164 -0.587594   \n",
              "7  0.245676 -0.055836 -0.055837 -0.055856 -0.136697 -0.295595 -0.427690   \n",
              "8  0.245676 -0.055851 -0.055853 -0.055945 -0.669006 -0.768736 -0.829736   \n",
              "9  0.245676 -0.055846 -0.055847 -0.055884 -0.261747 -0.379763 -0.471854   \n",
              "\n",
              "      PIR_7     PIR_8     PIR_9    PIR_10    PIR_11    PIR_12    PIR_13  \\\n",
              "0 -0.591841 -0.616061 -0.595164 -0.558023 -0.411659 -0.230890 -0.040599   \n",
              "1 -0.429731 -0.513508 -0.591975 -0.663652 -0.589812 -0.484048 -0.392987   \n",
              "2 -0.505024 -0.507475 -0.441310 -0.358319 -0.315456 -0.255089 -0.158718   \n",
              "3 -0.876879 -0.671862 -0.461239 -0.219681 -0.083858  0.072528  0.242886   \n",
              "4 -0.582622 -0.519540 -0.386305 -0.231234 -0.107908  0.035299  0.157250   \n",
              "5 -0.676354 -0.462231 -0.257961 -0.028230  0.115674  0.273566  0.339350   \n",
              "6 -0.465072 -0.331023 -0.306588 -0.272496 -0.221035 -0.154570 -0.041583   \n",
              "7 -0.513475 -0.584390 -0.602339 -0.607536 -0.526568 -0.415174 -0.325069   \n",
              "8 -0.776232 -0.702025 -0.679664 -0.638895 -0.504299 -0.333270 -0.132141   \n",
              "9 -0.564182 -0.640191 -0.706768 -0.762678 -0.701157 -0.605043 -0.569181   \n",
              "\n",
              "     PIR_14    PIR_15    PIR_16    PIR_17    PIR_18    PIR_19    PIR_20  \\\n",
              "0  0.165129  0.314171  0.450680  0.498168  0.525723  0.528330  0.515734   \n",
              "1 -0.277654 -0.099009  0.084387  0.286769  0.470167  0.586064  0.679292   \n",
              "2 -0.048284  0.057081  0.160448  0.287766  0.399285  0.471542  0.526638   \n",
              "3  0.414444  0.538615  0.640832  0.693612  0.719211  0.759267  0.775610   \n",
              "4  0.280811  0.278464  0.264531  0.253863  0.234533  0.313482  0.379435   \n",
              "5  0.394498  0.515151  0.614811  0.670677  0.700054  0.753588  0.782879   \n",
              "6  0.081359  0.193788  0.298558  0.350587  0.387791  0.405289  0.410329   \n",
              "7 -0.213829 -0.037797  0.140432  0.259846  0.364802  0.487632  0.590244   \n",
              "8  0.089337  0.273363  0.446677  0.454293  0.445262  0.520758  0.577522   \n",
              "9 -0.505029 -0.376504 -0.231866 -0.047281  0.131083  0.260480  0.375800   \n",
              "\n",
              "     PIR_21    PIR_22    PIR_23    PIR_24    PIR_25    PIR_26    PIR_27  \\\n",
              "0  0.447967  0.370196  0.279563  0.184010  0.194594  0.201863  0.243975   \n",
              "1  0.689126  0.679981  0.685867  0.676146  0.731050  0.773828  0.707510   \n",
              "2  0.563107  0.583173  0.570159  0.544447  0.605553  0.656956  0.614442   \n",
              "3  0.672807  0.555011  0.499497  0.433544  0.429681  0.417899  0.371379   \n",
              "4  0.416236  0.440602  0.366124  0.284517  0.358094  0.426753  0.493362   \n",
              "5  0.692752  0.586693  0.518046  0.438742  0.419959  0.393108  0.348790   \n",
              "6  0.408983  0.396598  0.393505  0.381557  0.399632  0.410816  0.463544   \n",
              "7  0.552227  0.500447  0.491548  0.471667  0.439402  0.398420  0.318068   \n",
              "8  0.591212  0.588454  0.576342  0.551379  0.582575  0.603832  0.592756   \n",
              "9  0.479698  0.567332  0.578108  0.575639  0.620578  0.655185  0.682210   \n",
              "\n",
              "     PIR_28    PIR_29    PIR_30    PIR_31    PIR_32    PIR_33    PIR_34  \\\n",
              "0  0.280221  0.381752  0.477645  0.453421  0.416393  0.281833  0.137928   \n",
              "1  0.621484  0.646532  0.658225  0.535910  0.397845  0.323367  0.238492   \n",
              "2  0.555027  0.538965  0.510813  0.500290  0.475745  0.345078  0.203108   \n",
              "3  0.314347  0.308202  0.295222  0.303442  0.303253  0.193100  0.076472   \n",
              "4  0.547843  0.461737  0.363400  0.313753  0.255029  0.307320  0.350229   \n",
              "5  0.294590  0.177651  0.053835  0.023168 -0.008346 -0.036283 -0.063201   \n",
              "6  0.504736  0.527013  0.538452  0.475918  0.399700  0.374341  0.337193   \n",
              "7  0.228133  0.214426  0.195719  0.210642  0.219789  0.198764  0.171449   \n",
              "8  0.565804  0.449785  0.321019  0.265947  0.203096  0.127967  0.048537   \n",
              "9  0.691532  0.646532  0.586361  0.480605  0.360750  0.285608  0.201246   \n",
              "\n",
              "     PIR_35    PIR_36    PIR_37    PIR_38    PIR_39    PIR_40    PIR_41  \\\n",
              "0  0.199226  0.257101  0.307502  0.352263  0.280735  0.197236  0.140089   \n",
              "1  0.144941  0.045904 -0.056683 -0.161902 -0.176954 -0.186373 -0.169357   \n",
              "2  0.127798  0.047807  0.080746  0.112979  0.082539  0.048281 -0.071746   \n",
              "3  0.121131  0.163870  0.179890  0.192081  0.147924  0.097252  0.078823   \n",
              "4  0.282082  0.205729  0.252531  0.294914  0.237827  0.170709  0.109975   \n",
              "5 -0.113152 -0.161487 -0.187240 -0.209363 -0.269922 -0.323085 -0.280467   \n",
              "6  0.273511  0.201923  0.173019  0.138687  0.093777  0.044200 -0.090438   \n",
              "7  0.217321  0.259003  0.305539  0.346330  0.366552  0.374757  0.327002   \n",
              "8 -0.016010 -0.081575 -0.052756 -0.021495  0.035545  0.093172  0.133858   \n",
              "9  0.201131  0.196215  0.240752  0.281071  0.252129  0.213559  0.208624   \n",
              "\n",
              "     PIR_42    PIR_43    PIR_44    PIR_45    PIR_46    PIR_47    PIR_48  \\\n",
              "0  0.078255 -0.035004 -0.148685 -0.162059 -0.170861 -0.131657 -0.087877   \n",
              "1 -0.146818 -0.144674 -0.138325 -0.143058 -0.143695 -0.165733 -0.182316   \n",
              "2 -0.189786 -0.165564 -0.136253 -0.123001 -0.106080 -0.086932 -0.064792   \n",
              "3  0.057794  0.081978  0.104101  0.128234  0.148866  0.153729  0.153465   \n",
              "4  0.045517  0.003642 -0.038868 -0.078666 -0.116529 -0.141241 -0.161329   \n",
              "5 -0.228662 -0.156163 -0.078237  0.019506  0.117520  0.160118  0.197537   \n",
              "6 -0.222524 -0.343125 -0.455343 -0.356291 -0.246091 -0.119943  0.010758   \n",
              "7  0.268544  0.156136  0.037796 -0.078666 -0.193848 -0.198744 -0.197006   \n",
              "8  0.170330  0.174937  0.174549  0.197904  0.215737  0.171832  0.121986   \n",
              "9  0.196930  0.175981  0.149685  0.075453 -0.001594  0.044047  0.088408   \n",
              "\n",
              "     PIR_49    PIR_50    PIR_51    PIR_52    PIR_53    PIR_54    PIR_55  \\\n",
              "0 -0.019885  0.050418  0.032361  0.013493 -0.033058 -0.077806 -0.092583   \n",
              "1 -0.138881 -0.089082 -0.023935  0.040904  0.013455 -0.013779 -0.041444   \n",
              "2 -0.097773 -0.127713 -0.135444 -0.138317 -0.155684 -0.168683 -0.265832   \n",
              "3  0.023387 -0.110544 -0.164674 -0.212114 -0.268795 -0.317390 -0.334714   \n",
              "4 -0.202707 -0.237168 -0.308661 -0.368142 -0.314251 -0.253363 -0.190688   \n",
              "5  0.145629  0.086903 -0.054248 -0.191029 -0.156741 -0.119114 -0.019527   \n",
              "6  0.066658  0.121242  0.086492  0.049338  0.000770 -0.046825 -0.076928   \n",
              "7 -0.142127 -0.080498 -0.177665 -0.266935 -0.332222 -0.387613 -0.253308   \n",
              "8  0.087212  0.048272 -0.003365 -0.053978 -0.149341 -0.238906 -0.325321   \n",
              "9  0.120748  0.149142  0.064840 -0.020242 -0.157798 -0.288475 -0.323234   \n",
              "\n",
              "     PIR_56    PIR_57    PIR_58    PIR_59    PIR_60    PIR_61    PIR_62  \\\n",
              "0 -0.103976 -0.173862 -0.238827 -0.266700 -0.288882 -0.233162 -0.170143   \n",
              "1 -0.067306 -0.124524 -0.178165 -0.179000 -0.175699 -0.074587  0.029915   \n",
              "2 -0.352515 -0.325986 -0.291400 -0.253287 -0.208625 -0.174353 -0.134716   \n",
              "3 -0.340292 -0.336264 -0.323754 -0.282177 -0.233319 -0.207958 -0.176395   \n",
              "4 -0.122311 -0.088549 -0.052796  0.042830  0.139156  0.166951  0.190379   \n",
              "5  0.079373  0.083105  0.084704  0.049020  0.011567 -0.131296 -0.272257   \n",
              "6 -0.103976 -0.098828 -0.091216 -0.138761 -0.183930 -0.255215 -0.320187   \n",
              "7 -0.112125 -0.003236  0.104925  0.096482  0.085651  0.073486  0.059090   \n",
              "8 -0.399371 -0.413354 -0.416769 -0.296621 -0.167467 -0.103991 -0.036771   \n",
              "9 -0.346404 -0.348599 -0.341952 -0.382258 -0.414412 -0.279369 -0.134716   \n",
              "\n",
              "     PIR_63    PIR_64    PIR_65    PIR_66    PIR_67    PIR_68    PIR_69  \\\n",
              "0 -0.050141  0.069416  0.075027  0.078371  0.093931  0.107238  0.127637   \n",
              "1  0.093617  0.153468  0.104805  0.054383 -0.042301 -0.137586 -0.217401   \n",
              "2 -0.171863 -0.203240 -0.132395 -0.059561 -0.021109  0.017668  0.106326   \n",
              "3 -0.175011 -0.168389 -0.148824 -0.125529 -0.107894 -0.087825 -0.065179   \n",
              "4  0.122998  0.053016 -0.017389 -0.085548 -0.147250 -0.205261 -0.203194   \n",
              "5 -0.247414 -0.215540 -0.076945  0.060380  0.117141  0.170932  0.128652   \n",
              "6 -0.340804 -0.350843 -0.273072 -0.189497 -0.151286 -0.109720 -0.109831   \n",
              "7  0.092567  0.122718  0.106859  0.088366  0.085858  0.081362  0.042392   \n",
              "8  0.045348  0.124768  0.137664  0.146338  0.088885  0.029611  0.029199   \n",
              "9 -0.090015 -0.043336  0.027792  0.096362  0.083839  0.069420  0.039348   \n",
              "\n",
              "     PIR_70    PIR_71    PIR_72    PIR_73    PIR_74    PIR_75    PIR_76  \\\n",
              "0  0.145402  0.116187  0.082729  0.108872  0.129407  0.287074  0.399382   \n",
              "1 -0.293455 -0.223551 -0.145031 -0.104631 -0.059855  0.043706  0.127043   \n",
              "2  0.193939  0.140156  0.080640  0.244834  0.394786  0.496506  0.548397   \n",
              "3 -0.040657 -0.034924 -0.028016  0.069571  0.162322  0.533351  0.807033   \n",
              "4 -0.196380 -0.251689 -0.301747 -0.147119  0.012147  0.271560  0.469608   \n",
              "5  0.082708 -0.008870 -0.103240 -0.035587  0.032719  0.416030  0.707690   \n",
              "6 -0.107396 -0.033881  0.043028  0.185351  0.316612  0.468388  0.563813   \n",
              "7  0.001813 -0.064104 -0.130404 -0.274583 -0.403408 -0.380007 -0.335419   \n",
              "8  0.028104 -0.046387 -0.122046 -0.088698 -0.051627  0.149391  0.306890   \n",
              "9  0.007880 -0.010954 -0.030106 -0.086573 -0.138029 -0.140517 -0.133306   \n",
              "\n",
              "     PIR_77    PIR_78    PIR_79    PIR_80    PIR_81    PIR_82    PIR_83  \\\n",
              "0  0.472744  0.512783  0.502623  0.475682  0.477421  0.467011  0.412986   \n",
              "1  0.285027  0.406865  0.546212  0.658589  0.799215  0.930333  0.896097   \n",
              "2  0.592200  0.601048  0.599971  0.578221  0.522559  0.449076  0.316198   \n",
              "3  0.905874  0.946753  0.804837  0.643346  0.438834  0.206953  0.113523   \n",
              "4  0.580011  0.646652  0.668259  0.665517  0.666712  0.650845  0.597461   \n",
              "5  0.862805  0.954109  0.831716  0.687687  0.419905  0.120267 -0.080879   \n",
              "6  0.636895  0.668718  0.631936  0.575449  0.462860  0.329509  0.250846   \n",
              "7 -0.169232 -0.018278  0.233828  0.463212  0.633950  0.801799  0.795173   \n",
              "8  0.394731  0.450998  0.486641  0.503396  0.479605  0.441603  0.386514   \n",
              "9  0.039613  0.186202  0.368225  0.526952  0.655063  0.776391  0.791864   \n",
              "\n",
              "     PIR_84    PIR_85    PIR_86    PIR_87    PIR_88    PIR_89    PIR_90  \\\n",
              "0  0.333056  0.177566 -0.012290 -0.189309 -0.350064 -0.398756 -0.426881   \n",
              "1  0.821685  0.664868  0.440365  0.232227  0.014222 -0.144127 -0.282181   \n",
              "2  0.145808  0.049845 -0.063265 -0.165370 -0.254513 -0.312589 -0.352722   \n",
              "3 -0.002207 -0.054296 -0.110161 -0.296514 -0.459548 -0.541077 -0.593285   \n",
              "4  0.511388  0.291532  0.020334 -0.172656 -0.350064 -0.510096 -0.634887   \n",
              "5 -0.317854 -0.473808 -0.619908 -0.690989 -0.716340 -0.740520 -0.732559   \n",
              "6  0.147591  0.076372 -0.010251 -0.093553 -0.168916 -0.322270 -0.448586   \n",
              "7  0.757485  0.624587  0.430170  0.233268  0.026166 -0.075386 -0.164613   \n",
              "8  0.306307  0.204092  0.073348 -0.070654 -0.206738 -0.253530 -0.285799   \n",
              "9  0.780669  0.658974  0.475028  0.256166  0.026166 -0.140254 -0.285799   \n",
              "\n",
              "     PIR_91    PIR_92    PIR_93    PIR_94    PIR_95    PIR_96    PIR_97  \\\n",
              "0 -0.457676 -0.472273 -0.531099 -0.577291 -0.599922 -0.614463 -0.599804   \n",
              "1 -0.436510 -0.565744 -0.619964 -0.660100 -0.662097 -0.654840 -0.610573   \n",
              "2 -0.430337 -0.488964 -0.533568 -0.566144 -0.593462 -0.612848 -0.588206   \n",
              "3 -0.593491 -0.575759 -0.519580 -0.456263 -0.518367 -0.574085 -0.671875   \n",
              "4 -0.710786 -0.759362 -0.623256 -0.481743 -0.382711 -0.276907 -0.362880   \n",
              "5 -0.737243 -0.719303 -0.629015 -0.531109 -0.497373 -0.456183 -0.381934   \n",
              "6 -0.612011 -0.744340 -0.747502 -0.736538 -0.737192 -0.727520 -0.635425   \n",
              "7 -0.230142 -0.283663 -0.414259 -0.531109 -0.564393 -0.590236 -0.580750   \n",
              "8 -0.280411 -0.266972 -0.299063 -0.324088 -0.410165 -0.491715 -0.532703   \n",
              "9 -0.388887 -0.472273 -0.519580 -0.554996 -0.592654 -0.622538 -0.599804   \n",
              "\n",
              "     PIR_98    PIR_99   PIR_100   PIR_101   PIR_102   PIR_103   PIR_104  \\\n",
              "0 -0.574078 -0.578531 -0.572173 -0.534485 -0.483428 -0.424869 -0.355541   \n",
              "1 -0.554029 -0.495710 -0.425187 -0.472090 -0.510887 -0.524366 -0.529183   \n",
              "2 -0.552358 -0.524180 -0.484681 -0.463952 -0.432172 -0.431502 -0.423068   \n",
              "3 -0.761205 -0.715704 -0.654415 -0.605921 -0.542007 -0.475091 -0.396057   \n",
              "4 -0.445428 -0.540571 -0.629918 -0.585123 -0.525531 -0.428659 -0.318883   \n",
              "5 -0.298400 -0.324028 -0.344695 -0.341876 -0.331490 -0.216397 -0.091219   \n",
              "6 -0.528967 -0.488808 -0.437436 -0.362675 -0.276572 -0.243877 -0.205051   \n",
              "7 -0.560712 -0.576806 -0.582672 -0.577889 -0.560312 -0.581222 -0.592851   \n",
              "8 -0.565724 -0.575943 -0.575673 -0.617677 -0.648180 -0.602069 -0.542688   \n",
              "9 -0.565724 -0.479318 -0.379691 -0.396132 -0.404713 -0.436240 -0.461655   \n",
              "\n",
              "    PIR_105   PIR_106   PIR_107   PIR_108  Label  \n",
              "0 -0.338631 -0.314113 -0.347892 -0.376149    0.0  \n",
              "1 -0.526364 -0.513220 -0.464623 -0.404123    1.0  \n",
              "2 -0.430992 -0.431114 -0.391401 -0.341720    0.0  \n",
              "3 -0.298474 -0.188902 -0.122920 -0.051221    0.0  \n",
              "4 -0.321564 -0.318218 -0.331974 -0.339568    0.0  \n",
              "5 -0.103713 -0.115007 -0.064554 -0.010336    0.0  \n",
              "6 -0.217156 -0.225850 -0.168551 -0.105017    0.0  \n",
              "7 -0.538411 -0.470114 -0.404135 -0.326657    0.0  \n",
              "8 -0.510301 -0.466009 -0.456133 -0.436401    0.0  \n",
              "9 -0.513313 -0.558378 -0.549518 -0.528930    0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-75eeb1d0-6029-4f22-9a19-237eb40d1a1b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PIR_0</th>\n",
              "      <th>PIR_1</th>\n",
              "      <th>PIR_2</th>\n",
              "      <th>PIR_3</th>\n",
              "      <th>PIR_4</th>\n",
              "      <th>PIR_5</th>\n",
              "      <th>PIR_6</th>\n",
              "      <th>PIR_7</th>\n",
              "      <th>PIR_8</th>\n",
              "      <th>PIR_9</th>\n",
              "      <th>PIR_10</th>\n",
              "      <th>PIR_11</th>\n",
              "      <th>PIR_12</th>\n",
              "      <th>PIR_13</th>\n",
              "      <th>PIR_14</th>\n",
              "      <th>PIR_15</th>\n",
              "      <th>PIR_16</th>\n",
              "      <th>PIR_17</th>\n",
              "      <th>PIR_18</th>\n",
              "      <th>PIR_19</th>\n",
              "      <th>PIR_20</th>\n",
              "      <th>PIR_21</th>\n",
              "      <th>PIR_22</th>\n",
              "      <th>PIR_23</th>\n",
              "      <th>PIR_24</th>\n",
              "      <th>PIR_25</th>\n",
              "      <th>PIR_26</th>\n",
              "      <th>PIR_27</th>\n",
              "      <th>PIR_28</th>\n",
              "      <th>PIR_29</th>\n",
              "      <th>PIR_30</th>\n",
              "      <th>PIR_31</th>\n",
              "      <th>PIR_32</th>\n",
              "      <th>PIR_33</th>\n",
              "      <th>PIR_34</th>\n",
              "      <th>PIR_35</th>\n",
              "      <th>PIR_36</th>\n",
              "      <th>PIR_37</th>\n",
              "      <th>PIR_38</th>\n",
              "      <th>PIR_39</th>\n",
              "      <th>PIR_40</th>\n",
              "      <th>PIR_41</th>\n",
              "      <th>PIR_42</th>\n",
              "      <th>PIR_43</th>\n",
              "      <th>PIR_44</th>\n",
              "      <th>PIR_45</th>\n",
              "      <th>PIR_46</th>\n",
              "      <th>PIR_47</th>\n",
              "      <th>PIR_48</th>\n",
              "      <th>PIR_49</th>\n",
              "      <th>PIR_50</th>\n",
              "      <th>PIR_51</th>\n",
              "      <th>PIR_52</th>\n",
              "      <th>PIR_53</th>\n",
              "      <th>PIR_54</th>\n",
              "      <th>PIR_55</th>\n",
              "      <th>PIR_56</th>\n",
              "      <th>PIR_57</th>\n",
              "      <th>PIR_58</th>\n",
              "      <th>PIR_59</th>\n",
              "      <th>PIR_60</th>\n",
              "      <th>PIR_61</th>\n",
              "      <th>PIR_62</th>\n",
              "      <th>PIR_63</th>\n",
              "      <th>PIR_64</th>\n",
              "      <th>PIR_65</th>\n",
              "      <th>PIR_66</th>\n",
              "      <th>PIR_67</th>\n",
              "      <th>PIR_68</th>\n",
              "      <th>PIR_69</th>\n",
              "      <th>PIR_70</th>\n",
              "      <th>PIR_71</th>\n",
              "      <th>PIR_72</th>\n",
              "      <th>PIR_73</th>\n",
              "      <th>PIR_74</th>\n",
              "      <th>PIR_75</th>\n",
              "      <th>PIR_76</th>\n",
              "      <th>PIR_77</th>\n",
              "      <th>PIR_78</th>\n",
              "      <th>PIR_79</th>\n",
              "      <th>PIR_80</th>\n",
              "      <th>PIR_81</th>\n",
              "      <th>PIR_82</th>\n",
              "      <th>PIR_83</th>\n",
              "      <th>PIR_84</th>\n",
              "      <th>PIR_85</th>\n",
              "      <th>PIR_86</th>\n",
              "      <th>PIR_87</th>\n",
              "      <th>PIR_88</th>\n",
              "      <th>PIR_89</th>\n",
              "      <th>PIR_90</th>\n",
              "      <th>PIR_91</th>\n",
              "      <th>PIR_92</th>\n",
              "      <th>PIR_93</th>\n",
              "      <th>PIR_94</th>\n",
              "      <th>PIR_95</th>\n",
              "      <th>PIR_96</th>\n",
              "      <th>PIR_97</th>\n",
              "      <th>PIR_98</th>\n",
              "      <th>PIR_99</th>\n",
              "      <th>PIR_100</th>\n",
              "      <th>PIR_101</th>\n",
              "      <th>PIR_102</th>\n",
              "      <th>PIR_103</th>\n",
              "      <th>PIR_104</th>\n",
              "      <th>PIR_105</th>\n",
              "      <th>PIR_106</th>\n",
              "      <th>PIR_107</th>\n",
              "      <th>PIR_108</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055874</td>\n",
              "      <td>-0.055875</td>\n",
              "      <td>-0.055944</td>\n",
              "      <td>-0.494950</td>\n",
              "      <td>-0.535025</td>\n",
              "      <td>-0.551045</td>\n",
              "      <td>-0.591841</td>\n",
              "      <td>-0.616061</td>\n",
              "      <td>-0.595164</td>\n",
              "      <td>-0.558023</td>\n",
              "      <td>-0.411659</td>\n",
              "      <td>-0.230890</td>\n",
              "      <td>-0.040599</td>\n",
              "      <td>0.165129</td>\n",
              "      <td>0.314171</td>\n",
              "      <td>0.450680</td>\n",
              "      <td>0.498168</td>\n",
              "      <td>0.525723</td>\n",
              "      <td>0.528330</td>\n",
              "      <td>0.515734</td>\n",
              "      <td>0.447967</td>\n",
              "      <td>0.370196</td>\n",
              "      <td>0.279563</td>\n",
              "      <td>0.184010</td>\n",
              "      <td>0.194594</td>\n",
              "      <td>0.201863</td>\n",
              "      <td>0.243975</td>\n",
              "      <td>0.280221</td>\n",
              "      <td>0.381752</td>\n",
              "      <td>0.477645</td>\n",
              "      <td>0.453421</td>\n",
              "      <td>0.416393</td>\n",
              "      <td>0.281833</td>\n",
              "      <td>0.137928</td>\n",
              "      <td>0.199226</td>\n",
              "      <td>0.257101</td>\n",
              "      <td>0.307502</td>\n",
              "      <td>0.352263</td>\n",
              "      <td>0.280735</td>\n",
              "      <td>0.197236</td>\n",
              "      <td>0.140089</td>\n",
              "      <td>0.078255</td>\n",
              "      <td>-0.035004</td>\n",
              "      <td>-0.148685</td>\n",
              "      <td>-0.162059</td>\n",
              "      <td>-0.170861</td>\n",
              "      <td>-0.131657</td>\n",
              "      <td>-0.087877</td>\n",
              "      <td>-0.019885</td>\n",
              "      <td>0.050418</td>\n",
              "      <td>0.032361</td>\n",
              "      <td>0.013493</td>\n",
              "      <td>-0.033058</td>\n",
              "      <td>-0.077806</td>\n",
              "      <td>-0.092583</td>\n",
              "      <td>-0.103976</td>\n",
              "      <td>-0.173862</td>\n",
              "      <td>-0.238827</td>\n",
              "      <td>-0.266700</td>\n",
              "      <td>-0.288882</td>\n",
              "      <td>-0.233162</td>\n",
              "      <td>-0.170143</td>\n",
              "      <td>-0.050141</td>\n",
              "      <td>0.069416</td>\n",
              "      <td>0.075027</td>\n",
              "      <td>0.078371</td>\n",
              "      <td>0.093931</td>\n",
              "      <td>0.107238</td>\n",
              "      <td>0.127637</td>\n",
              "      <td>0.145402</td>\n",
              "      <td>0.116187</td>\n",
              "      <td>0.082729</td>\n",
              "      <td>0.108872</td>\n",
              "      <td>0.129407</td>\n",
              "      <td>0.287074</td>\n",
              "      <td>0.399382</td>\n",
              "      <td>0.472744</td>\n",
              "      <td>0.512783</td>\n",
              "      <td>0.502623</td>\n",
              "      <td>0.475682</td>\n",
              "      <td>0.477421</td>\n",
              "      <td>0.467011</td>\n",
              "      <td>0.412986</td>\n",
              "      <td>0.333056</td>\n",
              "      <td>0.177566</td>\n",
              "      <td>-0.012290</td>\n",
              "      <td>-0.189309</td>\n",
              "      <td>-0.350064</td>\n",
              "      <td>-0.398756</td>\n",
              "      <td>-0.426881</td>\n",
              "      <td>-0.457676</td>\n",
              "      <td>-0.472273</td>\n",
              "      <td>-0.531099</td>\n",
              "      <td>-0.577291</td>\n",
              "      <td>-0.599922</td>\n",
              "      <td>-0.614463</td>\n",
              "      <td>-0.599804</td>\n",
              "      <td>-0.574078</td>\n",
              "      <td>-0.578531</td>\n",
              "      <td>-0.572173</td>\n",
              "      <td>-0.534485</td>\n",
              "      <td>-0.483428</td>\n",
              "      <td>-0.424869</td>\n",
              "      <td>-0.355541</td>\n",
              "      <td>-0.338631</td>\n",
              "      <td>-0.314113</td>\n",
              "      <td>-0.347892</td>\n",
              "      <td>-0.376149</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055852</td>\n",
              "      <td>-0.055853</td>\n",
              "      <td>-0.055878</td>\n",
              "      <td>-0.180633</td>\n",
              "      <td>-0.266177</td>\n",
              "      <td>-0.333270</td>\n",
              "      <td>-0.429731</td>\n",
              "      <td>-0.513508</td>\n",
              "      <td>-0.591975</td>\n",
              "      <td>-0.663652</td>\n",
              "      <td>-0.589812</td>\n",
              "      <td>-0.484048</td>\n",
              "      <td>-0.392987</td>\n",
              "      <td>-0.277654</td>\n",
              "      <td>-0.099009</td>\n",
              "      <td>0.084387</td>\n",
              "      <td>0.286769</td>\n",
              "      <td>0.470167</td>\n",
              "      <td>0.586064</td>\n",
              "      <td>0.679292</td>\n",
              "      <td>0.689126</td>\n",
              "      <td>0.679981</td>\n",
              "      <td>0.685867</td>\n",
              "      <td>0.676146</td>\n",
              "      <td>0.731050</td>\n",
              "      <td>0.773828</td>\n",
              "      <td>0.707510</td>\n",
              "      <td>0.621484</td>\n",
              "      <td>0.646532</td>\n",
              "      <td>0.658225</td>\n",
              "      <td>0.535910</td>\n",
              "      <td>0.397845</td>\n",
              "      <td>0.323367</td>\n",
              "      <td>0.238492</td>\n",
              "      <td>0.144941</td>\n",
              "      <td>0.045904</td>\n",
              "      <td>-0.056683</td>\n",
              "      <td>-0.161902</td>\n",
              "      <td>-0.176954</td>\n",
              "      <td>-0.186373</td>\n",
              "      <td>-0.169357</td>\n",
              "      <td>-0.146818</td>\n",
              "      <td>-0.144674</td>\n",
              "      <td>-0.138325</td>\n",
              "      <td>-0.143058</td>\n",
              "      <td>-0.143695</td>\n",
              "      <td>-0.165733</td>\n",
              "      <td>-0.182316</td>\n",
              "      <td>-0.138881</td>\n",
              "      <td>-0.089082</td>\n",
              "      <td>-0.023935</td>\n",
              "      <td>0.040904</td>\n",
              "      <td>0.013455</td>\n",
              "      <td>-0.013779</td>\n",
              "      <td>-0.041444</td>\n",
              "      <td>-0.067306</td>\n",
              "      <td>-0.124524</td>\n",
              "      <td>-0.178165</td>\n",
              "      <td>-0.179000</td>\n",
              "      <td>-0.175699</td>\n",
              "      <td>-0.074587</td>\n",
              "      <td>0.029915</td>\n",
              "      <td>0.093617</td>\n",
              "      <td>0.153468</td>\n",
              "      <td>0.104805</td>\n",
              "      <td>0.054383</td>\n",
              "      <td>-0.042301</td>\n",
              "      <td>-0.137586</td>\n",
              "      <td>-0.217401</td>\n",
              "      <td>-0.293455</td>\n",
              "      <td>-0.223551</td>\n",
              "      <td>-0.145031</td>\n",
              "      <td>-0.104631</td>\n",
              "      <td>-0.059855</td>\n",
              "      <td>0.043706</td>\n",
              "      <td>0.127043</td>\n",
              "      <td>0.285027</td>\n",
              "      <td>0.406865</td>\n",
              "      <td>0.546212</td>\n",
              "      <td>0.658589</td>\n",
              "      <td>0.799215</td>\n",
              "      <td>0.930333</td>\n",
              "      <td>0.896097</td>\n",
              "      <td>0.821685</td>\n",
              "      <td>0.664868</td>\n",
              "      <td>0.440365</td>\n",
              "      <td>0.232227</td>\n",
              "      <td>0.014222</td>\n",
              "      <td>-0.144127</td>\n",
              "      <td>-0.282181</td>\n",
              "      <td>-0.436510</td>\n",
              "      <td>-0.565744</td>\n",
              "      <td>-0.619964</td>\n",
              "      <td>-0.660100</td>\n",
              "      <td>-0.662097</td>\n",
              "      <td>-0.654840</td>\n",
              "      <td>-0.610573</td>\n",
              "      <td>-0.554029</td>\n",
              "      <td>-0.495710</td>\n",
              "      <td>-0.425187</td>\n",
              "      <td>-0.472090</td>\n",
              "      <td>-0.510887</td>\n",
              "      <td>-0.524366</td>\n",
              "      <td>-0.529183</td>\n",
              "      <td>-0.526364</td>\n",
              "      <td>-0.513220</td>\n",
              "      <td>-0.464623</td>\n",
              "      <td>-0.404123</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055860</td>\n",
              "      <td>-0.055861</td>\n",
              "      <td>-0.055913</td>\n",
              "      <td>-0.373279</td>\n",
              "      <td>-0.442685</td>\n",
              "      <td>-0.488606</td>\n",
              "      <td>-0.505024</td>\n",
              "      <td>-0.507475</td>\n",
              "      <td>-0.441310</td>\n",
              "      <td>-0.358319</td>\n",
              "      <td>-0.315456</td>\n",
              "      <td>-0.255089</td>\n",
              "      <td>-0.158718</td>\n",
              "      <td>-0.048284</td>\n",
              "      <td>0.057081</td>\n",
              "      <td>0.160448</td>\n",
              "      <td>0.287766</td>\n",
              "      <td>0.399285</td>\n",
              "      <td>0.471542</td>\n",
              "      <td>0.526638</td>\n",
              "      <td>0.563107</td>\n",
              "      <td>0.583173</td>\n",
              "      <td>0.570159</td>\n",
              "      <td>0.544447</td>\n",
              "      <td>0.605553</td>\n",
              "      <td>0.656956</td>\n",
              "      <td>0.614442</td>\n",
              "      <td>0.555027</td>\n",
              "      <td>0.538965</td>\n",
              "      <td>0.510813</td>\n",
              "      <td>0.500290</td>\n",
              "      <td>0.475745</td>\n",
              "      <td>0.345078</td>\n",
              "      <td>0.203108</td>\n",
              "      <td>0.127798</td>\n",
              "      <td>0.047807</td>\n",
              "      <td>0.080746</td>\n",
              "      <td>0.112979</td>\n",
              "      <td>0.082539</td>\n",
              "      <td>0.048281</td>\n",
              "      <td>-0.071746</td>\n",
              "      <td>-0.189786</td>\n",
              "      <td>-0.165564</td>\n",
              "      <td>-0.136253</td>\n",
              "      <td>-0.123001</td>\n",
              "      <td>-0.106080</td>\n",
              "      <td>-0.086932</td>\n",
              "      <td>-0.064792</td>\n",
              "      <td>-0.097773</td>\n",
              "      <td>-0.127713</td>\n",
              "      <td>-0.135444</td>\n",
              "      <td>-0.138317</td>\n",
              "      <td>-0.155684</td>\n",
              "      <td>-0.168683</td>\n",
              "      <td>-0.265832</td>\n",
              "      <td>-0.352515</td>\n",
              "      <td>-0.325986</td>\n",
              "      <td>-0.291400</td>\n",
              "      <td>-0.253287</td>\n",
              "      <td>-0.208625</td>\n",
              "      <td>-0.174353</td>\n",
              "      <td>-0.134716</td>\n",
              "      <td>-0.171863</td>\n",
              "      <td>-0.203240</td>\n",
              "      <td>-0.132395</td>\n",
              "      <td>-0.059561</td>\n",
              "      <td>-0.021109</td>\n",
              "      <td>0.017668</td>\n",
              "      <td>0.106326</td>\n",
              "      <td>0.193939</td>\n",
              "      <td>0.140156</td>\n",
              "      <td>0.080640</td>\n",
              "      <td>0.244834</td>\n",
              "      <td>0.394786</td>\n",
              "      <td>0.496506</td>\n",
              "      <td>0.548397</td>\n",
              "      <td>0.592200</td>\n",
              "      <td>0.601048</td>\n",
              "      <td>0.599971</td>\n",
              "      <td>0.578221</td>\n",
              "      <td>0.522559</td>\n",
              "      <td>0.449076</td>\n",
              "      <td>0.316198</td>\n",
              "      <td>0.145808</td>\n",
              "      <td>0.049845</td>\n",
              "      <td>-0.063265</td>\n",
              "      <td>-0.165370</td>\n",
              "      <td>-0.254513</td>\n",
              "      <td>-0.312589</td>\n",
              "      <td>-0.352722</td>\n",
              "      <td>-0.430337</td>\n",
              "      <td>-0.488964</td>\n",
              "      <td>-0.533568</td>\n",
              "      <td>-0.566144</td>\n",
              "      <td>-0.593462</td>\n",
              "      <td>-0.612848</td>\n",
              "      <td>-0.588206</td>\n",
              "      <td>-0.552358</td>\n",
              "      <td>-0.524180</td>\n",
              "      <td>-0.484681</td>\n",
              "      <td>-0.463952</td>\n",
              "      <td>-0.432172</td>\n",
              "      <td>-0.431502</td>\n",
              "      <td>-0.423068</td>\n",
              "      <td>-0.430992</td>\n",
              "      <td>-0.431114</td>\n",
              "      <td>-0.391401</td>\n",
              "      <td>-0.341720</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055897</td>\n",
              "      <td>-0.055899</td>\n",
              "      <td>-0.056036</td>\n",
              "      <td>-0.995152</td>\n",
              "      <td>-1.049842</td>\n",
              "      <td>-1.059694</td>\n",
              "      <td>-0.876879</td>\n",
              "      <td>-0.671862</td>\n",
              "      <td>-0.461239</td>\n",
              "      <td>-0.219681</td>\n",
              "      <td>-0.083858</td>\n",
              "      <td>0.072528</td>\n",
              "      <td>0.242886</td>\n",
              "      <td>0.414444</td>\n",
              "      <td>0.538615</td>\n",
              "      <td>0.640832</td>\n",
              "      <td>0.693612</td>\n",
              "      <td>0.719211</td>\n",
              "      <td>0.759267</td>\n",
              "      <td>0.775610</td>\n",
              "      <td>0.672807</td>\n",
              "      <td>0.555011</td>\n",
              "      <td>0.499497</td>\n",
              "      <td>0.433544</td>\n",
              "      <td>0.429681</td>\n",
              "      <td>0.417899</td>\n",
              "      <td>0.371379</td>\n",
              "      <td>0.314347</td>\n",
              "      <td>0.308202</td>\n",
              "      <td>0.295222</td>\n",
              "      <td>0.303442</td>\n",
              "      <td>0.303253</td>\n",
              "      <td>0.193100</td>\n",
              "      <td>0.076472</td>\n",
              "      <td>0.121131</td>\n",
              "      <td>0.163870</td>\n",
              "      <td>0.179890</td>\n",
              "      <td>0.192081</td>\n",
              "      <td>0.147924</td>\n",
              "      <td>0.097252</td>\n",
              "      <td>0.078823</td>\n",
              "      <td>0.057794</td>\n",
              "      <td>0.081978</td>\n",
              "      <td>0.104101</td>\n",
              "      <td>0.128234</td>\n",
              "      <td>0.148866</td>\n",
              "      <td>0.153729</td>\n",
              "      <td>0.153465</td>\n",
              "      <td>0.023387</td>\n",
              "      <td>-0.110544</td>\n",
              "      <td>-0.164674</td>\n",
              "      <td>-0.212114</td>\n",
              "      <td>-0.268795</td>\n",
              "      <td>-0.317390</td>\n",
              "      <td>-0.334714</td>\n",
              "      <td>-0.340292</td>\n",
              "      <td>-0.336264</td>\n",
              "      <td>-0.323754</td>\n",
              "      <td>-0.282177</td>\n",
              "      <td>-0.233319</td>\n",
              "      <td>-0.207958</td>\n",
              "      <td>-0.176395</td>\n",
              "      <td>-0.175011</td>\n",
              "      <td>-0.168389</td>\n",
              "      <td>-0.148824</td>\n",
              "      <td>-0.125529</td>\n",
              "      <td>-0.107894</td>\n",
              "      <td>-0.087825</td>\n",
              "      <td>-0.065179</td>\n",
              "      <td>-0.040657</td>\n",
              "      <td>-0.034924</td>\n",
              "      <td>-0.028016</td>\n",
              "      <td>0.069571</td>\n",
              "      <td>0.162322</td>\n",
              "      <td>0.533351</td>\n",
              "      <td>0.807033</td>\n",
              "      <td>0.905874</td>\n",
              "      <td>0.946753</td>\n",
              "      <td>0.804837</td>\n",
              "      <td>0.643346</td>\n",
              "      <td>0.438834</td>\n",
              "      <td>0.206953</td>\n",
              "      <td>0.113523</td>\n",
              "      <td>-0.002207</td>\n",
              "      <td>-0.054296</td>\n",
              "      <td>-0.110161</td>\n",
              "      <td>-0.296514</td>\n",
              "      <td>-0.459548</td>\n",
              "      <td>-0.541077</td>\n",
              "      <td>-0.593285</td>\n",
              "      <td>-0.593491</td>\n",
              "      <td>-0.575759</td>\n",
              "      <td>-0.519580</td>\n",
              "      <td>-0.456263</td>\n",
              "      <td>-0.518367</td>\n",
              "      <td>-0.574085</td>\n",
              "      <td>-0.671875</td>\n",
              "      <td>-0.761205</td>\n",
              "      <td>-0.715704</td>\n",
              "      <td>-0.654415</td>\n",
              "      <td>-0.605921</td>\n",
              "      <td>-0.542007</td>\n",
              "      <td>-0.475091</td>\n",
              "      <td>-0.396057</td>\n",
              "      <td>-0.298474</td>\n",
              "      <td>-0.188902</td>\n",
              "      <td>-0.122920</td>\n",
              "      <td>-0.051221</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055862</td>\n",
              "      <td>-0.055863</td>\n",
              "      <td>-0.055944</td>\n",
              "      <td>-0.586203</td>\n",
              "      <td>-0.621645</td>\n",
              "      <td>-0.630236</td>\n",
              "      <td>-0.582622</td>\n",
              "      <td>-0.519540</td>\n",
              "      <td>-0.386305</td>\n",
              "      <td>-0.231234</td>\n",
              "      <td>-0.107908</td>\n",
              "      <td>0.035299</td>\n",
              "      <td>0.157250</td>\n",
              "      <td>0.280811</td>\n",
              "      <td>0.278464</td>\n",
              "      <td>0.264531</td>\n",
              "      <td>0.253863</td>\n",
              "      <td>0.234533</td>\n",
              "      <td>0.313482</td>\n",
              "      <td>0.379435</td>\n",
              "      <td>0.416236</td>\n",
              "      <td>0.440602</td>\n",
              "      <td>0.366124</td>\n",
              "      <td>0.284517</td>\n",
              "      <td>0.358094</td>\n",
              "      <td>0.426753</td>\n",
              "      <td>0.493362</td>\n",
              "      <td>0.547843</td>\n",
              "      <td>0.461737</td>\n",
              "      <td>0.363400</td>\n",
              "      <td>0.313753</td>\n",
              "      <td>0.255029</td>\n",
              "      <td>0.307320</td>\n",
              "      <td>0.350229</td>\n",
              "      <td>0.282082</td>\n",
              "      <td>0.205729</td>\n",
              "      <td>0.252531</td>\n",
              "      <td>0.294914</td>\n",
              "      <td>0.237827</td>\n",
              "      <td>0.170709</td>\n",
              "      <td>0.109975</td>\n",
              "      <td>0.045517</td>\n",
              "      <td>0.003642</td>\n",
              "      <td>-0.038868</td>\n",
              "      <td>-0.078666</td>\n",
              "      <td>-0.116529</td>\n",
              "      <td>-0.141241</td>\n",
              "      <td>-0.161329</td>\n",
              "      <td>-0.202707</td>\n",
              "      <td>-0.237168</td>\n",
              "      <td>-0.308661</td>\n",
              "      <td>-0.368142</td>\n",
              "      <td>-0.314251</td>\n",
              "      <td>-0.253363</td>\n",
              "      <td>-0.190688</td>\n",
              "      <td>-0.122311</td>\n",
              "      <td>-0.088549</td>\n",
              "      <td>-0.052796</td>\n",
              "      <td>0.042830</td>\n",
              "      <td>0.139156</td>\n",
              "      <td>0.166951</td>\n",
              "      <td>0.190379</td>\n",
              "      <td>0.122998</td>\n",
              "      <td>0.053016</td>\n",
              "      <td>-0.017389</td>\n",
              "      <td>-0.085548</td>\n",
              "      <td>-0.147250</td>\n",
              "      <td>-0.205261</td>\n",
              "      <td>-0.203194</td>\n",
              "      <td>-0.196380</td>\n",
              "      <td>-0.251689</td>\n",
              "      <td>-0.301747</td>\n",
              "      <td>-0.147119</td>\n",
              "      <td>0.012147</td>\n",
              "      <td>0.271560</td>\n",
              "      <td>0.469608</td>\n",
              "      <td>0.580011</td>\n",
              "      <td>0.646652</td>\n",
              "      <td>0.668259</td>\n",
              "      <td>0.665517</td>\n",
              "      <td>0.666712</td>\n",
              "      <td>0.650845</td>\n",
              "      <td>0.597461</td>\n",
              "      <td>0.511388</td>\n",
              "      <td>0.291532</td>\n",
              "      <td>0.020334</td>\n",
              "      <td>-0.172656</td>\n",
              "      <td>-0.350064</td>\n",
              "      <td>-0.510096</td>\n",
              "      <td>-0.634887</td>\n",
              "      <td>-0.710786</td>\n",
              "      <td>-0.759362</td>\n",
              "      <td>-0.623256</td>\n",
              "      <td>-0.481743</td>\n",
              "      <td>-0.382711</td>\n",
              "      <td>-0.276907</td>\n",
              "      <td>-0.362880</td>\n",
              "      <td>-0.445428</td>\n",
              "      <td>-0.540571</td>\n",
              "      <td>-0.629918</td>\n",
              "      <td>-0.585123</td>\n",
              "      <td>-0.525531</td>\n",
              "      <td>-0.428659</td>\n",
              "      <td>-0.318883</td>\n",
              "      <td>-0.321564</td>\n",
              "      <td>-0.318218</td>\n",
              "      <td>-0.331974</td>\n",
              "      <td>-0.339568</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055874</td>\n",
              "      <td>-0.055875</td>\n",
              "      <td>-0.055998</td>\n",
              "      <td>-0.888690</td>\n",
              "      <td>-0.898666</td>\n",
              "      <td>-0.873900</td>\n",
              "      <td>-0.676354</td>\n",
              "      <td>-0.462231</td>\n",
              "      <td>-0.257961</td>\n",
              "      <td>-0.028230</td>\n",
              "      <td>0.115674</td>\n",
              "      <td>0.273566</td>\n",
              "      <td>0.339350</td>\n",
              "      <td>0.394498</td>\n",
              "      <td>0.515151</td>\n",
              "      <td>0.614811</td>\n",
              "      <td>0.670677</td>\n",
              "      <td>0.700054</td>\n",
              "      <td>0.753588</td>\n",
              "      <td>0.782879</td>\n",
              "      <td>0.692752</td>\n",
              "      <td>0.586693</td>\n",
              "      <td>0.518046</td>\n",
              "      <td>0.438742</td>\n",
              "      <td>0.419959</td>\n",
              "      <td>0.393108</td>\n",
              "      <td>0.348790</td>\n",
              "      <td>0.294590</td>\n",
              "      <td>0.177651</td>\n",
              "      <td>0.053835</td>\n",
              "      <td>0.023168</td>\n",
              "      <td>-0.008346</td>\n",
              "      <td>-0.036283</td>\n",
              "      <td>-0.063201</td>\n",
              "      <td>-0.113152</td>\n",
              "      <td>-0.161487</td>\n",
              "      <td>-0.187240</td>\n",
              "      <td>-0.209363</td>\n",
              "      <td>-0.269922</td>\n",
              "      <td>-0.323085</td>\n",
              "      <td>-0.280467</td>\n",
              "      <td>-0.228662</td>\n",
              "      <td>-0.156163</td>\n",
              "      <td>-0.078237</td>\n",
              "      <td>0.019506</td>\n",
              "      <td>0.117520</td>\n",
              "      <td>0.160118</td>\n",
              "      <td>0.197537</td>\n",
              "      <td>0.145629</td>\n",
              "      <td>0.086903</td>\n",
              "      <td>-0.054248</td>\n",
              "      <td>-0.191029</td>\n",
              "      <td>-0.156741</td>\n",
              "      <td>-0.119114</td>\n",
              "      <td>-0.019527</td>\n",
              "      <td>0.079373</td>\n",
              "      <td>0.083105</td>\n",
              "      <td>0.084704</td>\n",
              "      <td>0.049020</td>\n",
              "      <td>0.011567</td>\n",
              "      <td>-0.131296</td>\n",
              "      <td>-0.272257</td>\n",
              "      <td>-0.247414</td>\n",
              "      <td>-0.215540</td>\n",
              "      <td>-0.076945</td>\n",
              "      <td>0.060380</td>\n",
              "      <td>0.117141</td>\n",
              "      <td>0.170932</td>\n",
              "      <td>0.128652</td>\n",
              "      <td>0.082708</td>\n",
              "      <td>-0.008870</td>\n",
              "      <td>-0.103240</td>\n",
              "      <td>-0.035587</td>\n",
              "      <td>0.032719</td>\n",
              "      <td>0.416030</td>\n",
              "      <td>0.707690</td>\n",
              "      <td>0.862805</td>\n",
              "      <td>0.954109</td>\n",
              "      <td>0.831716</td>\n",
              "      <td>0.687687</td>\n",
              "      <td>0.419905</td>\n",
              "      <td>0.120267</td>\n",
              "      <td>-0.080879</td>\n",
              "      <td>-0.317854</td>\n",
              "      <td>-0.473808</td>\n",
              "      <td>-0.619908</td>\n",
              "      <td>-0.690989</td>\n",
              "      <td>-0.716340</td>\n",
              "      <td>-0.740520</td>\n",
              "      <td>-0.732559</td>\n",
              "      <td>-0.737243</td>\n",
              "      <td>-0.719303</td>\n",
              "      <td>-0.629015</td>\n",
              "      <td>-0.531109</td>\n",
              "      <td>-0.497373</td>\n",
              "      <td>-0.456183</td>\n",
              "      <td>-0.381934</td>\n",
              "      <td>-0.298400</td>\n",
              "      <td>-0.324028</td>\n",
              "      <td>-0.344695</td>\n",
              "      <td>-0.341876</td>\n",
              "      <td>-0.331490</td>\n",
              "      <td>-0.216397</td>\n",
              "      <td>-0.091219</td>\n",
              "      <td>-0.103713</td>\n",
              "      <td>-0.115007</td>\n",
              "      <td>-0.064554</td>\n",
              "      <td>-0.010336</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055864</td>\n",
              "      <td>-0.055865</td>\n",
              "      <td>-0.055938</td>\n",
              "      <td>-0.527057</td>\n",
              "      <td>-0.570164</td>\n",
              "      <td>-0.587594</td>\n",
              "      <td>-0.465072</td>\n",
              "      <td>-0.331023</td>\n",
              "      <td>-0.306588</td>\n",
              "      <td>-0.272496</td>\n",
              "      <td>-0.221035</td>\n",
              "      <td>-0.154570</td>\n",
              "      <td>-0.041583</td>\n",
              "      <td>0.081359</td>\n",
              "      <td>0.193788</td>\n",
              "      <td>0.298558</td>\n",
              "      <td>0.350587</td>\n",
              "      <td>0.387791</td>\n",
              "      <td>0.405289</td>\n",
              "      <td>0.410329</td>\n",
              "      <td>0.408983</td>\n",
              "      <td>0.396598</td>\n",
              "      <td>0.393505</td>\n",
              "      <td>0.381557</td>\n",
              "      <td>0.399632</td>\n",
              "      <td>0.410816</td>\n",
              "      <td>0.463544</td>\n",
              "      <td>0.504736</td>\n",
              "      <td>0.527013</td>\n",
              "      <td>0.538452</td>\n",
              "      <td>0.475918</td>\n",
              "      <td>0.399700</td>\n",
              "      <td>0.374341</td>\n",
              "      <td>0.337193</td>\n",
              "      <td>0.273511</td>\n",
              "      <td>0.201923</td>\n",
              "      <td>0.173019</td>\n",
              "      <td>0.138687</td>\n",
              "      <td>0.093777</td>\n",
              "      <td>0.044200</td>\n",
              "      <td>-0.090438</td>\n",
              "      <td>-0.222524</td>\n",
              "      <td>-0.343125</td>\n",
              "      <td>-0.455343</td>\n",
              "      <td>-0.356291</td>\n",
              "      <td>-0.246091</td>\n",
              "      <td>-0.119943</td>\n",
              "      <td>0.010758</td>\n",
              "      <td>0.066658</td>\n",
              "      <td>0.121242</td>\n",
              "      <td>0.086492</td>\n",
              "      <td>0.049338</td>\n",
              "      <td>0.000770</td>\n",
              "      <td>-0.046825</td>\n",
              "      <td>-0.076928</td>\n",
              "      <td>-0.103976</td>\n",
              "      <td>-0.098828</td>\n",
              "      <td>-0.091216</td>\n",
              "      <td>-0.138761</td>\n",
              "      <td>-0.183930</td>\n",
              "      <td>-0.255215</td>\n",
              "      <td>-0.320187</td>\n",
              "      <td>-0.340804</td>\n",
              "      <td>-0.350843</td>\n",
              "      <td>-0.273072</td>\n",
              "      <td>-0.189497</td>\n",
              "      <td>-0.151286</td>\n",
              "      <td>-0.109720</td>\n",
              "      <td>-0.109831</td>\n",
              "      <td>-0.107396</td>\n",
              "      <td>-0.033881</td>\n",
              "      <td>0.043028</td>\n",
              "      <td>0.185351</td>\n",
              "      <td>0.316612</td>\n",
              "      <td>0.468388</td>\n",
              "      <td>0.563813</td>\n",
              "      <td>0.636895</td>\n",
              "      <td>0.668718</td>\n",
              "      <td>0.631936</td>\n",
              "      <td>0.575449</td>\n",
              "      <td>0.462860</td>\n",
              "      <td>0.329509</td>\n",
              "      <td>0.250846</td>\n",
              "      <td>0.147591</td>\n",
              "      <td>0.076372</td>\n",
              "      <td>-0.010251</td>\n",
              "      <td>-0.093553</td>\n",
              "      <td>-0.168916</td>\n",
              "      <td>-0.322270</td>\n",
              "      <td>-0.448586</td>\n",
              "      <td>-0.612011</td>\n",
              "      <td>-0.744340</td>\n",
              "      <td>-0.747502</td>\n",
              "      <td>-0.736538</td>\n",
              "      <td>-0.737192</td>\n",
              "      <td>-0.727520</td>\n",
              "      <td>-0.635425</td>\n",
              "      <td>-0.528967</td>\n",
              "      <td>-0.488808</td>\n",
              "      <td>-0.437436</td>\n",
              "      <td>-0.362675</td>\n",
              "      <td>-0.276572</td>\n",
              "      <td>-0.243877</td>\n",
              "      <td>-0.205051</td>\n",
              "      <td>-0.217156</td>\n",
              "      <td>-0.225850</td>\n",
              "      <td>-0.168551</td>\n",
              "      <td>-0.105017</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055836</td>\n",
              "      <td>-0.055837</td>\n",
              "      <td>-0.055856</td>\n",
              "      <td>-0.136697</td>\n",
              "      <td>-0.295595</td>\n",
              "      <td>-0.427690</td>\n",
              "      <td>-0.513475</td>\n",
              "      <td>-0.584390</td>\n",
              "      <td>-0.602339</td>\n",
              "      <td>-0.607536</td>\n",
              "      <td>-0.526568</td>\n",
              "      <td>-0.415174</td>\n",
              "      <td>-0.325069</td>\n",
              "      <td>-0.213829</td>\n",
              "      <td>-0.037797</td>\n",
              "      <td>0.140432</td>\n",
              "      <td>0.259846</td>\n",
              "      <td>0.364802</td>\n",
              "      <td>0.487632</td>\n",
              "      <td>0.590244</td>\n",
              "      <td>0.552227</td>\n",
              "      <td>0.500447</td>\n",
              "      <td>0.491548</td>\n",
              "      <td>0.471667</td>\n",
              "      <td>0.439402</td>\n",
              "      <td>0.398420</td>\n",
              "      <td>0.318068</td>\n",
              "      <td>0.228133</td>\n",
              "      <td>0.214426</td>\n",
              "      <td>0.195719</td>\n",
              "      <td>0.210642</td>\n",
              "      <td>0.219789</td>\n",
              "      <td>0.198764</td>\n",
              "      <td>0.171449</td>\n",
              "      <td>0.217321</td>\n",
              "      <td>0.259003</td>\n",
              "      <td>0.305539</td>\n",
              "      <td>0.346330</td>\n",
              "      <td>0.366552</td>\n",
              "      <td>0.374757</td>\n",
              "      <td>0.327002</td>\n",
              "      <td>0.268544</td>\n",
              "      <td>0.156136</td>\n",
              "      <td>0.037796</td>\n",
              "      <td>-0.078666</td>\n",
              "      <td>-0.193848</td>\n",
              "      <td>-0.198744</td>\n",
              "      <td>-0.197006</td>\n",
              "      <td>-0.142127</td>\n",
              "      <td>-0.080498</td>\n",
              "      <td>-0.177665</td>\n",
              "      <td>-0.266935</td>\n",
              "      <td>-0.332222</td>\n",
              "      <td>-0.387613</td>\n",
              "      <td>-0.253308</td>\n",
              "      <td>-0.112125</td>\n",
              "      <td>-0.003236</td>\n",
              "      <td>0.104925</td>\n",
              "      <td>0.096482</td>\n",
              "      <td>0.085651</td>\n",
              "      <td>0.073486</td>\n",
              "      <td>0.059090</td>\n",
              "      <td>0.092567</td>\n",
              "      <td>0.122718</td>\n",
              "      <td>0.106859</td>\n",
              "      <td>0.088366</td>\n",
              "      <td>0.085858</td>\n",
              "      <td>0.081362</td>\n",
              "      <td>0.042392</td>\n",
              "      <td>0.001813</td>\n",
              "      <td>-0.064104</td>\n",
              "      <td>-0.130404</td>\n",
              "      <td>-0.274583</td>\n",
              "      <td>-0.403408</td>\n",
              "      <td>-0.380007</td>\n",
              "      <td>-0.335419</td>\n",
              "      <td>-0.169232</td>\n",
              "      <td>-0.018278</td>\n",
              "      <td>0.233828</td>\n",
              "      <td>0.463212</td>\n",
              "      <td>0.633950</td>\n",
              "      <td>0.801799</td>\n",
              "      <td>0.795173</td>\n",
              "      <td>0.757485</td>\n",
              "      <td>0.624587</td>\n",
              "      <td>0.430170</td>\n",
              "      <td>0.233268</td>\n",
              "      <td>0.026166</td>\n",
              "      <td>-0.075386</td>\n",
              "      <td>-0.164613</td>\n",
              "      <td>-0.230142</td>\n",
              "      <td>-0.283663</td>\n",
              "      <td>-0.414259</td>\n",
              "      <td>-0.531109</td>\n",
              "      <td>-0.564393</td>\n",
              "      <td>-0.590236</td>\n",
              "      <td>-0.580750</td>\n",
              "      <td>-0.560712</td>\n",
              "      <td>-0.576806</td>\n",
              "      <td>-0.582672</td>\n",
              "      <td>-0.577889</td>\n",
              "      <td>-0.560312</td>\n",
              "      <td>-0.581222</td>\n",
              "      <td>-0.592851</td>\n",
              "      <td>-0.538411</td>\n",
              "      <td>-0.470114</td>\n",
              "      <td>-0.404135</td>\n",
              "      <td>-0.326657</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055851</td>\n",
              "      <td>-0.055853</td>\n",
              "      <td>-0.055945</td>\n",
              "      <td>-0.669006</td>\n",
              "      <td>-0.768736</td>\n",
              "      <td>-0.829736</td>\n",
              "      <td>-0.776232</td>\n",
              "      <td>-0.702025</td>\n",
              "      <td>-0.679664</td>\n",
              "      <td>-0.638895</td>\n",
              "      <td>-0.504299</td>\n",
              "      <td>-0.333270</td>\n",
              "      <td>-0.132141</td>\n",
              "      <td>0.089337</td>\n",
              "      <td>0.273363</td>\n",
              "      <td>0.446677</td>\n",
              "      <td>0.454293</td>\n",
              "      <td>0.445262</td>\n",
              "      <td>0.520758</td>\n",
              "      <td>0.577522</td>\n",
              "      <td>0.591212</td>\n",
              "      <td>0.588454</td>\n",
              "      <td>0.576342</td>\n",
              "      <td>0.551379</td>\n",
              "      <td>0.582575</td>\n",
              "      <td>0.603832</td>\n",
              "      <td>0.592756</td>\n",
              "      <td>0.565804</td>\n",
              "      <td>0.449785</td>\n",
              "      <td>0.321019</td>\n",
              "      <td>0.265947</td>\n",
              "      <td>0.203096</td>\n",
              "      <td>0.127967</td>\n",
              "      <td>0.048537</td>\n",
              "      <td>-0.016010</td>\n",
              "      <td>-0.081575</td>\n",
              "      <td>-0.052756</td>\n",
              "      <td>-0.021495</td>\n",
              "      <td>0.035545</td>\n",
              "      <td>0.093172</td>\n",
              "      <td>0.133858</td>\n",
              "      <td>0.170330</td>\n",
              "      <td>0.174937</td>\n",
              "      <td>0.174549</td>\n",
              "      <td>0.197904</td>\n",
              "      <td>0.215737</td>\n",
              "      <td>0.171832</td>\n",
              "      <td>0.121986</td>\n",
              "      <td>0.087212</td>\n",
              "      <td>0.048272</td>\n",
              "      <td>-0.003365</td>\n",
              "      <td>-0.053978</td>\n",
              "      <td>-0.149341</td>\n",
              "      <td>-0.238906</td>\n",
              "      <td>-0.325321</td>\n",
              "      <td>-0.399371</td>\n",
              "      <td>-0.413354</td>\n",
              "      <td>-0.416769</td>\n",
              "      <td>-0.296621</td>\n",
              "      <td>-0.167467</td>\n",
              "      <td>-0.103991</td>\n",
              "      <td>-0.036771</td>\n",
              "      <td>0.045348</td>\n",
              "      <td>0.124768</td>\n",
              "      <td>0.137664</td>\n",
              "      <td>0.146338</td>\n",
              "      <td>0.088885</td>\n",
              "      <td>0.029611</td>\n",
              "      <td>0.029199</td>\n",
              "      <td>0.028104</td>\n",
              "      <td>-0.046387</td>\n",
              "      <td>-0.122046</td>\n",
              "      <td>-0.088698</td>\n",
              "      <td>-0.051627</td>\n",
              "      <td>0.149391</td>\n",
              "      <td>0.306890</td>\n",
              "      <td>0.394731</td>\n",
              "      <td>0.450998</td>\n",
              "      <td>0.486641</td>\n",
              "      <td>0.503396</td>\n",
              "      <td>0.479605</td>\n",
              "      <td>0.441603</td>\n",
              "      <td>0.386514</td>\n",
              "      <td>0.306307</td>\n",
              "      <td>0.204092</td>\n",
              "      <td>0.073348</td>\n",
              "      <td>-0.070654</td>\n",
              "      <td>-0.206738</td>\n",
              "      <td>-0.253530</td>\n",
              "      <td>-0.285799</td>\n",
              "      <td>-0.280411</td>\n",
              "      <td>-0.266972</td>\n",
              "      <td>-0.299063</td>\n",
              "      <td>-0.324088</td>\n",
              "      <td>-0.410165</td>\n",
              "      <td>-0.491715</td>\n",
              "      <td>-0.532703</td>\n",
              "      <td>-0.565724</td>\n",
              "      <td>-0.575943</td>\n",
              "      <td>-0.575673</td>\n",
              "      <td>-0.617677</td>\n",
              "      <td>-0.648180</td>\n",
              "      <td>-0.602069</td>\n",
              "      <td>-0.542688</td>\n",
              "      <td>-0.510301</td>\n",
              "      <td>-0.466009</td>\n",
              "      <td>-0.456133</td>\n",
              "      <td>-0.436401</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.245676</td>\n",
              "      <td>-0.055846</td>\n",
              "      <td>-0.055847</td>\n",
              "      <td>-0.055884</td>\n",
              "      <td>-0.261747</td>\n",
              "      <td>-0.379763</td>\n",
              "      <td>-0.471854</td>\n",
              "      <td>-0.564182</td>\n",
              "      <td>-0.640191</td>\n",
              "      <td>-0.706768</td>\n",
              "      <td>-0.762678</td>\n",
              "      <td>-0.701157</td>\n",
              "      <td>-0.605043</td>\n",
              "      <td>-0.569181</td>\n",
              "      <td>-0.505029</td>\n",
              "      <td>-0.376504</td>\n",
              "      <td>-0.231866</td>\n",
              "      <td>-0.047281</td>\n",
              "      <td>0.131083</td>\n",
              "      <td>0.260480</td>\n",
              "      <td>0.375800</td>\n",
              "      <td>0.479698</td>\n",
              "      <td>0.567332</td>\n",
              "      <td>0.578108</td>\n",
              "      <td>0.575639</td>\n",
              "      <td>0.620578</td>\n",
              "      <td>0.655185</td>\n",
              "      <td>0.682210</td>\n",
              "      <td>0.691532</td>\n",
              "      <td>0.646532</td>\n",
              "      <td>0.586361</td>\n",
              "      <td>0.480605</td>\n",
              "      <td>0.360750</td>\n",
              "      <td>0.285608</td>\n",
              "      <td>0.201246</td>\n",
              "      <td>0.201131</td>\n",
              "      <td>0.196215</td>\n",
              "      <td>0.240752</td>\n",
              "      <td>0.281071</td>\n",
              "      <td>0.252129</td>\n",
              "      <td>0.213559</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.196930</td>\n",
              "      <td>0.175981</td>\n",
              "      <td>0.149685</td>\n",
              "      <td>0.075453</td>\n",
              "      <td>-0.001594</td>\n",
              "      <td>0.044047</td>\n",
              "      <td>0.088408</td>\n",
              "      <td>0.120748</td>\n",
              "      <td>0.149142</td>\n",
              "      <td>0.064840</td>\n",
              "      <td>-0.020242</td>\n",
              "      <td>-0.157798</td>\n",
              "      <td>-0.288475</td>\n",
              "      <td>-0.323234</td>\n",
              "      <td>-0.346404</td>\n",
              "      <td>-0.348599</td>\n",
              "      <td>-0.341952</td>\n",
              "      <td>-0.382258</td>\n",
              "      <td>-0.414412</td>\n",
              "      <td>-0.279369</td>\n",
              "      <td>-0.134716</td>\n",
              "      <td>-0.090015</td>\n",
              "      <td>-0.043336</td>\n",
              "      <td>0.027792</td>\n",
              "      <td>0.096362</td>\n",
              "      <td>0.083839</td>\n",
              "      <td>0.069420</td>\n",
              "      <td>0.039348</td>\n",
              "      <td>0.007880</td>\n",
              "      <td>-0.010954</td>\n",
              "      <td>-0.030106</td>\n",
              "      <td>-0.086573</td>\n",
              "      <td>-0.138029</td>\n",
              "      <td>-0.140517</td>\n",
              "      <td>-0.133306</td>\n",
              "      <td>0.039613</td>\n",
              "      <td>0.186202</td>\n",
              "      <td>0.368225</td>\n",
              "      <td>0.526952</td>\n",
              "      <td>0.655063</td>\n",
              "      <td>0.776391</td>\n",
              "      <td>0.791864</td>\n",
              "      <td>0.780669</td>\n",
              "      <td>0.658974</td>\n",
              "      <td>0.475028</td>\n",
              "      <td>0.256166</td>\n",
              "      <td>0.026166</td>\n",
              "      <td>-0.140254</td>\n",
              "      <td>-0.285799</td>\n",
              "      <td>-0.388887</td>\n",
              "      <td>-0.472273</td>\n",
              "      <td>-0.519580</td>\n",
              "      <td>-0.554996</td>\n",
              "      <td>-0.592654</td>\n",
              "      <td>-0.622538</td>\n",
              "      <td>-0.599804</td>\n",
              "      <td>-0.565724</td>\n",
              "      <td>-0.479318</td>\n",
              "      <td>-0.379691</td>\n",
              "      <td>-0.396132</td>\n",
              "      <td>-0.404713</td>\n",
              "      <td>-0.436240</td>\n",
              "      <td>-0.461655</td>\n",
              "      <td>-0.513313</td>\n",
              "      <td>-0.558378</td>\n",
              "      <td>-0.549518</td>\n",
              "      <td>-0.528930</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75eeb1d0-6029-4f22-9a19-237eb40d1a1b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-75eeb1d0-6029-4f22-9a19-237eb40d1a1b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-75eeb1d0-6029-4f22-9a19-237eb40d1a1b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-434a8e21-7549-4208-8ef1-e59abfbfdd00\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-434a8e21-7549-4208-8ef1-e59abfbfdd00')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-434a8e21-7549-4208-8ef1-e59abfbfdd00 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "PIR_data"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_Meta = Meta_data.values\n",
        "X_Meta"
      ],
      "metadata": {
        "id": "R7Qv0ix83L7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c7b0e6-8b6d-45a2-891c-3dfdc5df0cf1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.24567575,  0.95447771, -0.61361985, -1.02379551, -1.8871757 ],\n",
              "       [ 0.24567575,  0.95447771, -0.55550719, -1.02379551, -1.8871757 ],\n",
              "       [ 0.24567575,  0.95447771, -0.55550719, -1.02379551, -1.8871757 ],\n",
              "       ...,\n",
              "       [ 0.3331651 , -0.97974146,  0.25807003, -2.10198675,  1.91350377],\n",
              "       [ 0.3331651 , -0.97974146,  0.31618269, -2.10198675,  1.91350377],\n",
              "       [ 0.3331651 , -0.97974146,  0.31618269, -2.10198675,  1.91350377]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Cross Validation"
      ],
      "metadata": {
        "id": "8-HpETZu2Ysk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpSUi6etzH3-",
        "outputId": "df469584-7f19-4f6c-e307-08dc8453cf5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9983660130718954)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(f1_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "autA569OzKVj",
        "outputId": "57286319-0a38-4f6c-ad65-32a0a63ba892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9960051083315564)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Load and prepare data\n",
        "X = PIR_data.iloc[:, :-1].values  # First 109 columns: PIR time series\n",
        "y = PIR_data.iloc[:, -1].values   # Last column: class label\n",
        "\n",
        "# Reshape for LSTM (samples, timesteps, features)\n",
        "X = X.reshape((X.shape[0], 109, 1))\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Set up cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "f1_scores = []\n",
        "accuracies = []\n",
        "\n",
        "def create_plain_lstm():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(109, 1), return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(f\"\\n--- Fold {fold_no} ---\")\n",
        "\n",
        "    # Create model\n",
        "    model = create_plain_lstm()\n",
        "    model.summary()\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y_categorical[train_index], y_categorical[test_index]\n",
        "\n",
        "    # Resampling\n",
        "    ros = RandomOverSampler(random_state=42)\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "    y_train_labels = np.argmax(y_train, axis=1)\n",
        "\n",
        "    X_resampled, y_resampled = ros.fit_resample(X_train_flat, y_train_labels)\n",
        "\n",
        "    # Reshape back to 3D\n",
        "    X_resampled = X_resampled.reshape(-1, 109, 1)\n",
        "    y_resampled = to_categorical(y_resampled)\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        X_resampled, y_resampled,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
        "\n",
        "    print(f\"Fold {fold_no} - Accuracy: {acc:.4f}, F1 Macro: {f1:.4f}\")\n",
        "\n",
        "    accuracies.append(acc)\n",
        "    f1_scores.append(f1)\n",
        "    fold_no += 1\n",
        "\n",
        "# Final results\n",
        "print(\"\\n\\nFinal Results:\")\n",
        "print(f\"Average Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
        "print(f\"Average F1 Macro: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Xjm0h3ohaozY",
        "outputId": "e7489fcc-eb5e-4a63-882c-ff02d1a14750"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 1 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m66,560\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 216ms/step - accuracy: 0.8720 - loss: 0.3371 - val_accuracy: 0.9598 - val_loss: 0.1191\n",
            "Epoch 2/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 221ms/step - accuracy: 0.8753 - loss: 0.3093 - val_accuracy: 0.8507 - val_loss: 0.3111\n",
            "Epoch 3/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 215ms/step - accuracy: 0.9587 - loss: 0.1395 - val_accuracy: 0.9461 - val_loss: 0.1522\n",
            "Epoch 4/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 214ms/step - accuracy: 0.9610 - loss: 0.1410 - val_accuracy: 0.7890 - val_loss: 0.3389\n",
            "Epoch 5/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 212ms/step - accuracy: 0.9468 - loss: 0.1674 - val_accuracy: 0.9036 - val_loss: 0.2049\n",
            "Epoch 6/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 213ms/step - accuracy: 0.9529 - loss: 0.1462 - val_accuracy: 0.9788 - val_loss: 0.0654\n",
            "Epoch 7/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 222ms/step - accuracy: 0.9631 - loss: 0.1185 - val_accuracy: 0.9670 - val_loss: 0.1212\n",
            "Epoch 8/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 221ms/step - accuracy: 0.9817 - loss: 0.0602 - val_accuracy: 0.9830 - val_loss: 0.0530\n",
            "Epoch 9/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 221ms/step - accuracy: 0.9857 - loss: 0.0415 - val_accuracy: 0.9690 - val_loss: 0.0897\n",
            "Epoch 10/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 225ms/step - accuracy: 0.9802 - loss: 0.0578 - val_accuracy: 0.9771 - val_loss: 0.0693\n",
            "Epoch 11/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 223ms/step - accuracy: 0.9861 - loss: 0.0437 - val_accuracy: 0.9804 - val_loss: 0.0592\n",
            "Epoch 12/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 219ms/step - accuracy: 0.9872 - loss: 0.0386 - val_accuracy: 0.9804 - val_loss: 0.0640\n",
            "Epoch 13/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 211ms/step - accuracy: 0.9873 - loss: 0.0370 - val_accuracy: 0.9739 - val_loss: 0.0689\n",
            "Epoch 14/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 215ms/step - accuracy: 0.9875 - loss: 0.0358 - val_accuracy: 0.9781 - val_loss: 0.0568\n",
            "Epoch 15/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 222ms/step - accuracy: 0.9876 - loss: 0.0382 - val_accuracy: 0.9837 - val_loss: 0.0554\n",
            "Epoch 16/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 225ms/step - accuracy: 0.9876 - loss: 0.0395 - val_accuracy: 0.9866 - val_loss: 0.0484\n",
            "Epoch 17/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 222ms/step - accuracy: 0.9891 - loss: 0.0348 - val_accuracy: 0.9840 - val_loss: 0.0487\n",
            "Epoch 18/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 221ms/step - accuracy: 0.9887 - loss: 0.0350 - val_accuracy: 0.9739 - val_loss: 0.0729\n",
            "Epoch 19/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 228ms/step - accuracy: 0.9894 - loss: 0.0312 - val_accuracy: 0.5818 - val_loss: 1.9439\n",
            "Epoch 20/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 222ms/step - accuracy: 0.9152 - loss: 0.2146 - val_accuracy: 0.9569 - val_loss: 0.1199\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step\n",
            "Fold 1 - Accuracy: 0.9569, F1 Macro: 0.9363\n",
            "\n",
            "--- Fold 2 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m66,560\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m 95/937\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:00\u001b[0m 215ms/step - accuracy: 0.7437 - loss: 0.5893"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-cde2cc809248>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plain lstm with weighting\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load and prepare data\n",
        "X = PIR_data.iloc[:, :-1].values  # First 109 columns: PIR time series\n",
        "y = PIR_data.iloc[:, -1].values   # Last column: class label\n",
        "\n",
        "# Reshape for LSTM (samples, timesteps, features)\n",
        "X = X.reshape((X.shape[0], 109, 1))\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced',\n",
        "                                     classes=np.unique(y_encoded),\n",
        "                                     y=y_encoded)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# Set up cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "f1_scores = []\n",
        "accuracies = []\n",
        "\n",
        "def create_plain_lstm():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(109, 1), return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(f\"\\n--- Fold {fold_no} ---\")\n",
        "\n",
        "    # Create model\n",
        "    model = create_plain_lstm()\n",
        "    model.summary()\n",
        "\n",
        "    # Split data\n",
        "    X_resampled, X_test = X[train_index], X[test_index]\n",
        "    y_resampled, y_test = y_categorical[train_index], y_categorical[test_index]\n",
        "\n",
        "\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        X_resampled, y_resampled,\n",
        "        validation_data=(X_test, y_test),\n",
        "        class_weight=class_weights,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
        "\n",
        "    print(f\"Fold {fold_no} - Accuracy: {acc:.4f}, F1 Macro: {f1:.4f}\")\n",
        "\n",
        "    accuracies.append(acc)\n",
        "    f1_scores.append(f1)\n",
        "    fold_no += 1\n",
        "\n",
        "# Final results\n",
        "print(\"\\n\\nFinal Results:\")\n",
        "print(f\"Average Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
        "print(f\"Average F1 Macro: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1ZEmmejKq30t",
        "outputId": "eec668de-c518-437c-f738-3cc712fb7ef2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 1 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m66,560\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 232ms/step - accuracy: 0.8613 - loss: 0.5142 - val_accuracy: 0.9454 - val_loss: 0.2093\n",
            "Epoch 2/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 217ms/step - accuracy: 0.7816 - loss: 0.6022 - val_accuracy: 0.5992 - val_loss: 0.7225\n",
            "Epoch 3/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 229ms/step - accuracy: 0.6226 - loss: 0.7807 - val_accuracy: 0.6001 - val_loss: 0.7078\n",
            "Epoch 4/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 232ms/step - accuracy: 0.6415 - loss: 0.7641 - val_accuracy: 0.6230 - val_loss: 0.6508\n",
            "Epoch 5/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 232ms/step - accuracy: 0.6334 - loss: 0.5964 - val_accuracy: 0.1875 - val_loss: 0.7126\n",
            "Epoch 6/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 233ms/step - accuracy: 0.6093 - loss: 0.4996 - val_accuracy: 0.9788 - val_loss: 0.0744\n",
            "Epoch 7/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 239ms/step - accuracy: 0.9589 - loss: 0.1236 - val_accuracy: 0.9748 - val_loss: 0.0838\n",
            "Epoch 8/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 232ms/step - accuracy: 0.9721 - loss: 0.0781 - val_accuracy: 0.9788 - val_loss: 0.0648\n",
            "Epoch 9/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 226ms/step - accuracy: 0.9729 - loss: 0.0897 - val_accuracy: 0.9837 - val_loss: 0.0536\n",
            "Epoch 10/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 325ms/step - accuracy: 0.9751 - loss: 0.0898 - val_accuracy: 0.9758 - val_loss: 0.0725\n",
            "Epoch 11/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 346ms/step - accuracy: 0.9660 - loss: 0.1067 - val_accuracy: 0.9719 - val_loss: 0.0828\n",
            "Epoch 12/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 243ms/step - accuracy: 0.9711 - loss: 0.0646 - val_accuracy: 0.9840 - val_loss: 0.0487\n",
            "Epoch 13/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 221ms/step - accuracy: 0.9714 - loss: 0.0747 - val_accuracy: 0.9807 - val_loss: 0.0554\n",
            "Epoch 14/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 217ms/step - accuracy: 0.9717 - loss: 0.0663 - val_accuracy: 0.9605 - val_loss: 0.1122\n",
            "Epoch 15/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 226ms/step - accuracy: 0.9753 - loss: 0.0596 - val_accuracy: 0.9856 - val_loss: 0.0409\n",
            "Epoch 16/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 227ms/step - accuracy: 0.9767 - loss: 0.0547 - val_accuracy: 0.9856 - val_loss: 0.0501\n",
            "Epoch 17/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 230ms/step - accuracy: 0.9789 - loss: 0.0492 - val_accuracy: 0.9853 - val_loss: 0.0455\n",
            "Epoch 18/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 229ms/step - accuracy: 0.9764 - loss: 0.0436 - val_accuracy: 0.9667 - val_loss: 0.0935\n",
            "Epoch 19/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 235ms/step - accuracy: 0.9757 - loss: 0.0505 - val_accuracy: 0.9837 - val_loss: 0.0532\n",
            "Epoch 20/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 230ms/step - accuracy: 0.9765 - loss: 0.0458 - val_accuracy: 0.9824 - val_loss: 0.0585\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step\n",
            "Fold 1 - Accuracy: 0.9824, F1 Macro: 0.9717\n",
            "\n",
            "--- Fold 2 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m66,560\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 230ms/step - accuracy: 0.8731 - loss: 0.4824 - val_accuracy: 0.9252 - val_loss: 0.2068\n",
            "Epoch 2/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 227ms/step - accuracy: 0.9732 - loss: 0.1030 - val_accuracy: 0.9585 - val_loss: 0.1302\n",
            "Epoch 3/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 229ms/step - accuracy: 0.9726 - loss: 0.0872 - val_accuracy: 0.9748 - val_loss: 0.1058\n",
            "Epoch 4/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 229ms/step - accuracy: 0.9730 - loss: 0.0855 - val_accuracy: 0.9755 - val_loss: 0.0809\n",
            "Epoch 5/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 233ms/step - accuracy: 0.9759 - loss: 0.0864 - val_accuracy: 0.9807 - val_loss: 0.0640\n",
            "Epoch 6/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 229ms/step - accuracy: 0.6697 - loss: 0.7779 - val_accuracy: 0.8683 - val_loss: 0.4517\n",
            "Epoch 7/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 230ms/step - accuracy: 0.8202 - loss: 0.3692 - val_accuracy: 0.9830 - val_loss: 0.0724\n",
            "Epoch 8/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 230ms/step - accuracy: 0.9647 - loss: 0.0922 - val_accuracy: 0.6475 - val_loss: 0.6209\n",
            "Epoch 9/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 218ms/step - accuracy: 0.8363 - loss: 0.2739 - val_accuracy: 0.9670 - val_loss: 0.1006\n",
            "Epoch 10/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 235ms/step - accuracy: 0.9698 - loss: 0.0699 - val_accuracy: 0.9709 - val_loss: 0.0922\n",
            "Epoch 11/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 232ms/step - accuracy: 0.9696 - loss: 0.0685 - val_accuracy: 0.9556 - val_loss: 0.1114\n",
            "Epoch 12/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 232ms/step - accuracy: 0.9753 - loss: 0.0519 - val_accuracy: 0.9327 - val_loss: 0.2104\n",
            "Epoch 13/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 232ms/step - accuracy: 0.9695 - loss: 0.0592 - val_accuracy: 0.9837 - val_loss: 0.0455\n",
            "Epoch 14/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 234ms/step - accuracy: 0.9765 - loss: 0.0492 - val_accuracy: 0.9716 - val_loss: 0.0879\n",
            "Epoch 15/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 233ms/step - accuracy: 0.9779 - loss: 0.0475 - val_accuracy: 0.9664 - val_loss: 0.1051\n",
            "Epoch 16/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 226ms/step - accuracy: 0.9755 - loss: 0.0438 - val_accuracy: 0.9794 - val_loss: 0.0653\n",
            "Epoch 17/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 223ms/step - accuracy: 0.9711 - loss: 0.0513 - val_accuracy: 0.9853 - val_loss: 0.0445\n",
            "Epoch 18/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 231ms/step - accuracy: 0.9746 - loss: 0.0522 - val_accuracy: 0.9811 - val_loss: 0.0541\n",
            "Epoch 19/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 231ms/step - accuracy: 0.9793 - loss: 0.0379 - val_accuracy: 0.9804 - val_loss: 0.0558\n",
            "Epoch 20/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 231ms/step - accuracy: 0.9815 - loss: 0.0344 - val_accuracy: 0.9791 - val_loss: 0.0546\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 68ms/step\n",
            "Fold 2 - Accuracy: 0.9791, F1 Macro: 0.9663\n",
            "\n",
            "--- Fold 3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m66,560\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 227ms/step - accuracy: 0.8362 - loss: 0.5566 - val_accuracy: 0.9719 - val_loss: 0.0949\n",
            "Epoch 2/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 236ms/step - accuracy: 0.9526 - loss: 0.1555 - val_accuracy: 0.0693 - val_loss: 1.4258\n",
            "Epoch 3/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 236ms/step - accuracy: 0.5853 - loss: 0.9096 - val_accuracy: 0.9778 - val_loss: 0.0699\n",
            "Epoch 4/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 237ms/step - accuracy: 0.9557 - loss: 0.1085 - val_accuracy: 0.9056 - val_loss: 0.3499\n",
            "Epoch 5/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 235ms/step - accuracy: 0.9511 - loss: 0.1078 - val_accuracy: 0.9794 - val_loss: 0.0600\n",
            "Epoch 6/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 237ms/step - accuracy: 0.9704 - loss: 0.0782 - val_accuracy: 0.9863 - val_loss: 0.0449\n",
            "Epoch 7/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 235ms/step - accuracy: 0.9472 - loss: 0.3448 - val_accuracy: 0.9863 - val_loss: 0.0377\n",
            "Epoch 8/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 235ms/step - accuracy: 0.9770 - loss: 0.0638 - val_accuracy: 0.9797 - val_loss: 0.0707\n",
            "Epoch 9/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 236ms/step - accuracy: 0.9643 - loss: 0.0836 - val_accuracy: 0.9846 - val_loss: 0.0453\n",
            "Epoch 10/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 237ms/step - accuracy: 0.9757 - loss: 0.0511 - val_accuracy: 0.9853 - val_loss: 0.0391\n",
            "Epoch 11/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 240ms/step - accuracy: 0.9766 - loss: 0.0454 - val_accuracy: 0.9771 - val_loss: 0.0625\n",
            "Epoch 12/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 235ms/step - accuracy: 0.9777 - loss: 0.0420 - val_accuracy: 0.9833 - val_loss: 0.0426\n",
            "Epoch 13/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 230ms/step - accuracy: 0.9765 - loss: 0.0420 - val_accuracy: 0.9846 - val_loss: 0.0385\n",
            "Epoch 14/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 237ms/step - accuracy: 0.9734 - loss: 0.0503 - val_accuracy: 0.9654 - val_loss: 0.0736\n",
            "Epoch 15/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 238ms/step - accuracy: 0.9762 - loss: 0.0418 - val_accuracy: 0.9912 - val_loss: 0.0257\n",
            "Epoch 16/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 234ms/step - accuracy: 0.9766 - loss: 0.0421 - val_accuracy: 0.9859 - val_loss: 0.0332\n",
            "Epoch 17/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 237ms/step - accuracy: 0.9778 - loss: 0.0374 - val_accuracy: 0.9863 - val_loss: 0.0337\n",
            "Epoch 18/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 238ms/step - accuracy: 0.9785 - loss: 0.0413 - val_accuracy: 0.9846 - val_loss: 0.0353\n",
            "Epoch 19/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 236ms/step - accuracy: 0.9773 - loss: 0.0422 - val_accuracy: 0.9814 - val_loss: 0.0464\n",
            "Epoch 20/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 226ms/step - accuracy: 0.9766 - loss: 0.0447 - val_accuracy: 0.9840 - val_loss: 0.0427\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 82ms/step\n",
            "Fold 3 - Accuracy: 0.9840, F1 Macro: 0.9731\n",
            "\n",
            "--- Fold 4 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m66,560\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 224ms/step - accuracy: 0.8659 - loss: 0.4888 - val_accuracy: 0.8425 - val_loss: 0.3669\n",
            "Epoch 2/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 223ms/step - accuracy: 0.7916 - loss: 0.6004 - val_accuracy: 0.9526 - val_loss: 0.1527\n",
            "Epoch 3/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 231ms/step - accuracy: 0.9588 - loss: 0.2657 - val_accuracy: 0.9552 - val_loss: 0.1333\n",
            "Epoch 4/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 231ms/step - accuracy: 0.9520 - loss: 0.3145 - val_accuracy: 0.9627 - val_loss: 0.1095\n",
            "Epoch 5/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 235ms/step - accuracy: 0.9521 - loss: 0.3149 - val_accuracy: 0.9513 - val_loss: 0.1408\n",
            "Epoch 6/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 234ms/step - accuracy: 0.9544 - loss: 0.3006 - val_accuracy: 0.9353 - val_loss: 0.2146\n",
            "Epoch 7/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 231ms/step - accuracy: 0.9456 - loss: 0.3140 - val_accuracy: 0.9641 - val_loss: 0.1123\n",
            "Epoch 8/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 222ms/step - accuracy: 0.9529 - loss: 0.3104 - val_accuracy: 0.9742 - val_loss: 0.0976\n",
            "Epoch 9/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 224ms/step - accuracy: 0.9747 - loss: 0.0976 - val_accuracy: 0.9827 - val_loss: 0.0501\n",
            "Epoch 10/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 238ms/step - accuracy: 0.9733 - loss: 0.0801 - val_accuracy: 0.9562 - val_loss: 0.1024\n",
            "Epoch 11/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 238ms/step - accuracy: 0.9536 - loss: 0.2498 - val_accuracy: 0.9598 - val_loss: 0.1369\n",
            "Epoch 12/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 233ms/step - accuracy: 0.9559 - loss: 0.2761 - val_accuracy: 0.9582 - val_loss: 0.1316\n",
            "Epoch 13/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 227ms/step - accuracy: 0.9566 - loss: 0.2780 - val_accuracy: 0.9503 - val_loss: 0.1632\n",
            "Epoch 14/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 233ms/step - accuracy: 0.9563 - loss: 0.2023 - val_accuracy: 0.9846 - val_loss: 0.0612\n",
            "Epoch 15/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 236ms/step - accuracy: 0.9646 - loss: 0.2053 - val_accuracy: 0.9343 - val_loss: 0.1726\n",
            "Epoch 16/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 224ms/step - accuracy: 0.9605 - loss: 0.1264 - val_accuracy: 0.9722 - val_loss: 0.0742\n",
            "Epoch 17/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 224ms/step - accuracy: 0.9778 - loss: 0.0683 - val_accuracy: 0.9781 - val_loss: 0.0515\n",
            "Epoch 18/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 235ms/step - accuracy: 0.9793 - loss: 0.0617 - val_accuracy: 0.9729 - val_loss: 0.0734\n",
            "Epoch 19/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 234ms/step - accuracy: 0.9775 - loss: 0.0578 - val_accuracy: 0.9735 - val_loss: 0.0763\n",
            "Epoch 20/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 234ms/step - accuracy: 0.9747 - loss: 0.0674 - val_accuracy: 0.9745 - val_loss: 0.0680\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 82ms/step\n",
            "Fold 4 - Accuracy: 0.9745, F1 Macro: 0.9535\n",
            "\n",
            "--- Fold 5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m66,560\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,163\u001b[0m (453.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,163</span> (453.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 225ms/step - accuracy: 0.8783 - loss: 0.4734 - val_accuracy: 0.9647 - val_loss: 0.1085\n",
            "Epoch 2/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 232ms/step - accuracy: 0.9304 - loss: 0.3771 - val_accuracy: 0.9673 - val_loss: 0.1133\n",
            "Epoch 3/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 233ms/step - accuracy: 0.9517 - loss: 0.2749 - val_accuracy: 0.9709 - val_loss: 0.0800\n",
            "Epoch 4/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 224ms/step - accuracy: 0.9566 - loss: 0.1366 - val_accuracy: 0.9719 - val_loss: 0.0795\n",
            "Epoch 5/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 224ms/step - accuracy: 0.9638 - loss: 0.1080 - val_accuracy: 0.9428 - val_loss: 0.2002\n",
            "Epoch 6/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 232ms/step - accuracy: 0.9461 - loss: 0.3560 - val_accuracy: 0.9657 - val_loss: 0.1210\n",
            "Epoch 7/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 232ms/step - accuracy: 0.9462 - loss: 0.3182 - val_accuracy: 0.9595 - val_loss: 0.1325\n",
            "Epoch 8/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 226ms/step - accuracy: 0.9474 - loss: 0.3177 - val_accuracy: 0.9552 - val_loss: 0.1273\n",
            "Epoch 9/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 221ms/step - accuracy: 0.9541 - loss: 0.2810 - val_accuracy: 0.9660 - val_loss: 0.1031\n",
            "Epoch 10/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 236ms/step - accuracy: 0.9638 - loss: 0.1913 - val_accuracy: 0.9667 - val_loss: 0.0954\n",
            "Epoch 11/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 233ms/step - accuracy: 0.9788 - loss: 0.0602 - val_accuracy: 0.9395 - val_loss: 0.1615\n",
            "Epoch 12/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 233ms/step - accuracy: 0.9747 - loss: 0.0814 - val_accuracy: 0.9794 - val_loss: 0.0525\n",
            "Epoch 13/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 238ms/step - accuracy: 0.9762 - loss: 0.0710 - val_accuracy: 0.9634 - val_loss: 0.1025\n",
            "Epoch 14/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 225ms/step - accuracy: 0.9776 - loss: 0.0627 - val_accuracy: 0.9824 - val_loss: 0.0464\n",
            "Epoch 15/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 226ms/step - accuracy: 0.9757 - loss: 0.0660 - val_accuracy: 0.9843 - val_loss: 0.0459\n",
            "Epoch 16/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 234ms/step - accuracy: 0.9758 - loss: 0.0631 - val_accuracy: 0.9516 - val_loss: 0.1225\n",
            "Epoch 17/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 237ms/step - accuracy: 0.9713 - loss: 0.0620 - val_accuracy: 0.9641 - val_loss: 0.0940\n",
            "Epoch 18/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 234ms/step - accuracy: 0.9738 - loss: 0.0511 - val_accuracy: 0.9873 - val_loss: 0.0377\n",
            "Epoch 19/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 236ms/step - accuracy: 0.9759 - loss: 0.0466 - val_accuracy: 0.9837 - val_loss: 0.0521\n",
            "Epoch 20/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 226ms/step - accuracy: 0.9777 - loss: 0.0589 - val_accuracy: 0.9814 - val_loss: 0.0516\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 69ms/step\n",
            "Fold 5 - Accuracy: 0.9814, F1 Macro: 0.9664\n",
            "\n",
            "\n",
            "Final Results:\n",
            "Average Accuracy: 0.9803 ± 0.0033\n",
            "Average F1 Macro: 0.9662 ± 0.0069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== Modified: Class Weight Calculation ========\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced',\n",
        "                                     classes=np.unique(y_encoded),\n",
        "                                     y=y_encoded)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# ======== Modified Model Compilation ========\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(1, 109, 1)))\n",
        "    model.add(TimeDistributed(Conv1D(256, 3, activation='relu')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
        "    model.add(TimeDistributed(Conv1D(128, 3, activation='relu')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "    model.add(Bidirectional(LSTM(50)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Use weighted crossentropy\n",
        "    model.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ======== Modified Training Loop ========\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(f\"\\n--- Fold {fold_no} ---\")\n",
        "\n",
        "    model = create_model()\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y_categorical[train_index], y_categorical[test_index]\n",
        "\n",
        "    # ======== REMOVED OVERSAMPLING ========\n",
        "    # Train with class weights\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        class_weight=class_weights,  # Apply class weights\n",
        "        epochs=20,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    # Evaluation remains same\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
        "\n",
        "    accuracies.append(acc)\n",
        "    f1_scores.append(f1)\n",
        "    fold_no += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "ZtuijNinb5rQ",
        "outputId": "b2bd0ec7-45ba-4e48-d9c2-53b2b36f8b59"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 2 ---\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 1 for '{{node sequential_3_1/time_distributed_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_3_1/time_distributed_1/convolution/ExpandDims, sequential_3_1/time_distributed_1/convolution/ExpandDims_1)' with input shapes: [?,1,1,1], [1,3,1,256].\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(None, 109, 1, 1), dtype=float32)\n  • training=True\n  • mask=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b075d7d078fe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# ======== REMOVED OVERSAMPLING ========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Train with class weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 1 for '{{node sequential_3_1/time_distributed_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_3_1/time_distributed_1/convolution/ExpandDims, sequential_3_1/time_distributed_1/convolution/ExpandDims_1)' with input shapes: [?,1,1,1], [1,3,1,256].\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(None, 109, 1, 1), dtype=float32)\n  • training=True\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Load and prepare data\n",
        "X_ts = PIR_data.iloc[:, :-1].values.reshape(-1, 109, 1)  # Time series data\n",
        "X_meta = X_Meta # Metadata\n",
        "y = PIR_data.iloc[:, -1].values  # Labels\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Cross-validation setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores, accuracies = [], []\n",
        "\n",
        "def create_lstm_with_meta():\n",
        "    # Time series branch\n",
        "    ts_input = Input(shape=(109, 1), name='ts_input')\n",
        "    x = LSTM(128, return_sequences=True)(ts_input)\n",
        "    x = Dropout(0.3)(x)\n",
        "    ts_features = LSTM(64)(x)\n",
        "\n",
        "    # Metadata branch\n",
        "    meta_input = Input(shape=(X_meta.shape[1],), name='meta_input')\n",
        "\n",
        "    # Concatenate before final FC\n",
        "    merged = Concatenate()([ts_features, meta_input])\n",
        "    x = Dropout(0.4)(merged)\n",
        "    outputs = Dense(3, activation='softmax')(x)\n",
        "\n",
        "    return Model(inputs=[ts_input, meta_input], outputs=outputs)\n",
        "\n",
        "for fold_no, (train_idx, test_idx) in enumerate(kf.split(X_ts), 1):\n",
        "    print(f\"\\n--- Fold {fold_no} ---\")\n",
        "\n",
        "    # Split data\n",
        "    X_train_ts, X_test_ts = X_ts[train_idx], X_ts[test_idx]\n",
        "    X_train_meta, X_test_meta = X_meta[train_idx], X_meta[test_idx]\n",
        "    y_train, y_test = y_categorical[train_idx], y_categorical[test_idx]\n",
        "\n",
        "    # ROS oversampling\n",
        "    ros = RandomOverSampler(random_state=42)\n",
        "    X_train_flat = X_train_ts.reshape(len(X_train_ts), -1)\n",
        "    combined_train = np.hstack([X_train_flat, X_train_meta])\n",
        "\n",
        "    X_resampled, y_resampled = ros.fit_resample(combined_train, np.argmax(y_train, axis=1))\n",
        "\n",
        "    # Split back components\n",
        "    X_resampled_ts = X_resampled[:, :-X_meta.shape[1]].reshape(-1, 109, 1)\n",
        "    X_resampled_meta = X_resampled[:, -X_meta.shape[1]:]\n",
        "    y_resampled = to_categorical(y_resampled)\n",
        "\n",
        "    # Create and train model\n",
        "    model = create_lstm_with_meta()\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(\n",
        "        [X_resampled_ts, X_resampled_meta], y_resampled,\n",
        "        validation_data=([X_test_ts, X_test_meta], y_test),\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = model.predict([X_test_ts, X_test_meta])\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
        "\n",
        "    accuracies.append(acc)\n",
        "    f1_scores.append(f1)\n",
        "    print(f\"Fold {fold_no} - Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\nFinal Metrics:\")\n",
        "print(f\"Mean Accuracy: {np.mean(accuracies):.4f} (±{np.std(accuracies):.4f})\")\n",
        "print(f\"Mean F1-Score: {np.mean(f1_scores):.4f} (±{np.std(f1_scores):.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DMIQIstqcPbn",
        "outputId": "6ed6df24-73b0-469f-8d5f-3ce2f88238e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 1 ---\n",
            "Epoch 1/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 256ms/step - accuracy: 0.8898 - loss: 0.3212 - val_accuracy: 0.9755 - val_loss: 0.1140\n",
            "Epoch 2/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 221ms/step - accuracy: 0.9227 - loss: 0.2156 - val_accuracy: 0.9755 - val_loss: 0.0626\n",
            "Epoch 3/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 214ms/step - accuracy: 0.9818 - loss: 0.0587 - val_accuracy: 0.9781 - val_loss: 0.0630\n",
            "Epoch 4/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 225ms/step - accuracy: 0.9845 - loss: 0.0510 - val_accuracy: 0.9778 - val_loss: 0.0693\n",
            "Epoch 5/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 226ms/step - accuracy: 0.9871 - loss: 0.0407 - val_accuracy: 0.9781 - val_loss: 0.0545\n",
            "Epoch 6/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 229ms/step - accuracy: 0.9874 - loss: 0.0410 - val_accuracy: 0.9850 - val_loss: 0.0372\n",
            "Epoch 7/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 222ms/step - accuracy: 0.9888 - loss: 0.0370 - val_accuracy: 0.9755 - val_loss: 0.0716\n",
            "Epoch 8/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 227ms/step - accuracy: 0.9898 - loss: 0.0316 - val_accuracy: 0.9856 - val_loss: 0.0337\n",
            "Epoch 9/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 222ms/step - accuracy: 0.9875 - loss: 0.0394 - val_accuracy: 0.9801 - val_loss: 0.0483\n",
            "Epoch 10/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 222ms/step - accuracy: 0.9870 - loss: 0.0435 - val_accuracy: 0.9804 - val_loss: 0.0579\n",
            "Epoch 11/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 224ms/step - accuracy: 0.9871 - loss: 0.0421 - val_accuracy: 0.9873 - val_loss: 0.0377\n",
            "Epoch 12/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 225ms/step - accuracy: 0.9875 - loss: 0.0407 - val_accuracy: 0.9833 - val_loss: 0.0422\n",
            "Epoch 13/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 227ms/step - accuracy: 0.9855 - loss: 0.0383 - val_accuracy: 0.9820 - val_loss: 0.0538\n",
            "Epoch 14/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 224ms/step - accuracy: 0.9876 - loss: 0.0369 - val_accuracy: 0.9866 - val_loss: 0.0450\n",
            "Epoch 15/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 224ms/step - accuracy: 0.9891 - loss: 0.0331 - val_accuracy: 0.9572 - val_loss: 0.1075\n",
            "Epoch 16/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 226ms/step - accuracy: 0.9871 - loss: 0.0368 - val_accuracy: 0.9791 - val_loss: 0.0512\n",
            "Epoch 17/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 219ms/step - accuracy: 0.9901 - loss: 0.0273 - val_accuracy: 0.9840 - val_loss: 0.0434\n",
            "Epoch 18/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 238ms/step - accuracy: 0.9903 - loss: 0.0309 - val_accuracy: 0.9716 - val_loss: 0.0807\n",
            "Epoch 19/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 232ms/step - accuracy: 0.9907 - loss: 0.0292 - val_accuracy: 0.9814 - val_loss: 0.0568\n",
            "Epoch 20/20\n",
            "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 237ms/step - accuracy: 0.9897 - loss: 0.0309 - val_accuracy: 0.9797 - val_loss: 0.0483\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 85ms/step\n",
            "Fold 1 - Accuracy: 0.9797, F1: 0.9680\n",
            "\n",
            "--- Fold 2 ---\n",
            "Epoch 1/20\n",
            "\u001b[1m390/937\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 212ms/step - accuracy: 0.7741 - loss: 0.5712"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7baa24363dc3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mX_resampled_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_resampled_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted CE  + Meta data+ Normalisation\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Load and prepare data\n",
        "X_ts = PIR_data.iloc[:, :-1].values.reshape(-1, 109, 1)  # Time series data\n",
        "X_meta = X_Meta # Metadata\n",
        "y = PIR_data.iloc[:, -1].values  # Labels\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced',\n",
        "                                     classes=np.unique(y_encoded),\n",
        "                                     y=y_encoded)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# Cross-validation setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores, accuracies = [], []\n",
        "\n",
        "def create_lstm_with_meta():\n",
        "    # Time series branch\n",
        "    ts_input = Input(shape=(109, 1), name='ts_input')\n",
        "    x = LSTM(128, return_sequences=True)(ts_input)\n",
        "    x = Dropout(0.3)(x)\n",
        "    ts_features = LSTM(64)(x)\n",
        "\n",
        "    # Metadata branch\n",
        "    meta_input = Input(shape=(X_meta.shape[1],), name='meta_input')\n",
        "\n",
        "    # Concatenate before final FC\n",
        "    merged = Concatenate()([ts_features, meta_input])\n",
        "    x = Dropout(0.4)(merged)\n",
        "    outputs = Dense(3, activation='softmax')(x)\n",
        "\n",
        "    return Model(inputs=[ts_input, meta_input], outputs=outputs)\n",
        "\n",
        "for fold_no, (train_idx, test_idx) in enumerate(kf.split(X_ts), 1):\n",
        "    print(f\"\\n--- Fold {fold_no} ---\")\n",
        "\n",
        "    # Split data\n",
        "    X_train_ts, X_test_ts = X_ts[train_idx], X_ts[test_idx]\n",
        "    X_train_meta, X_test_meta = X_meta[train_idx], X_meta[test_idx]\n",
        "    y_train, y_test = y_categorical[train_idx], y_categorical[test_idx]\n",
        "\n",
        "\n",
        "    # Create and train model\n",
        "    model = create_lstm_with_meta()\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(\n",
        "        [X_train_ts, X_train_meta], y_train,\n",
        "        validation_data=([X_test_ts, X_test_meta], y_test),\n",
        "        class_weight = class_weights,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = model.predict([X_test_ts, X_test_meta])\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
        "\n",
        "    accuracies.append(acc)\n",
        "    f1_scores.append(f1)\n",
        "    print(f\"Fold {fold_no} - Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\nFinal Metrics:\")\n",
        "print(f\"Mean Accuracy: {np.mean(accuracies):.4f} (±{np.std(accuracies):.4f})\")\n",
        "print(f\"Mean F1-Score: {np.mean(f1_scores):.4f} (±{np.std(f1_scores):.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z0It7zV5eU2h",
        "outputId": "91dbfadb-597b-49b4-874f-138ce49be5ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 1 ---\n",
            "Epoch 1/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 244ms/step - accuracy: 0.7747 - loss: 0.8525 - val_accuracy: 0.6874 - val_loss: 0.6101\n",
            "Epoch 2/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 249ms/step - accuracy: 0.6939 - loss: 0.7032 - val_accuracy: 0.6223 - val_loss: 0.8832\n",
            "Epoch 3/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 236ms/step - accuracy: 0.5731 - loss: 0.8054 - val_accuracy: 0.8942 - val_loss: 0.3933\n",
            "Epoch 4/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 243ms/step - accuracy: 0.8587 - loss: 0.4005 - val_accuracy: 0.9543 - val_loss: 0.1707\n",
            "Epoch 5/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 248ms/step - accuracy: 0.9533 - loss: 0.1279 - val_accuracy: 0.9650 - val_loss: 0.1079\n",
            "Epoch 6/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 238ms/step - accuracy: 0.9619 - loss: 0.1326 - val_accuracy: 0.9794 - val_loss: 0.0661\n",
            "Epoch 7/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 242ms/step - accuracy: 0.9742 - loss: 0.0614 - val_accuracy: 0.9866 - val_loss: 0.0471\n",
            "Epoch 8/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 245ms/step - accuracy: 0.9617 - loss: 0.1105 - val_accuracy: 0.9778 - val_loss: 0.0663\n",
            "Epoch 9/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 241ms/step - accuracy: 0.9711 - loss: 0.0704 - val_accuracy: 0.9732 - val_loss: 0.0838\n",
            "Epoch 10/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 247ms/step - accuracy: 0.9687 - loss: 0.0609 - val_accuracy: 0.9797 - val_loss: 0.0659\n",
            "Epoch 11/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 241ms/step - accuracy: 0.9745 - loss: 0.0551 - val_accuracy: 0.9752 - val_loss: 0.0716\n",
            "Epoch 12/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 248ms/step - accuracy: 0.9789 - loss: 0.0412 - val_accuracy: 0.9856 - val_loss: 0.0507\n",
            "Epoch 13/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 240ms/step - accuracy: 0.9711 - loss: 0.0555 - val_accuracy: 0.9860 - val_loss: 0.0454\n",
            "Epoch 14/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 239ms/step - accuracy: 0.9773 - loss: 0.0451 - val_accuracy: 0.9833 - val_loss: 0.0547\n",
            "Epoch 15/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 240ms/step - accuracy: 0.9784 - loss: 0.0382 - val_accuracy: 0.9866 - val_loss: 0.0431\n",
            "Epoch 16/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 242ms/step - accuracy: 0.9817 - loss: 0.0395 - val_accuracy: 0.9869 - val_loss: 0.0402\n",
            "Epoch 17/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 244ms/step - accuracy: 0.9860 - loss: 0.0334 - val_accuracy: 0.9680 - val_loss: 0.0896\n",
            "Epoch 18/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 240ms/step - accuracy: 0.9810 - loss: 0.0403 - val_accuracy: 0.9840 - val_loss: 0.0418\n",
            "Epoch 19/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 239ms/step - accuracy: 0.9793 - loss: 0.0476 - val_accuracy: 0.9811 - val_loss: 0.0526\n",
            "Epoch 20/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 240ms/step - accuracy: 0.9790 - loss: 0.0476 - val_accuracy: 0.9739 - val_loss: 0.0923\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step\n",
            "Fold 1 - Accuracy: 0.9739, F1: 0.9595\n",
            "\n",
            "--- Fold 2 ---\n",
            "Epoch 1/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 234ms/step - accuracy: 0.8330 - loss: 0.4837 - val_accuracy: 0.9040 - val_loss: 0.2308\n",
            "Epoch 2/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 221ms/step - accuracy: 0.9480 - loss: 0.2404 - val_accuracy: 0.9713 - val_loss: 0.1028\n",
            "Epoch 3/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 233ms/step - accuracy: 0.9620 - loss: 0.2022 - val_accuracy: 0.9686 - val_loss: 0.1173\n",
            "Epoch 4/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 220ms/step - accuracy: 0.9710 - loss: 0.1549 - val_accuracy: 0.9699 - val_loss: 0.1047\n",
            "Epoch 5/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 236ms/step - accuracy: 0.9693 - loss: 0.1507 - val_accuracy: 0.9758 - val_loss: 0.0919\n",
            "Epoch 6/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 233ms/step - accuracy: 0.9690 - loss: 0.1590 - val_accuracy: 0.9807 - val_loss: 0.0568\n",
            "Epoch 7/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 233ms/step - accuracy: 0.9758 - loss: 0.0741 - val_accuracy: 0.9768 - val_loss: 0.0707\n",
            "Epoch 8/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 232ms/step - accuracy: 0.9676 - loss: 0.1541 - val_accuracy: 0.9713 - val_loss: 0.0863\n",
            "Epoch 9/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 234ms/step - accuracy: 0.9721 - loss: 0.0839 - val_accuracy: 0.9670 - val_loss: 0.0783\n",
            "Epoch 10/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 234ms/step - accuracy: 0.9754 - loss: 0.0681 - val_accuracy: 0.9647 - val_loss: 0.1054\n",
            "Epoch 11/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 234ms/step - accuracy: 0.9717 - loss: 0.0736 - val_accuracy: 0.9784 - val_loss: 0.0585\n",
            "Epoch 12/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 238ms/step - accuracy: 0.9578 - loss: 0.1030 - val_accuracy: 0.9768 - val_loss: 0.0768\n",
            "Epoch 13/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 223ms/step - accuracy: 0.9780 - loss: 0.0566 - val_accuracy: 0.9585 - val_loss: 0.1140\n",
            "Epoch 14/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 233ms/step - accuracy: 0.9795 - loss: 0.0562 - val_accuracy: 0.9670 - val_loss: 0.1017\n",
            "Epoch 15/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 235ms/step - accuracy: 0.9766 - loss: 0.0657 - val_accuracy: 0.9771 - val_loss: 0.0773\n",
            "Epoch 16/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 228ms/step - accuracy: 0.9754 - loss: 0.0511 - val_accuracy: 0.9827 - val_loss: 0.0610\n",
            "Epoch 17/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 235ms/step - accuracy: 0.9820 - loss: 0.0461 - val_accuracy: 0.9752 - val_loss: 0.0703\n",
            "Epoch 18/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 236ms/step - accuracy: 0.9786 - loss: 0.0487 - val_accuracy: 0.9794 - val_loss: 0.0644\n",
            "Epoch 19/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 237ms/step - accuracy: 0.9782 - loss: 0.0493 - val_accuracy: 0.9846 - val_loss: 0.0507\n",
            "Epoch 20/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 235ms/step - accuracy: 0.9765 - loss: 0.0500 - val_accuracy: 0.9650 - val_loss: 0.0993\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step\n",
            "Fold 2 - Accuracy: 0.9650, F1: 0.9444\n",
            "\n",
            "--- Fold 3 ---\n",
            "Epoch 1/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 234ms/step - accuracy: 0.7753 - loss: 0.7002 - val_accuracy: 0.9650 - val_loss: 0.1069\n",
            "Epoch 2/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 231ms/step - accuracy: 0.9409 - loss: 0.3118 - val_accuracy: 0.9641 - val_loss: 0.1051\n",
            "Epoch 3/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 233ms/step - accuracy: 0.9664 - loss: 0.0948 - val_accuracy: 0.9598 - val_loss: 0.1130\n",
            "Epoch 4/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 225ms/step - accuracy: 0.9762 - loss: 0.0742 - val_accuracy: 0.9837 - val_loss: 0.0441\n",
            "Epoch 5/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 234ms/step - accuracy: 0.9700 - loss: 0.0825 - val_accuracy: 0.9742 - val_loss: 0.0747\n",
            "Epoch 6/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 232ms/step - accuracy: 0.9734 - loss: 0.0793 - val_accuracy: 0.9539 - val_loss: 0.1330\n",
            "Epoch 7/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 235ms/step - accuracy: 0.9670 - loss: 0.0906 - val_accuracy: 0.9840 - val_loss: 0.0457\n",
            "Epoch 8/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 234ms/step - accuracy: 0.9751 - loss: 0.0671 - val_accuracy: 0.9696 - val_loss: 0.0940\n",
            "Epoch 9/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 227ms/step - accuracy: 0.9755 - loss: 0.0814 - val_accuracy: 0.9526 - val_loss: 0.1357\n",
            "Epoch 10/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 233ms/step - accuracy: 0.9695 - loss: 0.0740 - val_accuracy: 0.9833 - val_loss: 0.1325\n",
            "Epoch 11/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 235ms/step - accuracy: 0.8122 - loss: 0.4524 - val_accuracy: 0.7167 - val_loss: 0.4125\n",
            "Epoch 12/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 232ms/step - accuracy: 0.7510 - loss: 0.3803 - val_accuracy: 0.9732 - val_loss: 0.0923\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-27f6f69b9291>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mX_train_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BILSTM + CNN with oversampling,109 PIR values only (NO META DATA included)\n",
        "X = PIR_data.iloc[:, :-1].values  # first 55 columns: the PIR time series\n",
        "y = PIR_data.iloc[:, -1].values   # last column: the class label\n",
        "\n",
        "X = X.reshape((X.shape[0], 1, 109, 1))\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)  # now y becomes 0, 1, 2\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Set up cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "\n",
        "f1_scores = []\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(f\"\\n--- Fold {fold_no} ---\")\n",
        "\n",
        "    # --- Build CNN-LSTM Model ---\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(1, 109, 1)))\n",
        "    model.add(TimeDistributed(Conv1D(filters=256, kernel_size=3, activation='relu')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=2, padding='same')))\n",
        "    model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=2, padding='same')))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(Bidirectional(LSTM(128, activation='relu', return_sequences=True)))\n",
        "    model.add(Bidirectional(LSTM(50, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y_categorical[train_index], y_categorical[test_index]\n",
        "\n",
        "    # Apply Random Oversampling\n",
        "    ros = RandomOverSampler(random_state=42)\n",
        "    y_train_labels = np.argmax(y_train, axis=1)  # Convert one-hot to label\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1)  # Flatten to 2D\n",
        "\n",
        "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_flat, y_train_labels)\n",
        "\n",
        "    # Reshape X back to 4D and y back to one-hot\n",
        "    X_train_resampled = X_train_resampled.reshape(X_train_resampled.shape[0], 1, 109, 1)\n",
        "    y_train_resampled = to_categorical(y_train_resampled)\n",
        "\n",
        "    # --- Train the Model ---\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
        "\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"F1 Macro Score:\", f1)\n",
        "    # print(classification_report(y_true_classes, y_pred_classes, target_names=label_encoder.classes_))\n",
        "\n",
        "    accuracies.append(acc)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "    # y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
        "    # y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
        "\n",
        "    # from sklearn.metrics import classification_report\n",
        "    # print(classification_report(y_true_classes, y_pred_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "86LM3QpNy8NN",
        "outputId": "467d3e37-5cd4-47c9-c8f1-e6970bdfed3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 1 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m107\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │        \u001b[38;5;34m98,432\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3328\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │     \u001b[38;5;34m3,539,968\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m122,800\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m303\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">107</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3328</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,539,968</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">122,800</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">303</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,762,527\u001b[0m (14.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,762,527</span> (14.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,762,527\u001b[0m (14.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,762,527</span> (14.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 122ms/step - accuracy: 0.9122 - loss: 0.3625 - val_accuracy: 0.9837 - val_loss: 0.0596\n",
            "Epoch 2/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 161ms/step - accuracy: 0.9849 - loss: 0.0617 - val_accuracy: 0.9899 - val_loss: 0.0299\n",
            "Epoch 3/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 119ms/step - accuracy: 0.9905 - loss: 0.0301 - val_accuracy: 0.9869 - val_loss: 0.0512\n",
            "Epoch 4/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 117ms/step - accuracy: 0.9896 - loss: 0.0309 - val_accuracy: 0.9915 - val_loss: 0.0192\n",
            "Epoch 5/20\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 116ms/step - accuracy: 0.9898 - loss: 0.0272 - val_accuracy: 0.9837 - val_loss: 0.0410\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a8089b8dad27>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# --- Train the Model ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}