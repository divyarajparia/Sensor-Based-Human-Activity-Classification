{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We first read the 2 data files\n",
    "df1 = pd.read_csv('pirvision_office_dataset1.csv')\n",
    "df2 = pd.read_csv('pirvision_office_dataset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7651, 59) (7651, 59) (15302, 59)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Label</th>\n",
       "      <th>Temperature_F</th>\n",
       "      <th>PIR_1</th>\n",
       "      <th>PIR_2</th>\n",
       "      <th>PIR_3</th>\n",
       "      <th>PIR_4</th>\n",
       "      <th>PIR_5</th>\n",
       "      <th>PIR_6</th>\n",
       "      <th>PIR_7</th>\n",
       "      <th>PIR_8</th>\n",
       "      <th>PIR_9</th>\n",
       "      <th>PIR_10</th>\n",
       "      <th>PIR_11</th>\n",
       "      <th>PIR_12</th>\n",
       "      <th>PIR_13</th>\n",
       "      <th>PIR_14</th>\n",
       "      <th>PIR_15</th>\n",
       "      <th>PIR_16</th>\n",
       "      <th>PIR_17</th>\n",
       "      <th>PIR_18</th>\n",
       "      <th>PIR_19</th>\n",
       "      <th>PIR_20</th>\n",
       "      <th>PIR_21</th>\n",
       "      <th>PIR_22</th>\n",
       "      <th>PIR_23</th>\n",
       "      <th>PIR_24</th>\n",
       "      <th>PIR_25</th>\n",
       "      <th>PIR_26</th>\n",
       "      <th>PIR_27</th>\n",
       "      <th>PIR_28</th>\n",
       "      <th>PIR_29</th>\n",
       "      <th>PIR_30</th>\n",
       "      <th>PIR_31</th>\n",
       "      <th>PIR_32</th>\n",
       "      <th>PIR_33</th>\n",
       "      <th>PIR_34</th>\n",
       "      <th>PIR_35</th>\n",
       "      <th>PIR_36</th>\n",
       "      <th>PIR_37</th>\n",
       "      <th>PIR_38</th>\n",
       "      <th>PIR_39</th>\n",
       "      <th>PIR_40</th>\n",
       "      <th>PIR_41</th>\n",
       "      <th>PIR_42</th>\n",
       "      <th>PIR_43</th>\n",
       "      <th>PIR_44</th>\n",
       "      <th>PIR_45</th>\n",
       "      <th>PIR_46</th>\n",
       "      <th>PIR_47</th>\n",
       "      <th>PIR_48</th>\n",
       "      <th>PIR_49</th>\n",
       "      <th>PIR_50</th>\n",
       "      <th>PIR_51</th>\n",
       "      <th>PIR_52</th>\n",
       "      <th>PIR_53</th>\n",
       "      <th>PIR_54</th>\n",
       "      <th>PIR_55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:19:56</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10269</td>\n",
       "      <td>10721</td>\n",
       "      <td>11156</td>\n",
       "      <td>11170</td>\n",
       "      <td>10931</td>\n",
       "      <td>10671</td>\n",
       "      <td>10395</td>\n",
       "      <td>10133</td>\n",
       "      <td>9885</td>\n",
       "      <td>9705</td>\n",
       "      <td>9538</td>\n",
       "      <td>9418</td>\n",
       "      <td>9469</td>\n",
       "      <td>9599</td>\n",
       "      <td>9817</td>\n",
       "      <td>9910</td>\n",
       "      <td>9890</td>\n",
       "      <td>10075</td>\n",
       "      <td>10231</td>\n",
       "      <td>10247</td>\n",
       "      <td>10271</td>\n",
       "      <td>10229</td>\n",
       "      <td>10272</td>\n",
       "      <td>10354</td>\n",
       "      <td>10449</td>\n",
       "      <td>10451</td>\n",
       "      <td>10419</td>\n",
       "      <td>10409</td>\n",
       "      <td>10336</td>\n",
       "      <td>10306</td>\n",
       "      <td>10356</td>\n",
       "      <td>10461</td>\n",
       "      <td>10456</td>\n",
       "      <td>10460</td>\n",
       "      <td>10467</td>\n",
       "      <td>10422</td>\n",
       "      <td>10303</td>\n",
       "      <td>9877</td>\n",
       "      <td>9308</td>\n",
       "      <td>9061</td>\n",
       "      <td>9299</td>\n",
       "      <td>9748</td>\n",
       "      <td>10209</td>\n",
       "      <td>10615</td>\n",
       "      <td>10975</td>\n",
       "      <td>11178</td>\n",
       "      <td>11197</td>\n",
       "      <td>11161</td>\n",
       "      <td>11096</td>\n",
       "      <td>10957</td>\n",
       "      <td>10839</td>\n",
       "      <td>10735</td>\n",
       "      <td>10590</td>\n",
       "      <td>10411</td>\n",
       "      <td>10329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:20:12</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>10364</td>\n",
       "      <td>10907</td>\n",
       "      <td>11299</td>\n",
       "      <td>11238</td>\n",
       "      <td>10867</td>\n",
       "      <td>10535</td>\n",
       "      <td>10173</td>\n",
       "      <td>9950</td>\n",
       "      <td>9856</td>\n",
       "      <td>9795</td>\n",
       "      <td>9714</td>\n",
       "      <td>9702</td>\n",
       "      <td>9792</td>\n",
       "      <td>9789</td>\n",
       "      <td>9915</td>\n",
       "      <td>9900</td>\n",
       "      <td>9944</td>\n",
       "      <td>9964</td>\n",
       "      <td>9971</td>\n",
       "      <td>10059</td>\n",
       "      <td>10161</td>\n",
       "      <td>10234</td>\n",
       "      <td>10285</td>\n",
       "      <td>10309</td>\n",
       "      <td>10384</td>\n",
       "      <td>10464</td>\n",
       "      <td>10450</td>\n",
       "      <td>10427</td>\n",
       "      <td>10366</td>\n",
       "      <td>10361</td>\n",
       "      <td>10452</td>\n",
       "      <td>10502</td>\n",
       "      <td>10444</td>\n",
       "      <td>10337</td>\n",
       "      <td>10250</td>\n",
       "      <td>10313</td>\n",
       "      <td>10211</td>\n",
       "      <td>9718</td>\n",
       "      <td>9236</td>\n",
       "      <td>9193</td>\n",
       "      <td>9609</td>\n",
       "      <td>10022</td>\n",
       "      <td>10431</td>\n",
       "      <td>10798</td>\n",
       "      <td>11055</td>\n",
       "      <td>11122</td>\n",
       "      <td>11145</td>\n",
       "      <td>11136</td>\n",
       "      <td>11108</td>\n",
       "      <td>11041</td>\n",
       "      <td>10824</td>\n",
       "      <td>10645</td>\n",
       "      <td>10493</td>\n",
       "      <td>10398</td>\n",
       "      <td>10357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:20:28</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10329</td>\n",
       "      <td>10793</td>\n",
       "      <td>11197</td>\n",
       "      <td>11242</td>\n",
       "      <td>11052</td>\n",
       "      <td>10658</td>\n",
       "      <td>10288</td>\n",
       "      <td>9988</td>\n",
       "      <td>9819</td>\n",
       "      <td>9711</td>\n",
       "      <td>9659</td>\n",
       "      <td>9626</td>\n",
       "      <td>9726</td>\n",
       "      <td>9752</td>\n",
       "      <td>9835</td>\n",
       "      <td>9942</td>\n",
       "      <td>9925</td>\n",
       "      <td>9965</td>\n",
       "      <td>10110</td>\n",
       "      <td>10174</td>\n",
       "      <td>10140</td>\n",
       "      <td>10235</td>\n",
       "      <td>10303</td>\n",
       "      <td>10365</td>\n",
       "      <td>10366</td>\n",
       "      <td>10379</td>\n",
       "      <td>10375</td>\n",
       "      <td>10287</td>\n",
       "      <td>10310</td>\n",
       "      <td>10345</td>\n",
       "      <td>10373</td>\n",
       "      <td>10328</td>\n",
       "      <td>10387</td>\n",
       "      <td>10415</td>\n",
       "      <td>10491</td>\n",
       "      <td>10421</td>\n",
       "      <td>10432</td>\n",
       "      <td>9964</td>\n",
       "      <td>9368</td>\n",
       "      <td>9135</td>\n",
       "      <td>9287</td>\n",
       "      <td>9643</td>\n",
       "      <td>10184</td>\n",
       "      <td>10663</td>\n",
       "      <td>11016</td>\n",
       "      <td>11168</td>\n",
       "      <td>11204</td>\n",
       "      <td>11162</td>\n",
       "      <td>11109</td>\n",
       "      <td>11007</td>\n",
       "      <td>10867</td>\n",
       "      <td>10700</td>\n",
       "      <td>10533</td>\n",
       "      <td>10427</td>\n",
       "      <td>10265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:20:44</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10169</td>\n",
       "      <td>10425</td>\n",
       "      <td>10822</td>\n",
       "      <td>11133</td>\n",
       "      <td>11136</td>\n",
       "      <td>10834</td>\n",
       "      <td>10520</td>\n",
       "      <td>10228</td>\n",
       "      <td>9986</td>\n",
       "      <td>9848</td>\n",
       "      <td>9643</td>\n",
       "      <td>9562</td>\n",
       "      <td>9591</td>\n",
       "      <td>9618</td>\n",
       "      <td>9718</td>\n",
       "      <td>9849</td>\n",
       "      <td>9857</td>\n",
       "      <td>10026</td>\n",
       "      <td>10150</td>\n",
       "      <td>10198</td>\n",
       "      <td>10261</td>\n",
       "      <td>10351</td>\n",
       "      <td>10425</td>\n",
       "      <td>10469</td>\n",
       "      <td>10374</td>\n",
       "      <td>10344</td>\n",
       "      <td>10303</td>\n",
       "      <td>10293</td>\n",
       "      <td>10294</td>\n",
       "      <td>10333</td>\n",
       "      <td>10353</td>\n",
       "      <td>10345</td>\n",
       "      <td>10354</td>\n",
       "      <td>10362</td>\n",
       "      <td>10375</td>\n",
       "      <td>10369</td>\n",
       "      <td>10319</td>\n",
       "      <td>10115</td>\n",
       "      <td>9603</td>\n",
       "      <td>9182</td>\n",
       "      <td>9125</td>\n",
       "      <td>9560</td>\n",
       "      <td>10161</td>\n",
       "      <td>10560</td>\n",
       "      <td>10883</td>\n",
       "      <td>11116</td>\n",
       "      <td>11273</td>\n",
       "      <td>11186</td>\n",
       "      <td>10984</td>\n",
       "      <td>10910</td>\n",
       "      <td>10807</td>\n",
       "      <td>10714</td>\n",
       "      <td>10651</td>\n",
       "      <td>10562</td>\n",
       "      <td>10463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:21:00</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10320</td>\n",
       "      <td>10667</td>\n",
       "      <td>11104</td>\n",
       "      <td>11234</td>\n",
       "      <td>11129</td>\n",
       "      <td>10814</td>\n",
       "      <td>10453</td>\n",
       "      <td>10040</td>\n",
       "      <td>9733</td>\n",
       "      <td>9630</td>\n",
       "      <td>9578</td>\n",
       "      <td>9476</td>\n",
       "      <td>9596</td>\n",
       "      <td>9748</td>\n",
       "      <td>9755</td>\n",
       "      <td>9823</td>\n",
       "      <td>10004</td>\n",
       "      <td>10048</td>\n",
       "      <td>10202</td>\n",
       "      <td>10234</td>\n",
       "      <td>10255</td>\n",
       "      <td>10282</td>\n",
       "      <td>10298</td>\n",
       "      <td>10319</td>\n",
       "      <td>10315</td>\n",
       "      <td>10270</td>\n",
       "      <td>10334</td>\n",
       "      <td>10400</td>\n",
       "      <td>10428</td>\n",
       "      <td>10514</td>\n",
       "      <td>10529</td>\n",
       "      <td>10453</td>\n",
       "      <td>10374</td>\n",
       "      <td>10303</td>\n",
       "      <td>10298</td>\n",
       "      <td>10238</td>\n",
       "      <td>10246</td>\n",
       "      <td>9918</td>\n",
       "      <td>9399</td>\n",
       "      <td>9198</td>\n",
       "      <td>9422</td>\n",
       "      <td>9848</td>\n",
       "      <td>10225</td>\n",
       "      <td>10615</td>\n",
       "      <td>10860</td>\n",
       "      <td>11006</td>\n",
       "      <td>11257</td>\n",
       "      <td>11370</td>\n",
       "      <td>11173</td>\n",
       "      <td>10924</td>\n",
       "      <td>10816</td>\n",
       "      <td>10754</td>\n",
       "      <td>10588</td>\n",
       "      <td>10428</td>\n",
       "      <td>10407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time  Label  Temperature_F  PIR_1  PIR_2  PIR_3  PIR_4  \\\n",
       "0  2024-08-08  19:19:56      0             86  10269  10721  11156  11170   \n",
       "1  2024-08-08  19:20:12      1             86  10364  10907  11299  11238   \n",
       "2  2024-08-08  19:20:28      0             86  10329  10793  11197  11242   \n",
       "3  2024-08-08  19:20:44      0             86  10169  10425  10822  11133   \n",
       "4  2024-08-08  19:21:00      0             86  10320  10667  11104  11234   \n",
       "\n",
       "   PIR_5  PIR_6  PIR_7  PIR_8  PIR_9  PIR_10  PIR_11  PIR_12  PIR_13  PIR_14  \\\n",
       "0  10931  10671  10395  10133   9885    9705    9538    9418    9469    9599   \n",
       "1  10867  10535  10173   9950   9856    9795    9714    9702    9792    9789   \n",
       "2  11052  10658  10288   9988   9819    9711    9659    9626    9726    9752   \n",
       "3  11136  10834  10520  10228   9986    9848    9643    9562    9591    9618   \n",
       "4  11129  10814  10453  10040   9733    9630    9578    9476    9596    9748   \n",
       "\n",
       "   PIR_15  PIR_16  PIR_17  PIR_18  PIR_19  PIR_20  PIR_21  PIR_22  PIR_23  \\\n",
       "0    9817    9910    9890   10075   10231   10247   10271   10229   10272   \n",
       "1    9915    9900    9944    9964    9971   10059   10161   10234   10285   \n",
       "2    9835    9942    9925    9965   10110   10174   10140   10235   10303   \n",
       "3    9718    9849    9857   10026   10150   10198   10261   10351   10425   \n",
       "4    9755    9823   10004   10048   10202   10234   10255   10282   10298   \n",
       "\n",
       "   PIR_24  PIR_25  PIR_26  PIR_27  PIR_28  PIR_29  PIR_30  PIR_31  PIR_32  \\\n",
       "0   10354   10449   10451   10419   10409   10336   10306   10356   10461   \n",
       "1   10309   10384   10464   10450   10427   10366   10361   10452   10502   \n",
       "2   10365   10366   10379   10375   10287   10310   10345   10373   10328   \n",
       "3   10469   10374   10344   10303   10293   10294   10333   10353   10345   \n",
       "4   10319   10315   10270   10334   10400   10428   10514   10529   10453   \n",
       "\n",
       "   PIR_33  PIR_34  PIR_35  PIR_36  PIR_37  PIR_38  PIR_39  PIR_40  PIR_41  \\\n",
       "0   10456   10460   10467   10422   10303    9877    9308    9061    9299   \n",
       "1   10444   10337   10250   10313   10211    9718    9236    9193    9609   \n",
       "2   10387   10415   10491   10421   10432    9964    9368    9135    9287   \n",
       "3   10354   10362   10375   10369   10319   10115    9603    9182    9125   \n",
       "4   10374   10303   10298   10238   10246    9918    9399    9198    9422   \n",
       "\n",
       "   PIR_42  PIR_43  PIR_44  PIR_45  PIR_46  PIR_47  PIR_48  PIR_49  PIR_50  \\\n",
       "0    9748   10209   10615   10975   11178   11197   11161   11096   10957   \n",
       "1   10022   10431   10798   11055   11122   11145   11136   11108   11041   \n",
       "2    9643   10184   10663   11016   11168   11204   11162   11109   11007   \n",
       "3    9560   10161   10560   10883   11116   11273   11186   10984   10910   \n",
       "4    9848   10225   10615   10860   11006   11257   11370   11173   10924   \n",
       "\n",
       "   PIR_51  PIR_52  PIR_53  PIR_54  PIR_55  \n",
       "0   10839   10735   10590   10411   10329  \n",
       "1   10824   10645   10493   10398   10357  \n",
       "2   10867   10700   10533   10427   10265  \n",
       "3   10807   10714   10651   10562   10463  \n",
       "4   10816   10754   10588   10428   10407  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #We first shuffle these 2 dataframes\n",
    "# df1 = df1.sample(frac = 1, random_state=1).reset_index(drop=True)\n",
    "# df2 = df2.sample(frac = 1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "#We now merge these 2 dataframes\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "#We print the shapes of all datafmrames\n",
    "print(df1.shape, df2.shape, df.shape)\n",
    "\n",
    "#Displaying the merged dataframe\n",
    "display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date' and 'Time' are parsed correctly\n",
    "df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "\n",
    "# Extract temporal features\n",
    "df['Hour'] = df['Datetime'].dt.hour\n",
    "df['Minute'] = df['Datetime'].dt.minute\n",
    "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
    "df['Month'] = df['Datetime'].dt.month\n",
    "\n",
    "meta_features = ['Hour', 'Minute', 'DayOfWeek', 'Month', 'Temperature_F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Label</th>\n",
       "      <th>Temperature_F</th>\n",
       "      <th>PIR_1</th>\n",
       "      <th>PIR_2</th>\n",
       "      <th>PIR_3</th>\n",
       "      <th>PIR_4</th>\n",
       "      <th>PIR_5</th>\n",
       "      <th>PIR_6</th>\n",
       "      <th>PIR_7</th>\n",
       "      <th>PIR_8</th>\n",
       "      <th>PIR_9</th>\n",
       "      <th>PIR_10</th>\n",
       "      <th>PIR_11</th>\n",
       "      <th>PIR_12</th>\n",
       "      <th>PIR_13</th>\n",
       "      <th>PIR_14</th>\n",
       "      <th>PIR_15</th>\n",
       "      <th>PIR_16</th>\n",
       "      <th>PIR_17</th>\n",
       "      <th>PIR_18</th>\n",
       "      <th>PIR_19</th>\n",
       "      <th>PIR_20</th>\n",
       "      <th>PIR_21</th>\n",
       "      <th>PIR_22</th>\n",
       "      <th>PIR_23</th>\n",
       "      <th>PIR_24</th>\n",
       "      <th>PIR_25</th>\n",
       "      <th>PIR_26</th>\n",
       "      <th>PIR_27</th>\n",
       "      <th>PIR_28</th>\n",
       "      <th>PIR_29</th>\n",
       "      <th>PIR_30</th>\n",
       "      <th>PIR_31</th>\n",
       "      <th>PIR_32</th>\n",
       "      <th>PIR_33</th>\n",
       "      <th>PIR_34</th>\n",
       "      <th>PIR_35</th>\n",
       "      <th>PIR_36</th>\n",
       "      <th>PIR_37</th>\n",
       "      <th>PIR_38</th>\n",
       "      <th>PIR_39</th>\n",
       "      <th>PIR_40</th>\n",
       "      <th>PIR_41</th>\n",
       "      <th>PIR_42</th>\n",
       "      <th>PIR_43</th>\n",
       "      <th>PIR_44</th>\n",
       "      <th>PIR_45</th>\n",
       "      <th>PIR_46</th>\n",
       "      <th>PIR_47</th>\n",
       "      <th>PIR_48</th>\n",
       "      <th>PIR_49</th>\n",
       "      <th>PIR_50</th>\n",
       "      <th>PIR_51</th>\n",
       "      <th>PIR_52</th>\n",
       "      <th>PIR_53</th>\n",
       "      <th>PIR_54</th>\n",
       "      <th>PIR_55</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:19:56</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10269</td>\n",
       "      <td>10721</td>\n",
       "      <td>11156</td>\n",
       "      <td>11170</td>\n",
       "      <td>10931</td>\n",
       "      <td>10671</td>\n",
       "      <td>10395</td>\n",
       "      <td>10133</td>\n",
       "      <td>9885</td>\n",
       "      <td>9705</td>\n",
       "      <td>9538</td>\n",
       "      <td>9418</td>\n",
       "      <td>9469</td>\n",
       "      <td>9599</td>\n",
       "      <td>9817</td>\n",
       "      <td>9910</td>\n",
       "      <td>9890</td>\n",
       "      <td>10075</td>\n",
       "      <td>10231</td>\n",
       "      <td>10247</td>\n",
       "      <td>10271</td>\n",
       "      <td>10229</td>\n",
       "      <td>10272</td>\n",
       "      <td>10354</td>\n",
       "      <td>10449</td>\n",
       "      <td>10451</td>\n",
       "      <td>10419</td>\n",
       "      <td>10409</td>\n",
       "      <td>10336</td>\n",
       "      <td>10306</td>\n",
       "      <td>10356</td>\n",
       "      <td>10461</td>\n",
       "      <td>10456</td>\n",
       "      <td>10460</td>\n",
       "      <td>10467</td>\n",
       "      <td>10422</td>\n",
       "      <td>10303</td>\n",
       "      <td>9877</td>\n",
       "      <td>9308</td>\n",
       "      <td>9061</td>\n",
       "      <td>9299</td>\n",
       "      <td>9748</td>\n",
       "      <td>10209</td>\n",
       "      <td>10615</td>\n",
       "      <td>10975</td>\n",
       "      <td>11178</td>\n",
       "      <td>11197</td>\n",
       "      <td>11161</td>\n",
       "      <td>11096</td>\n",
       "      <td>10957</td>\n",
       "      <td>10839</td>\n",
       "      <td>10735</td>\n",
       "      <td>10590</td>\n",
       "      <td>10411</td>\n",
       "      <td>10329</td>\n",
       "      <td>2024-08-08 19:19:56</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:20:12</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>10364</td>\n",
       "      <td>10907</td>\n",
       "      <td>11299</td>\n",
       "      <td>11238</td>\n",
       "      <td>10867</td>\n",
       "      <td>10535</td>\n",
       "      <td>10173</td>\n",
       "      <td>9950</td>\n",
       "      <td>9856</td>\n",
       "      <td>9795</td>\n",
       "      <td>9714</td>\n",
       "      <td>9702</td>\n",
       "      <td>9792</td>\n",
       "      <td>9789</td>\n",
       "      <td>9915</td>\n",
       "      <td>9900</td>\n",
       "      <td>9944</td>\n",
       "      <td>9964</td>\n",
       "      <td>9971</td>\n",
       "      <td>10059</td>\n",
       "      <td>10161</td>\n",
       "      <td>10234</td>\n",
       "      <td>10285</td>\n",
       "      <td>10309</td>\n",
       "      <td>10384</td>\n",
       "      <td>10464</td>\n",
       "      <td>10450</td>\n",
       "      <td>10427</td>\n",
       "      <td>10366</td>\n",
       "      <td>10361</td>\n",
       "      <td>10452</td>\n",
       "      <td>10502</td>\n",
       "      <td>10444</td>\n",
       "      <td>10337</td>\n",
       "      <td>10250</td>\n",
       "      <td>10313</td>\n",
       "      <td>10211</td>\n",
       "      <td>9718</td>\n",
       "      <td>9236</td>\n",
       "      <td>9193</td>\n",
       "      <td>9609</td>\n",
       "      <td>10022</td>\n",
       "      <td>10431</td>\n",
       "      <td>10798</td>\n",
       "      <td>11055</td>\n",
       "      <td>11122</td>\n",
       "      <td>11145</td>\n",
       "      <td>11136</td>\n",
       "      <td>11108</td>\n",
       "      <td>11041</td>\n",
       "      <td>10824</td>\n",
       "      <td>10645</td>\n",
       "      <td>10493</td>\n",
       "      <td>10398</td>\n",
       "      <td>10357</td>\n",
       "      <td>2024-08-08 19:20:12</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:20:28</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10329</td>\n",
       "      <td>10793</td>\n",
       "      <td>11197</td>\n",
       "      <td>11242</td>\n",
       "      <td>11052</td>\n",
       "      <td>10658</td>\n",
       "      <td>10288</td>\n",
       "      <td>9988</td>\n",
       "      <td>9819</td>\n",
       "      <td>9711</td>\n",
       "      <td>9659</td>\n",
       "      <td>9626</td>\n",
       "      <td>9726</td>\n",
       "      <td>9752</td>\n",
       "      <td>9835</td>\n",
       "      <td>9942</td>\n",
       "      <td>9925</td>\n",
       "      <td>9965</td>\n",
       "      <td>10110</td>\n",
       "      <td>10174</td>\n",
       "      <td>10140</td>\n",
       "      <td>10235</td>\n",
       "      <td>10303</td>\n",
       "      <td>10365</td>\n",
       "      <td>10366</td>\n",
       "      <td>10379</td>\n",
       "      <td>10375</td>\n",
       "      <td>10287</td>\n",
       "      <td>10310</td>\n",
       "      <td>10345</td>\n",
       "      <td>10373</td>\n",
       "      <td>10328</td>\n",
       "      <td>10387</td>\n",
       "      <td>10415</td>\n",
       "      <td>10491</td>\n",
       "      <td>10421</td>\n",
       "      <td>10432</td>\n",
       "      <td>9964</td>\n",
       "      <td>9368</td>\n",
       "      <td>9135</td>\n",
       "      <td>9287</td>\n",
       "      <td>9643</td>\n",
       "      <td>10184</td>\n",
       "      <td>10663</td>\n",
       "      <td>11016</td>\n",
       "      <td>11168</td>\n",
       "      <td>11204</td>\n",
       "      <td>11162</td>\n",
       "      <td>11109</td>\n",
       "      <td>11007</td>\n",
       "      <td>10867</td>\n",
       "      <td>10700</td>\n",
       "      <td>10533</td>\n",
       "      <td>10427</td>\n",
       "      <td>10265</td>\n",
       "      <td>2024-08-08 19:20:28</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:20:44</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10169</td>\n",
       "      <td>10425</td>\n",
       "      <td>10822</td>\n",
       "      <td>11133</td>\n",
       "      <td>11136</td>\n",
       "      <td>10834</td>\n",
       "      <td>10520</td>\n",
       "      <td>10228</td>\n",
       "      <td>9986</td>\n",
       "      <td>9848</td>\n",
       "      <td>9643</td>\n",
       "      <td>9562</td>\n",
       "      <td>9591</td>\n",
       "      <td>9618</td>\n",
       "      <td>9718</td>\n",
       "      <td>9849</td>\n",
       "      <td>9857</td>\n",
       "      <td>10026</td>\n",
       "      <td>10150</td>\n",
       "      <td>10198</td>\n",
       "      <td>10261</td>\n",
       "      <td>10351</td>\n",
       "      <td>10425</td>\n",
       "      <td>10469</td>\n",
       "      <td>10374</td>\n",
       "      <td>10344</td>\n",
       "      <td>10303</td>\n",
       "      <td>10293</td>\n",
       "      <td>10294</td>\n",
       "      <td>10333</td>\n",
       "      <td>10353</td>\n",
       "      <td>10345</td>\n",
       "      <td>10354</td>\n",
       "      <td>10362</td>\n",
       "      <td>10375</td>\n",
       "      <td>10369</td>\n",
       "      <td>10319</td>\n",
       "      <td>10115</td>\n",
       "      <td>9603</td>\n",
       "      <td>9182</td>\n",
       "      <td>9125</td>\n",
       "      <td>9560</td>\n",
       "      <td>10161</td>\n",
       "      <td>10560</td>\n",
       "      <td>10883</td>\n",
       "      <td>11116</td>\n",
       "      <td>11273</td>\n",
       "      <td>11186</td>\n",
       "      <td>10984</td>\n",
       "      <td>10910</td>\n",
       "      <td>10807</td>\n",
       "      <td>10714</td>\n",
       "      <td>10651</td>\n",
       "      <td>10562</td>\n",
       "      <td>10463</td>\n",
       "      <td>2024-08-08 19:20:44</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:21:00</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10320</td>\n",
       "      <td>10667</td>\n",
       "      <td>11104</td>\n",
       "      <td>11234</td>\n",
       "      <td>11129</td>\n",
       "      <td>10814</td>\n",
       "      <td>10453</td>\n",
       "      <td>10040</td>\n",
       "      <td>9733</td>\n",
       "      <td>9630</td>\n",
       "      <td>9578</td>\n",
       "      <td>9476</td>\n",
       "      <td>9596</td>\n",
       "      <td>9748</td>\n",
       "      <td>9755</td>\n",
       "      <td>9823</td>\n",
       "      <td>10004</td>\n",
       "      <td>10048</td>\n",
       "      <td>10202</td>\n",
       "      <td>10234</td>\n",
       "      <td>10255</td>\n",
       "      <td>10282</td>\n",
       "      <td>10298</td>\n",
       "      <td>10319</td>\n",
       "      <td>10315</td>\n",
       "      <td>10270</td>\n",
       "      <td>10334</td>\n",
       "      <td>10400</td>\n",
       "      <td>10428</td>\n",
       "      <td>10514</td>\n",
       "      <td>10529</td>\n",
       "      <td>10453</td>\n",
       "      <td>10374</td>\n",
       "      <td>10303</td>\n",
       "      <td>10298</td>\n",
       "      <td>10238</td>\n",
       "      <td>10246</td>\n",
       "      <td>9918</td>\n",
       "      <td>9399</td>\n",
       "      <td>9198</td>\n",
       "      <td>9422</td>\n",
       "      <td>9848</td>\n",
       "      <td>10225</td>\n",
       "      <td>10615</td>\n",
       "      <td>10860</td>\n",
       "      <td>11006</td>\n",
       "      <td>11257</td>\n",
       "      <td>11370</td>\n",
       "      <td>11173</td>\n",
       "      <td>10924</td>\n",
       "      <td>10816</td>\n",
       "      <td>10754</td>\n",
       "      <td>10588</td>\n",
       "      <td>10428</td>\n",
       "      <td>10407</td>\n",
       "      <td>2024-08-08 19:21:00</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time  Label  Temperature_F  PIR_1  PIR_2  PIR_3  PIR_4  \\\n",
       "0  2024-08-08  19:19:56      0             86  10269  10721  11156  11170   \n",
       "1  2024-08-08  19:20:12      1             86  10364  10907  11299  11238   \n",
       "2  2024-08-08  19:20:28      0             86  10329  10793  11197  11242   \n",
       "3  2024-08-08  19:20:44      0             86  10169  10425  10822  11133   \n",
       "4  2024-08-08  19:21:00      0             86  10320  10667  11104  11234   \n",
       "\n",
       "   PIR_5  PIR_6  PIR_7  PIR_8  PIR_9  PIR_10  PIR_11  PIR_12  PIR_13  PIR_14  \\\n",
       "0  10931  10671  10395  10133   9885    9705    9538    9418    9469    9599   \n",
       "1  10867  10535  10173   9950   9856    9795    9714    9702    9792    9789   \n",
       "2  11052  10658  10288   9988   9819    9711    9659    9626    9726    9752   \n",
       "3  11136  10834  10520  10228   9986    9848    9643    9562    9591    9618   \n",
       "4  11129  10814  10453  10040   9733    9630    9578    9476    9596    9748   \n",
       "\n",
       "   PIR_15  PIR_16  PIR_17  PIR_18  PIR_19  PIR_20  PIR_21  PIR_22  PIR_23  \\\n",
       "0    9817    9910    9890   10075   10231   10247   10271   10229   10272   \n",
       "1    9915    9900    9944    9964    9971   10059   10161   10234   10285   \n",
       "2    9835    9942    9925    9965   10110   10174   10140   10235   10303   \n",
       "3    9718    9849    9857   10026   10150   10198   10261   10351   10425   \n",
       "4    9755    9823   10004   10048   10202   10234   10255   10282   10298   \n",
       "\n",
       "   PIR_24  PIR_25  PIR_26  PIR_27  PIR_28  PIR_29  PIR_30  PIR_31  PIR_32  \\\n",
       "0   10354   10449   10451   10419   10409   10336   10306   10356   10461   \n",
       "1   10309   10384   10464   10450   10427   10366   10361   10452   10502   \n",
       "2   10365   10366   10379   10375   10287   10310   10345   10373   10328   \n",
       "3   10469   10374   10344   10303   10293   10294   10333   10353   10345   \n",
       "4   10319   10315   10270   10334   10400   10428   10514   10529   10453   \n",
       "\n",
       "   PIR_33  PIR_34  PIR_35  PIR_36  PIR_37  PIR_38  PIR_39  PIR_40  PIR_41  \\\n",
       "0   10456   10460   10467   10422   10303    9877    9308    9061    9299   \n",
       "1   10444   10337   10250   10313   10211    9718    9236    9193    9609   \n",
       "2   10387   10415   10491   10421   10432    9964    9368    9135    9287   \n",
       "3   10354   10362   10375   10369   10319   10115    9603    9182    9125   \n",
       "4   10374   10303   10298   10238   10246    9918    9399    9198    9422   \n",
       "\n",
       "   PIR_42  PIR_43  PIR_44  PIR_45  PIR_46  PIR_47  PIR_48  PIR_49  PIR_50  \\\n",
       "0    9748   10209   10615   10975   11178   11197   11161   11096   10957   \n",
       "1   10022   10431   10798   11055   11122   11145   11136   11108   11041   \n",
       "2    9643   10184   10663   11016   11168   11204   11162   11109   11007   \n",
       "3    9560   10161   10560   10883   11116   11273   11186   10984   10910   \n",
       "4    9848   10225   10615   10860   11006   11257   11370   11173   10924   \n",
       "\n",
       "   PIR_51  PIR_52  PIR_53  PIR_54  PIR_55            Datetime  Hour  Minute  \\\n",
       "0   10839   10735   10590   10411   10329 2024-08-08 19:19:56    19      19   \n",
       "1   10824   10645   10493   10398   10357 2024-08-08 19:20:12    19      20   \n",
       "2   10867   10700   10533   10427   10265 2024-08-08 19:20:28    19      20   \n",
       "3   10807   10714   10651   10562   10463 2024-08-08 19:20:44    19      20   \n",
       "4   10816   10754   10588   10428   10407 2024-08-08 19:21:00    19      21   \n",
       "\n",
       "   DayOfWeek  Month  \n",
       "0          3      8  \n",
       "1          3      8  \n",
       "2          3      8  \n",
       "3          3      8  \n",
       "4          3      8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class HybridTransformer(nn.Module):\n",
    "    def __init__(self, input_size=1, meta_input_size=5, seq_len=55, d_model=64, nhead=4, num_layers=9, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, d_model)  # PIR projection\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=128, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.meta_mlp = nn.Sequential(\n",
    "            nn.Linear(meta_input_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, d_model),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, 64),  # Transformer + Metadata\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq, x_meta):\n",
    "        # x_seq: (batch_size, seq_len, 1)\n",
    "        x_seq = self.input_proj(x_seq)         # (B, seq_len, d_model)\n",
    "        x_seq = self.pos_encoder(x_seq)        # Add positional encoding\n",
    "        x_seq = self.transformer_encoder(x_seq)  # (B, seq_len, d_model)\n",
    "        x_seq = x_seq.mean(dim=1)              # Mean pooling over time\n",
    "\n",
    "        # x_meta: (batch_size, meta_input_size)\n",
    "        x_meta = self.meta_mlp(x_meta)         # â†’ (B, d_model)\n",
    "\n",
    "        combined = torch.cat([x_seq, x_meta], dim=1)  # (B, 2*d_model)\n",
    "        return self.classifier(combined)              # (B, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in df:\n",
      "Label\n",
      "0    12494\n",
      "1     1666\n",
      "3     1142\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution in df:\")\n",
    "print(df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIR_columns = [f'PIR_{i}' for i in range(1, 56)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Train label distribution: [10041 10041 10041]\n",
      "Test label distribution: [2453  345  263]\n",
      "epoch 1, loss: 0.2649\n",
      "epoch 2, loss: 0.1392\n",
      "epoch 3, loss: 0.1304\n",
      "epoch 4, loss: 0.1269\n",
      "epoch 5, loss: 0.1173\n",
      "epoch 6, loss: 0.1126\n",
      "epoch 7, loss: 0.1069\n",
      "epoch 8, loss: 0.1035\n",
      "\n",
      "fold 1 test accuracy: 0.9510\n",
      "\n",
      "Per-class accuracy:\n",
      "  Class 0: 2368/2453 correct (96.53%)\n",
      "  Class 1: 280/345 correct (81.16%)\n",
      "  Class 2: 263/263 correct (100.00%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9733    0.9653    0.9693      2453\n",
      "           1     0.7671    0.8116    0.7887       345\n",
      "           2     1.0000    1.0000    1.0000       263\n",
      "\n",
      "    accuracy                         0.9510      3061\n",
      "   macro avg     0.9135    0.9256    0.9193      3061\n",
      "weighted avg     0.9523    0.9510    0.9516      3061\n",
      "\n",
      "Macro F1-Score: 0.9193\n",
      "\n",
      "Average accuracy until fold 1 is : 0.9510\n",
      "Average F1-Score until fold 1 is : 0.9193\n",
      "\n",
      "Fold 2\n",
      "Train label distribution: [9991 9991 9991]\n",
      "Test label distribution: [2503  336  222]\n",
      "epoch 1, loss: 0.3118\n",
      "epoch 2, loss: 0.1436\n",
      "epoch 3, loss: 0.1360\n",
      "epoch 4, loss: 0.1295\n",
      "epoch 5, loss: 0.1252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequences, metas, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m---> 99\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetas\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m    102\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 48\u001b[0m, in \u001b[0;36mHybridTransformer.forward\u001b[1;34m(self, x_seq, x_meta)\u001b[0m\n\u001b[0;32m     46\u001b[0m x_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_proj(x_seq)         \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m x_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(x_seq)        \u001b[38;5;66;03m# Add positional encoding\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m x_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m x_seq \u001b[38;5;241m=\u001b[39m x_seq\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)              \u001b[38;5;66;03m# Mean pooling over time\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# x_meta: (batch_size, meta_input_size)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:415\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    412\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 415\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    418\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:749\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    747\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 749\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    750\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:757\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    756\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 757\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1266\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1252\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1253\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1264\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1266\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\assignmentenv\\lib\\site-packages\\torch\\nn\\functional.py:5504\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5501\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   5502\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m-> 5504\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5505\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   5507\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "class HybridTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_seq, X_meta, y):\n",
    "        self.X_seq = X_seq\n",
    "        self.X_meta = X_meta\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_seq)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.X_seq[idx]                # shape: (55,)\n",
    "        meta = self.X_meta[idx]                   # shape: (9,)\n",
    "        label = self.y[idx]\n",
    "\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.float32).unsqueeze(1)  # (55, 1)\n",
    "        meta_tensor = torch.tensor(meta, dtype=torch.float32)                       # (9,)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return sequence_tensor, meta_tensor, label_tensor\n",
    "\n",
    "pir_columns = [f'PIR_{i}' for i in range(1, 56)]\n",
    "X_seq = df[pir_columns].values\n",
    "\n",
    "# Metadata features\n",
    "X_meta = df[meta_features].values\n",
    "\n",
    "# Labels (remapped as before)\n",
    "label_map = {0: 0, 1: 1, 3: 2}\n",
    "y_raw = df[\"Label\"].values\n",
    "y = np.array([label_map[label] for label in y_raw])\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "input_size = 1\n",
    "\n",
    "# Calculate class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y)\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Use in CrossEntropyLoss\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "nb_epochs = 8\n",
    "batch_size = 64  # you can adjust this based on memory\n",
    "accs = []\n",
    "f1_scores = []\n",
    "\n",
    "for fold, (train_i, test_i) in enumerate(kf.split(X_seq), 1):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "\n",
    "    model = HybridTransformer(input_size=input_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    X_seq_train, X_seq_test = X_seq[train_i], X_seq[test_i]\n",
    "    X_meta_train, X_meta_test = X_meta[train_i], X_meta[test_i]\n",
    "    y_train, y_test = y[train_i], y[test_i]\n",
    "\n",
    "    X_seq_train = pd.DataFrame(X_seq_train, columns=PIR_columns)\n",
    "    X_meta_train = pd.DataFrame(X_meta_train, columns=meta_features)\n",
    "\n",
    "    X_dummy = pd.concat([X_seq_train, X_meta_train], axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "    #Random Oversampler\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    # ros = RandomOverSampler(random_state=0)\n",
    "    # X_dummy, y_train = ros.fit_resample(X_dummy, y_train)\n",
    "\n",
    "    #SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_dummy, y_train = smote.fit_resample(X_dummy, y_train)\n",
    "\n",
    "    X_seq_train = X_dummy[PIR_columns].values\n",
    "    X_meta_train = X_dummy[meta_features].values\n",
    "\n",
    "\n",
    "    print(\"Train label distribution:\", np.bincount(y_train))\n",
    "    print(\"Test label distribution:\", np.bincount(y_test))\n",
    "\n",
    "    train_dataset = HybridTimeSeriesDataset(X_seq_train, X_meta_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for sequences, metas, labels in train_loader:\n",
    "            output = model(sequences, metas)  # Forward pass\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * sequences.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataset)\n",
    "        print(f\"epoch {epoch+1}, loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_counts = {0: 0, 1: 0, 2: 0}\n",
    "    class_correct = {0: 0, 1: 0, 2: 0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(X_seq_test.shape[0]):\n",
    "            sequence_tensor = torch.tensor(X_seq_test[i], dtype=torch.float32).unsqueeze(0).unsqueeze(2)\n",
    "            meta_tensor = torch.tensor(X_meta_test[i], dtype=torch.float32).unsqueeze(0)\n",
    "            label = y_test[i]\n",
    "\n",
    "            output = model(sequence_tensor, meta_tensor)\n",
    "            predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "            y_true.append(label)\n",
    "            y_pred.append(predicted_class)\n",
    "\n",
    "            class_counts[label] += 1\n",
    "            if predicted_class == label:\n",
    "                correct += 1\n",
    "                class_correct[label] += 1\n",
    "            total += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"\\nfold {fold} test accuracy: {accuracy:.4f}\")\n",
    "    accs.append(accuracy)\n",
    "\n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for cls in class_counts:\n",
    "        total_cls = class_counts[cls]\n",
    "        correct_cls = class_correct[cls]\n",
    "        acc_cls = correct_cls / total_cls if total_cls > 0 else 0.0\n",
    "        print(f\"  Class {cls}: {correct_cls}/{total_cls} correct ({acc_cls * 100:.2f}%)\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_scores.append(macro_f1)\n",
    "    print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "\n",
    "    avg_acc = np.mean(accs)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    print(f\"\\nAverage accuracy until fold {fold} is : {avg_acc:.4f}\")\n",
    "    print(f\"Average F1-Score until fold {fold} is : {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignmentenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
