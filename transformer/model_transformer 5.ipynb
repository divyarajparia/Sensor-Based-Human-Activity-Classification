{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer + extra PIRs (109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We first read the 2 data files\n",
    "df1 = pd.read_csv('pirvision_office_dataset1.csv')\n",
    "df2 = pd.read_csv('pirvision_office_dataset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7651, 59) (7651, 59) (15302, 59)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Label</th>\n",
       "      <th>Temperature_F</th>\n",
       "      <th>PIR_1</th>\n",
       "      <th>PIR_2</th>\n",
       "      <th>PIR_3</th>\n",
       "      <th>PIR_4</th>\n",
       "      <th>PIR_5</th>\n",
       "      <th>PIR_6</th>\n",
       "      <th>PIR_7</th>\n",
       "      <th>PIR_8</th>\n",
       "      <th>PIR_9</th>\n",
       "      <th>PIR_10</th>\n",
       "      <th>PIR_11</th>\n",
       "      <th>PIR_12</th>\n",
       "      <th>PIR_13</th>\n",
       "      <th>PIR_14</th>\n",
       "      <th>PIR_15</th>\n",
       "      <th>PIR_16</th>\n",
       "      <th>PIR_17</th>\n",
       "      <th>PIR_18</th>\n",
       "      <th>PIR_19</th>\n",
       "      <th>PIR_20</th>\n",
       "      <th>PIR_21</th>\n",
       "      <th>PIR_22</th>\n",
       "      <th>PIR_23</th>\n",
       "      <th>PIR_24</th>\n",
       "      <th>PIR_25</th>\n",
       "      <th>PIR_26</th>\n",
       "      <th>PIR_27</th>\n",
       "      <th>PIR_28</th>\n",
       "      <th>PIR_29</th>\n",
       "      <th>PIR_30</th>\n",
       "      <th>PIR_31</th>\n",
       "      <th>PIR_32</th>\n",
       "      <th>PIR_33</th>\n",
       "      <th>PIR_34</th>\n",
       "      <th>PIR_35</th>\n",
       "      <th>PIR_36</th>\n",
       "      <th>PIR_37</th>\n",
       "      <th>PIR_38</th>\n",
       "      <th>PIR_39</th>\n",
       "      <th>PIR_40</th>\n",
       "      <th>PIR_41</th>\n",
       "      <th>PIR_42</th>\n",
       "      <th>PIR_43</th>\n",
       "      <th>PIR_44</th>\n",
       "      <th>PIR_45</th>\n",
       "      <th>PIR_46</th>\n",
       "      <th>PIR_47</th>\n",
       "      <th>PIR_48</th>\n",
       "      <th>PIR_49</th>\n",
       "      <th>PIR_50</th>\n",
       "      <th>PIR_51</th>\n",
       "      <th>PIR_52</th>\n",
       "      <th>PIR_53</th>\n",
       "      <th>PIR_54</th>\n",
       "      <th>PIR_55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:19:56</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10269</td>\n",
       "      <td>10721</td>\n",
       "      <td>11156</td>\n",
       "      <td>11170</td>\n",
       "      <td>10931</td>\n",
       "      <td>10671</td>\n",
       "      <td>10395</td>\n",
       "      <td>10133</td>\n",
       "      <td>9885</td>\n",
       "      <td>9705</td>\n",
       "      <td>9538</td>\n",
       "      <td>9418</td>\n",
       "      <td>9469</td>\n",
       "      <td>9599</td>\n",
       "      <td>9817</td>\n",
       "      <td>9910</td>\n",
       "      <td>9890</td>\n",
       "      <td>10075</td>\n",
       "      <td>10231</td>\n",
       "      <td>10247</td>\n",
       "      <td>10271</td>\n",
       "      <td>10229</td>\n",
       "      <td>10272</td>\n",
       "      <td>10354</td>\n",
       "      <td>10449</td>\n",
       "      <td>10451</td>\n",
       "      <td>10419</td>\n",
       "      <td>10409</td>\n",
       "      <td>10336</td>\n",
       "      <td>10306</td>\n",
       "      <td>10356</td>\n",
       "      <td>10461</td>\n",
       "      <td>10456</td>\n",
       "      <td>10460</td>\n",
       "      <td>10467</td>\n",
       "      <td>10422</td>\n",
       "      <td>10303</td>\n",
       "      <td>9877</td>\n",
       "      <td>9308</td>\n",
       "      <td>9061</td>\n",
       "      <td>9299</td>\n",
       "      <td>9748</td>\n",
       "      <td>10209</td>\n",
       "      <td>10615</td>\n",
       "      <td>10975</td>\n",
       "      <td>11178</td>\n",
       "      <td>11197</td>\n",
       "      <td>11161</td>\n",
       "      <td>11096</td>\n",
       "      <td>10957</td>\n",
       "      <td>10839</td>\n",
       "      <td>10735</td>\n",
       "      <td>10590</td>\n",
       "      <td>10411</td>\n",
       "      <td>10329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:20:12</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>10364</td>\n",
       "      <td>10907</td>\n",
       "      <td>11299</td>\n",
       "      <td>11238</td>\n",
       "      <td>10867</td>\n",
       "      <td>10535</td>\n",
       "      <td>10173</td>\n",
       "      <td>9950</td>\n",
       "      <td>9856</td>\n",
       "      <td>9795</td>\n",
       "      <td>9714</td>\n",
       "      <td>9702</td>\n",
       "      <td>9792</td>\n",
       "      <td>9789</td>\n",
       "      <td>9915</td>\n",
       "      <td>9900</td>\n",
       "      <td>9944</td>\n",
       "      <td>9964</td>\n",
       "      <td>9971</td>\n",
       "      <td>10059</td>\n",
       "      <td>10161</td>\n",
       "      <td>10234</td>\n",
       "      <td>10285</td>\n",
       "      <td>10309</td>\n",
       "      <td>10384</td>\n",
       "      <td>10464</td>\n",
       "      <td>10450</td>\n",
       "      <td>10427</td>\n",
       "      <td>10366</td>\n",
       "      <td>10361</td>\n",
       "      <td>10452</td>\n",
       "      <td>10502</td>\n",
       "      <td>10444</td>\n",
       "      <td>10337</td>\n",
       "      <td>10250</td>\n",
       "      <td>10313</td>\n",
       "      <td>10211</td>\n",
       "      <td>9718</td>\n",
       "      <td>9236</td>\n",
       "      <td>9193</td>\n",
       "      <td>9609</td>\n",
       "      <td>10022</td>\n",
       "      <td>10431</td>\n",
       "      <td>10798</td>\n",
       "      <td>11055</td>\n",
       "      <td>11122</td>\n",
       "      <td>11145</td>\n",
       "      <td>11136</td>\n",
       "      <td>11108</td>\n",
       "      <td>11041</td>\n",
       "      <td>10824</td>\n",
       "      <td>10645</td>\n",
       "      <td>10493</td>\n",
       "      <td>10398</td>\n",
       "      <td>10357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:20:28</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10329</td>\n",
       "      <td>10793</td>\n",
       "      <td>11197</td>\n",
       "      <td>11242</td>\n",
       "      <td>11052</td>\n",
       "      <td>10658</td>\n",
       "      <td>10288</td>\n",
       "      <td>9988</td>\n",
       "      <td>9819</td>\n",
       "      <td>9711</td>\n",
       "      <td>9659</td>\n",
       "      <td>9626</td>\n",
       "      <td>9726</td>\n",
       "      <td>9752</td>\n",
       "      <td>9835</td>\n",
       "      <td>9942</td>\n",
       "      <td>9925</td>\n",
       "      <td>9965</td>\n",
       "      <td>10110</td>\n",
       "      <td>10174</td>\n",
       "      <td>10140</td>\n",
       "      <td>10235</td>\n",
       "      <td>10303</td>\n",
       "      <td>10365</td>\n",
       "      <td>10366</td>\n",
       "      <td>10379</td>\n",
       "      <td>10375</td>\n",
       "      <td>10287</td>\n",
       "      <td>10310</td>\n",
       "      <td>10345</td>\n",
       "      <td>10373</td>\n",
       "      <td>10328</td>\n",
       "      <td>10387</td>\n",
       "      <td>10415</td>\n",
       "      <td>10491</td>\n",
       "      <td>10421</td>\n",
       "      <td>10432</td>\n",
       "      <td>9964</td>\n",
       "      <td>9368</td>\n",
       "      <td>9135</td>\n",
       "      <td>9287</td>\n",
       "      <td>9643</td>\n",
       "      <td>10184</td>\n",
       "      <td>10663</td>\n",
       "      <td>11016</td>\n",
       "      <td>11168</td>\n",
       "      <td>11204</td>\n",
       "      <td>11162</td>\n",
       "      <td>11109</td>\n",
       "      <td>11007</td>\n",
       "      <td>10867</td>\n",
       "      <td>10700</td>\n",
       "      <td>10533</td>\n",
       "      <td>10427</td>\n",
       "      <td>10265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:20:44</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10169</td>\n",
       "      <td>10425</td>\n",
       "      <td>10822</td>\n",
       "      <td>11133</td>\n",
       "      <td>11136</td>\n",
       "      <td>10834</td>\n",
       "      <td>10520</td>\n",
       "      <td>10228</td>\n",
       "      <td>9986</td>\n",
       "      <td>9848</td>\n",
       "      <td>9643</td>\n",
       "      <td>9562</td>\n",
       "      <td>9591</td>\n",
       "      <td>9618</td>\n",
       "      <td>9718</td>\n",
       "      <td>9849</td>\n",
       "      <td>9857</td>\n",
       "      <td>10026</td>\n",
       "      <td>10150</td>\n",
       "      <td>10198</td>\n",
       "      <td>10261</td>\n",
       "      <td>10351</td>\n",
       "      <td>10425</td>\n",
       "      <td>10469</td>\n",
       "      <td>10374</td>\n",
       "      <td>10344</td>\n",
       "      <td>10303</td>\n",
       "      <td>10293</td>\n",
       "      <td>10294</td>\n",
       "      <td>10333</td>\n",
       "      <td>10353</td>\n",
       "      <td>10345</td>\n",
       "      <td>10354</td>\n",
       "      <td>10362</td>\n",
       "      <td>10375</td>\n",
       "      <td>10369</td>\n",
       "      <td>10319</td>\n",
       "      <td>10115</td>\n",
       "      <td>9603</td>\n",
       "      <td>9182</td>\n",
       "      <td>9125</td>\n",
       "      <td>9560</td>\n",
       "      <td>10161</td>\n",
       "      <td>10560</td>\n",
       "      <td>10883</td>\n",
       "      <td>11116</td>\n",
       "      <td>11273</td>\n",
       "      <td>11186</td>\n",
       "      <td>10984</td>\n",
       "      <td>10910</td>\n",
       "      <td>10807</td>\n",
       "      <td>10714</td>\n",
       "      <td>10651</td>\n",
       "      <td>10562</td>\n",
       "      <td>10463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>19:21:00</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10320</td>\n",
       "      <td>10667</td>\n",
       "      <td>11104</td>\n",
       "      <td>11234</td>\n",
       "      <td>11129</td>\n",
       "      <td>10814</td>\n",
       "      <td>10453</td>\n",
       "      <td>10040</td>\n",
       "      <td>9733</td>\n",
       "      <td>9630</td>\n",
       "      <td>9578</td>\n",
       "      <td>9476</td>\n",
       "      <td>9596</td>\n",
       "      <td>9748</td>\n",
       "      <td>9755</td>\n",
       "      <td>9823</td>\n",
       "      <td>10004</td>\n",
       "      <td>10048</td>\n",
       "      <td>10202</td>\n",
       "      <td>10234</td>\n",
       "      <td>10255</td>\n",
       "      <td>10282</td>\n",
       "      <td>10298</td>\n",
       "      <td>10319</td>\n",
       "      <td>10315</td>\n",
       "      <td>10270</td>\n",
       "      <td>10334</td>\n",
       "      <td>10400</td>\n",
       "      <td>10428</td>\n",
       "      <td>10514</td>\n",
       "      <td>10529</td>\n",
       "      <td>10453</td>\n",
       "      <td>10374</td>\n",
       "      <td>10303</td>\n",
       "      <td>10298</td>\n",
       "      <td>10238</td>\n",
       "      <td>10246</td>\n",
       "      <td>9918</td>\n",
       "      <td>9399</td>\n",
       "      <td>9198</td>\n",
       "      <td>9422</td>\n",
       "      <td>9848</td>\n",
       "      <td>10225</td>\n",
       "      <td>10615</td>\n",
       "      <td>10860</td>\n",
       "      <td>11006</td>\n",
       "      <td>11257</td>\n",
       "      <td>11370</td>\n",
       "      <td>11173</td>\n",
       "      <td>10924</td>\n",
       "      <td>10816</td>\n",
       "      <td>10754</td>\n",
       "      <td>10588</td>\n",
       "      <td>10428</td>\n",
       "      <td>10407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time  Label  Temperature_F  PIR_1  PIR_2  PIR_3  PIR_4  \\\n",
       "0  2024-08-08  19:19:56      0             86  10269  10721  11156  11170   \n",
       "1  2024-08-08  19:20:12      1             86  10364  10907  11299  11238   \n",
       "2  2024-08-08  19:20:28      0             86  10329  10793  11197  11242   \n",
       "3  2024-08-08  19:20:44      0             86  10169  10425  10822  11133   \n",
       "4  2024-08-08  19:21:00      0             86  10320  10667  11104  11234   \n",
       "\n",
       "   PIR_5  PIR_6  PIR_7  PIR_8  PIR_9  PIR_10  PIR_11  PIR_12  PIR_13  PIR_14  \\\n",
       "0  10931  10671  10395  10133   9885    9705    9538    9418    9469    9599   \n",
       "1  10867  10535  10173   9950   9856    9795    9714    9702    9792    9789   \n",
       "2  11052  10658  10288   9988   9819    9711    9659    9626    9726    9752   \n",
       "3  11136  10834  10520  10228   9986    9848    9643    9562    9591    9618   \n",
       "4  11129  10814  10453  10040   9733    9630    9578    9476    9596    9748   \n",
       "\n",
       "   PIR_15  PIR_16  PIR_17  PIR_18  PIR_19  PIR_20  PIR_21  PIR_22  PIR_23  \\\n",
       "0    9817    9910    9890   10075   10231   10247   10271   10229   10272   \n",
       "1    9915    9900    9944    9964    9971   10059   10161   10234   10285   \n",
       "2    9835    9942    9925    9965   10110   10174   10140   10235   10303   \n",
       "3    9718    9849    9857   10026   10150   10198   10261   10351   10425   \n",
       "4    9755    9823   10004   10048   10202   10234   10255   10282   10298   \n",
       "\n",
       "   PIR_24  PIR_25  PIR_26  PIR_27  PIR_28  PIR_29  PIR_30  PIR_31  PIR_32  \\\n",
       "0   10354   10449   10451   10419   10409   10336   10306   10356   10461   \n",
       "1   10309   10384   10464   10450   10427   10366   10361   10452   10502   \n",
       "2   10365   10366   10379   10375   10287   10310   10345   10373   10328   \n",
       "3   10469   10374   10344   10303   10293   10294   10333   10353   10345   \n",
       "4   10319   10315   10270   10334   10400   10428   10514   10529   10453   \n",
       "\n",
       "   PIR_33  PIR_34  PIR_35  PIR_36  PIR_37  PIR_38  PIR_39  PIR_40  PIR_41  \\\n",
       "0   10456   10460   10467   10422   10303    9877    9308    9061    9299   \n",
       "1   10444   10337   10250   10313   10211    9718    9236    9193    9609   \n",
       "2   10387   10415   10491   10421   10432    9964    9368    9135    9287   \n",
       "3   10354   10362   10375   10369   10319   10115    9603    9182    9125   \n",
       "4   10374   10303   10298   10238   10246    9918    9399    9198    9422   \n",
       "\n",
       "   PIR_42  PIR_43  PIR_44  PIR_45  PIR_46  PIR_47  PIR_48  PIR_49  PIR_50  \\\n",
       "0    9748   10209   10615   10975   11178   11197   11161   11096   10957   \n",
       "1   10022   10431   10798   11055   11122   11145   11136   11108   11041   \n",
       "2    9643   10184   10663   11016   11168   11204   11162   11109   11007   \n",
       "3    9560   10161   10560   10883   11116   11273   11186   10984   10910   \n",
       "4    9848   10225   10615   10860   11006   11257   11370   11173   10924   \n",
       "\n",
       "   PIR_51  PIR_52  PIR_53  PIR_54  PIR_55  \n",
       "0   10839   10735   10590   10411   10329  \n",
       "1   10824   10645   10493   10398   10357  \n",
       "2   10867   10700   10533   10427   10265  \n",
       "3   10807   10714   10651   10562   10463  \n",
       "4   10816   10754   10588   10428   10407  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #We first shuffle these 2 dataframes\n",
    "# df1 = df1.sample(frac = 1, random_state=1).reset_index(drop=True)\n",
    "# df2 = df2.sample(frac = 1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "#We now merge these 2 dataframes\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "#We print the shapes of all datafmrames\n",
    "print(df1.shape, df2.shape, df.shape)\n",
    "\n",
    "#Displaying the merged dataframe\n",
    "display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date' and 'Time' are parsed correctly\n",
    "df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "\n",
    "# Extract temporal features\n",
    "df['Hour'] = df['Datetime'].dt.hour\n",
    "df['Minute'] = df['Datetime'].dt.minute\n",
    "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
    "df['Month'] = df['Datetime'].dt.month\n",
    "\n",
    "df = df.drop(columns=['Date', 'Time', 'Datetime'])\n",
    "\n",
    "meta_features = ['Hour', 'Minute', 'DayOfWeek', 'Month', 'Temperature_F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Temperature_F</th>\n",
       "      <th>PIR_1</th>\n",
       "      <th>PIR_2</th>\n",
       "      <th>PIR_3</th>\n",
       "      <th>PIR_4</th>\n",
       "      <th>PIR_5</th>\n",
       "      <th>PIR_6</th>\n",
       "      <th>PIR_7</th>\n",
       "      <th>PIR_8</th>\n",
       "      <th>PIR_9</th>\n",
       "      <th>PIR_10</th>\n",
       "      <th>PIR_11</th>\n",
       "      <th>PIR_12</th>\n",
       "      <th>PIR_13</th>\n",
       "      <th>PIR_14</th>\n",
       "      <th>PIR_15</th>\n",
       "      <th>PIR_16</th>\n",
       "      <th>PIR_17</th>\n",
       "      <th>PIR_18</th>\n",
       "      <th>PIR_19</th>\n",
       "      <th>PIR_20</th>\n",
       "      <th>PIR_21</th>\n",
       "      <th>PIR_22</th>\n",
       "      <th>PIR_23</th>\n",
       "      <th>PIR_24</th>\n",
       "      <th>PIR_25</th>\n",
       "      <th>PIR_26</th>\n",
       "      <th>PIR_27</th>\n",
       "      <th>PIR_28</th>\n",
       "      <th>PIR_29</th>\n",
       "      <th>PIR_30</th>\n",
       "      <th>PIR_31</th>\n",
       "      <th>PIR_32</th>\n",
       "      <th>PIR_33</th>\n",
       "      <th>PIR_34</th>\n",
       "      <th>PIR_35</th>\n",
       "      <th>PIR_36</th>\n",
       "      <th>PIR_37</th>\n",
       "      <th>PIR_38</th>\n",
       "      <th>PIR_39</th>\n",
       "      <th>PIR_40</th>\n",
       "      <th>PIR_41</th>\n",
       "      <th>PIR_42</th>\n",
       "      <th>PIR_43</th>\n",
       "      <th>PIR_44</th>\n",
       "      <th>PIR_45</th>\n",
       "      <th>PIR_46</th>\n",
       "      <th>PIR_47</th>\n",
       "      <th>PIR_48</th>\n",
       "      <th>PIR_49</th>\n",
       "      <th>PIR_50</th>\n",
       "      <th>PIR_51</th>\n",
       "      <th>PIR_52</th>\n",
       "      <th>PIR_53</th>\n",
       "      <th>PIR_54</th>\n",
       "      <th>PIR_55</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10269</td>\n",
       "      <td>10721</td>\n",
       "      <td>11156</td>\n",
       "      <td>11170</td>\n",
       "      <td>10931</td>\n",
       "      <td>10671</td>\n",
       "      <td>10395</td>\n",
       "      <td>10133</td>\n",
       "      <td>9885</td>\n",
       "      <td>9705</td>\n",
       "      <td>9538</td>\n",
       "      <td>9418</td>\n",
       "      <td>9469</td>\n",
       "      <td>9599</td>\n",
       "      <td>9817</td>\n",
       "      <td>9910</td>\n",
       "      <td>9890</td>\n",
       "      <td>10075</td>\n",
       "      <td>10231</td>\n",
       "      <td>10247</td>\n",
       "      <td>10271</td>\n",
       "      <td>10229</td>\n",
       "      <td>10272</td>\n",
       "      <td>10354</td>\n",
       "      <td>10449</td>\n",
       "      <td>10451</td>\n",
       "      <td>10419</td>\n",
       "      <td>10409</td>\n",
       "      <td>10336</td>\n",
       "      <td>10306</td>\n",
       "      <td>10356</td>\n",
       "      <td>10461</td>\n",
       "      <td>10456</td>\n",
       "      <td>10460</td>\n",
       "      <td>10467</td>\n",
       "      <td>10422</td>\n",
       "      <td>10303</td>\n",
       "      <td>9877</td>\n",
       "      <td>9308</td>\n",
       "      <td>9061</td>\n",
       "      <td>9299</td>\n",
       "      <td>9748</td>\n",
       "      <td>10209</td>\n",
       "      <td>10615</td>\n",
       "      <td>10975</td>\n",
       "      <td>11178</td>\n",
       "      <td>11197</td>\n",
       "      <td>11161</td>\n",
       "      <td>11096</td>\n",
       "      <td>10957</td>\n",
       "      <td>10839</td>\n",
       "      <td>10735</td>\n",
       "      <td>10590</td>\n",
       "      <td>10411</td>\n",
       "      <td>10329</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>10364</td>\n",
       "      <td>10907</td>\n",
       "      <td>11299</td>\n",
       "      <td>11238</td>\n",
       "      <td>10867</td>\n",
       "      <td>10535</td>\n",
       "      <td>10173</td>\n",
       "      <td>9950</td>\n",
       "      <td>9856</td>\n",
       "      <td>9795</td>\n",
       "      <td>9714</td>\n",
       "      <td>9702</td>\n",
       "      <td>9792</td>\n",
       "      <td>9789</td>\n",
       "      <td>9915</td>\n",
       "      <td>9900</td>\n",
       "      <td>9944</td>\n",
       "      <td>9964</td>\n",
       "      <td>9971</td>\n",
       "      <td>10059</td>\n",
       "      <td>10161</td>\n",
       "      <td>10234</td>\n",
       "      <td>10285</td>\n",
       "      <td>10309</td>\n",
       "      <td>10384</td>\n",
       "      <td>10464</td>\n",
       "      <td>10450</td>\n",
       "      <td>10427</td>\n",
       "      <td>10366</td>\n",
       "      <td>10361</td>\n",
       "      <td>10452</td>\n",
       "      <td>10502</td>\n",
       "      <td>10444</td>\n",
       "      <td>10337</td>\n",
       "      <td>10250</td>\n",
       "      <td>10313</td>\n",
       "      <td>10211</td>\n",
       "      <td>9718</td>\n",
       "      <td>9236</td>\n",
       "      <td>9193</td>\n",
       "      <td>9609</td>\n",
       "      <td>10022</td>\n",
       "      <td>10431</td>\n",
       "      <td>10798</td>\n",
       "      <td>11055</td>\n",
       "      <td>11122</td>\n",
       "      <td>11145</td>\n",
       "      <td>11136</td>\n",
       "      <td>11108</td>\n",
       "      <td>11041</td>\n",
       "      <td>10824</td>\n",
       "      <td>10645</td>\n",
       "      <td>10493</td>\n",
       "      <td>10398</td>\n",
       "      <td>10357</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10329</td>\n",
       "      <td>10793</td>\n",
       "      <td>11197</td>\n",
       "      <td>11242</td>\n",
       "      <td>11052</td>\n",
       "      <td>10658</td>\n",
       "      <td>10288</td>\n",
       "      <td>9988</td>\n",
       "      <td>9819</td>\n",
       "      <td>9711</td>\n",
       "      <td>9659</td>\n",
       "      <td>9626</td>\n",
       "      <td>9726</td>\n",
       "      <td>9752</td>\n",
       "      <td>9835</td>\n",
       "      <td>9942</td>\n",
       "      <td>9925</td>\n",
       "      <td>9965</td>\n",
       "      <td>10110</td>\n",
       "      <td>10174</td>\n",
       "      <td>10140</td>\n",
       "      <td>10235</td>\n",
       "      <td>10303</td>\n",
       "      <td>10365</td>\n",
       "      <td>10366</td>\n",
       "      <td>10379</td>\n",
       "      <td>10375</td>\n",
       "      <td>10287</td>\n",
       "      <td>10310</td>\n",
       "      <td>10345</td>\n",
       "      <td>10373</td>\n",
       "      <td>10328</td>\n",
       "      <td>10387</td>\n",
       "      <td>10415</td>\n",
       "      <td>10491</td>\n",
       "      <td>10421</td>\n",
       "      <td>10432</td>\n",
       "      <td>9964</td>\n",
       "      <td>9368</td>\n",
       "      <td>9135</td>\n",
       "      <td>9287</td>\n",
       "      <td>9643</td>\n",
       "      <td>10184</td>\n",
       "      <td>10663</td>\n",
       "      <td>11016</td>\n",
       "      <td>11168</td>\n",
       "      <td>11204</td>\n",
       "      <td>11162</td>\n",
       "      <td>11109</td>\n",
       "      <td>11007</td>\n",
       "      <td>10867</td>\n",
       "      <td>10700</td>\n",
       "      <td>10533</td>\n",
       "      <td>10427</td>\n",
       "      <td>10265</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10169</td>\n",
       "      <td>10425</td>\n",
       "      <td>10822</td>\n",
       "      <td>11133</td>\n",
       "      <td>11136</td>\n",
       "      <td>10834</td>\n",
       "      <td>10520</td>\n",
       "      <td>10228</td>\n",
       "      <td>9986</td>\n",
       "      <td>9848</td>\n",
       "      <td>9643</td>\n",
       "      <td>9562</td>\n",
       "      <td>9591</td>\n",
       "      <td>9618</td>\n",
       "      <td>9718</td>\n",
       "      <td>9849</td>\n",
       "      <td>9857</td>\n",
       "      <td>10026</td>\n",
       "      <td>10150</td>\n",
       "      <td>10198</td>\n",
       "      <td>10261</td>\n",
       "      <td>10351</td>\n",
       "      <td>10425</td>\n",
       "      <td>10469</td>\n",
       "      <td>10374</td>\n",
       "      <td>10344</td>\n",
       "      <td>10303</td>\n",
       "      <td>10293</td>\n",
       "      <td>10294</td>\n",
       "      <td>10333</td>\n",
       "      <td>10353</td>\n",
       "      <td>10345</td>\n",
       "      <td>10354</td>\n",
       "      <td>10362</td>\n",
       "      <td>10375</td>\n",
       "      <td>10369</td>\n",
       "      <td>10319</td>\n",
       "      <td>10115</td>\n",
       "      <td>9603</td>\n",
       "      <td>9182</td>\n",
       "      <td>9125</td>\n",
       "      <td>9560</td>\n",
       "      <td>10161</td>\n",
       "      <td>10560</td>\n",
       "      <td>10883</td>\n",
       "      <td>11116</td>\n",
       "      <td>11273</td>\n",
       "      <td>11186</td>\n",
       "      <td>10984</td>\n",
       "      <td>10910</td>\n",
       "      <td>10807</td>\n",
       "      <td>10714</td>\n",
       "      <td>10651</td>\n",
       "      <td>10562</td>\n",
       "      <td>10463</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>10320</td>\n",
       "      <td>10667</td>\n",
       "      <td>11104</td>\n",
       "      <td>11234</td>\n",
       "      <td>11129</td>\n",
       "      <td>10814</td>\n",
       "      <td>10453</td>\n",
       "      <td>10040</td>\n",
       "      <td>9733</td>\n",
       "      <td>9630</td>\n",
       "      <td>9578</td>\n",
       "      <td>9476</td>\n",
       "      <td>9596</td>\n",
       "      <td>9748</td>\n",
       "      <td>9755</td>\n",
       "      <td>9823</td>\n",
       "      <td>10004</td>\n",
       "      <td>10048</td>\n",
       "      <td>10202</td>\n",
       "      <td>10234</td>\n",
       "      <td>10255</td>\n",
       "      <td>10282</td>\n",
       "      <td>10298</td>\n",
       "      <td>10319</td>\n",
       "      <td>10315</td>\n",
       "      <td>10270</td>\n",
       "      <td>10334</td>\n",
       "      <td>10400</td>\n",
       "      <td>10428</td>\n",
       "      <td>10514</td>\n",
       "      <td>10529</td>\n",
       "      <td>10453</td>\n",
       "      <td>10374</td>\n",
       "      <td>10303</td>\n",
       "      <td>10298</td>\n",
       "      <td>10238</td>\n",
       "      <td>10246</td>\n",
       "      <td>9918</td>\n",
       "      <td>9399</td>\n",
       "      <td>9198</td>\n",
       "      <td>9422</td>\n",
       "      <td>9848</td>\n",
       "      <td>10225</td>\n",
       "      <td>10615</td>\n",
       "      <td>10860</td>\n",
       "      <td>11006</td>\n",
       "      <td>11257</td>\n",
       "      <td>11370</td>\n",
       "      <td>11173</td>\n",
       "      <td>10924</td>\n",
       "      <td>10816</td>\n",
       "      <td>10754</td>\n",
       "      <td>10588</td>\n",
       "      <td>10428</td>\n",
       "      <td>10407</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Temperature_F  PIR_1  PIR_2  PIR_3  PIR_4  PIR_5  PIR_6  PIR_7  \\\n",
       "0      0             86  10269  10721  11156  11170  10931  10671  10395   \n",
       "1      1             86  10364  10907  11299  11238  10867  10535  10173   \n",
       "2      0             86  10329  10793  11197  11242  11052  10658  10288   \n",
       "3      0             86  10169  10425  10822  11133  11136  10834  10520   \n",
       "4      0             86  10320  10667  11104  11234  11129  10814  10453   \n",
       "\n",
       "   PIR_8  PIR_9  PIR_10  PIR_11  PIR_12  PIR_13  PIR_14  PIR_15  PIR_16  \\\n",
       "0  10133   9885    9705    9538    9418    9469    9599    9817    9910   \n",
       "1   9950   9856    9795    9714    9702    9792    9789    9915    9900   \n",
       "2   9988   9819    9711    9659    9626    9726    9752    9835    9942   \n",
       "3  10228   9986    9848    9643    9562    9591    9618    9718    9849   \n",
       "4  10040   9733    9630    9578    9476    9596    9748    9755    9823   \n",
       "\n",
       "   PIR_17  PIR_18  PIR_19  PIR_20  PIR_21  PIR_22  PIR_23  PIR_24  PIR_25  \\\n",
       "0    9890   10075   10231   10247   10271   10229   10272   10354   10449   \n",
       "1    9944    9964    9971   10059   10161   10234   10285   10309   10384   \n",
       "2    9925    9965   10110   10174   10140   10235   10303   10365   10366   \n",
       "3    9857   10026   10150   10198   10261   10351   10425   10469   10374   \n",
       "4   10004   10048   10202   10234   10255   10282   10298   10319   10315   \n",
       "\n",
       "   PIR_26  PIR_27  PIR_28  PIR_29  PIR_30  PIR_31  PIR_32  PIR_33  PIR_34  \\\n",
       "0   10451   10419   10409   10336   10306   10356   10461   10456   10460   \n",
       "1   10464   10450   10427   10366   10361   10452   10502   10444   10337   \n",
       "2   10379   10375   10287   10310   10345   10373   10328   10387   10415   \n",
       "3   10344   10303   10293   10294   10333   10353   10345   10354   10362   \n",
       "4   10270   10334   10400   10428   10514   10529   10453   10374   10303   \n",
       "\n",
       "   PIR_35  PIR_36  PIR_37  PIR_38  PIR_39  PIR_40  PIR_41  PIR_42  PIR_43  \\\n",
       "0   10467   10422   10303    9877    9308    9061    9299    9748   10209   \n",
       "1   10250   10313   10211    9718    9236    9193    9609   10022   10431   \n",
       "2   10491   10421   10432    9964    9368    9135    9287    9643   10184   \n",
       "3   10375   10369   10319   10115    9603    9182    9125    9560   10161   \n",
       "4   10298   10238   10246    9918    9399    9198    9422    9848   10225   \n",
       "\n",
       "   PIR_44  PIR_45  PIR_46  PIR_47  PIR_48  PIR_49  PIR_50  PIR_51  PIR_52  \\\n",
       "0   10615   10975   11178   11197   11161   11096   10957   10839   10735   \n",
       "1   10798   11055   11122   11145   11136   11108   11041   10824   10645   \n",
       "2   10663   11016   11168   11204   11162   11109   11007   10867   10700   \n",
       "3   10560   10883   11116   11273   11186   10984   10910   10807   10714   \n",
       "4   10615   10860   11006   11257   11370   11173   10924   10816   10754   \n",
       "\n",
       "   PIR_53  PIR_54  PIR_55  Hour  Minute  DayOfWeek  Month  \n",
       "0   10590   10411   10329    19      19          3      8  \n",
       "1   10493   10398   10357    19      20          3      8  \n",
       "2   10533   10427   10265    19      20          3      8  \n",
       "3   10651   10562   10463    19      20          3      8  \n",
       "4   10588   10428   10407    19      21          3      8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((df.shape[0], 2+55+54+4))\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "  for j in range (0, 2):\n",
    "    x[i,j] = df.iloc[i,j]\n",
    "  for j in range(2, 55+54+2):\n",
    "    if(j%2 == 0):\n",
    "      x[i,j] = df.iloc[i,j//2 + 1]\n",
    "    else:\n",
    "      x[i,j] = (df.iloc[i,j//2 + 1] + df.iloc[i,j//2 + 2])/2\n",
    "\n",
    "  for j in range(55+54+2, 55+54+2+4):\n",
    "    x[i,j] = df.iloc[i,j-54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame(x)\n",
    "x_df.columns = ['Label', 'Temperature_F'] + [f'PIR_{i}' for i in range(1, 110)] + ['Hour', 'Minute', 'DayOfWeek', 'Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIR_columns = [f'PIR_{i}' for i in range(1, 110)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class HybridTransformer(nn.Module):\n",
    "    def __init__(self, input_size=1, meta_input_size=5, seq_len=109, d_model=64, nhead=8, num_layers=9, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, d_model)  # PIR projection\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=128, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.meta_mlp = nn.Sequential(\n",
    "            nn.Linear(meta_input_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, d_model),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, 64),  # Transformer + Metadata\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq, x_meta):\n",
    "        # x_seq: (batch_size, seq_len, 1)\n",
    "        x_seq = self.input_proj(x_seq)         # (B, seq_len, d_model)\n",
    "        x_seq = self.pos_encoder(x_seq)        # Add positional encoding\n",
    "        x_seq = self.transformer_encoder(x_seq)  # (B, seq_len, d_model)\n",
    "        x_seq = x_seq.mean(dim=1)              # Mean pooling over time\n",
    "\n",
    "        # x_meta: (batch_size, meta_input_size)\n",
    "        x_meta = self.meta_mlp(x_meta)         # → (B, d_model)\n",
    "\n",
    "        combined = torch.cat([x_seq, x_meta], dim=1)  # (B, 2*d_model)\n",
    "        return self.classifier(combined)              # (B, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in df:\n",
      "Label\n",
      "0.0    12494\n",
      "1.0     1666\n",
      "3.0     1142\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution in df:\")\n",
    "print(df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Train label distribution: [10041 10041 10041]\n",
      "Test label distribution: [2453  345  263]\n",
      "epoch 1, loss: 0.1315\n",
      "epoch 2, loss: 0.0284\n",
      "epoch 3, loss: 0.0303\n",
      "epoch 4, loss: 0.0169\n",
      "epoch 5, loss: 0.0211\n",
      "epoch 6, loss: 0.0144\n",
      "epoch 7, loss: 0.0135\n",
      "epoch 8, loss: 0.0170\n",
      "epoch 9, loss: 0.0115\n",
      "epoch 10, loss: 0.0091\n",
      "epoch 11, loss: 0.0115\n",
      "epoch 12, loss: 0.0187\n",
      "epoch 13, loss: 0.0086\n",
      "epoch 14, loss: 0.0078\n",
      "epoch 15, loss: 0.0083\n",
      "epoch 16, loss: 0.0107\n",
      "epoch 17, loss: 0.0435\n",
      "epoch 18, loss: 0.0197\n",
      "epoch 19, loss: 0.0281\n",
      "epoch 20, loss: 0.0154\n",
      "\n",
      "fold 1 test accuracy: 0.9925\n",
      "\n",
      "Per-class accuracy:\n",
      "  Class 0: 2438/2453 correct (99.39%)\n",
      "  Class 1: 337/345 correct (97.68%)\n",
      "  Class 2: 263/263 correct (100.00%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9967    0.9939    0.9953      2453\n",
      "           1     0.9574    0.9768    0.9670       345\n",
      "           2     1.0000    1.0000    1.0000       263\n",
      "\n",
      "    accuracy                         0.9925      3061\n",
      "   macro avg     0.9847    0.9902    0.9874      3061\n",
      "weighted avg     0.9926    0.9925    0.9925      3061\n",
      "\n",
      "Macro F1-Score: 0.9874\n",
      "\n",
      "Average accuracy until fold 1 is : 0.9925\n",
      "Average F1-Score until fold 1 is : 0.9874\n",
      "\n",
      "Fold 2\n",
      "Train label distribution: [9991 9991 9991]\n",
      "Test label distribution: [2503  336  222]\n",
      "epoch 1, loss: 0.1178\n",
      "epoch 2, loss: 0.0248\n",
      "epoch 3, loss: 0.0384\n",
      "epoch 4, loss: 0.0237\n",
      "epoch 5, loss: 0.0180\n",
      "epoch 6, loss: 0.0184\n",
      "epoch 7, loss: 0.0310\n",
      "epoch 8, loss: 0.1317\n",
      "epoch 9, loss: 0.0999\n",
      "epoch 10, loss: 0.0762\n",
      "epoch 11, loss: 0.0609\n",
      "epoch 12, loss: 0.0520\n",
      "epoch 13, loss: 0.0477\n",
      "epoch 14, loss: 0.0448\n",
      "epoch 15, loss: 0.0407\n",
      "epoch 16, loss: 0.0383\n",
      "epoch 17, loss: 0.0387\n",
      "epoch 18, loss: 0.0361\n",
      "epoch 19, loss: 0.0358\n",
      "epoch 20, loss: 0.0352\n",
      "\n",
      "fold 2 test accuracy: 0.9899\n",
      "\n",
      "Per-class accuracy:\n",
      "  Class 0: 2492/2503 correct (99.56%)\n",
      "  Class 1: 316/336 correct (94.05%)\n",
      "  Class 2: 222/222 correct (100.00%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9920    0.9956    0.9938      2503\n",
      "           1     0.9664    0.9405    0.9532       336\n",
      "           2     1.0000    1.0000    1.0000       222\n",
      "\n",
      "    accuracy                         0.9899      3061\n",
      "   macro avg     0.9861    0.9787    0.9824      3061\n",
      "weighted avg     0.9898    0.9899    0.9898      3061\n",
      "\n",
      "Macro F1-Score: 0.9824\n",
      "\n",
      "Average accuracy until fold 2 is : 0.9912\n",
      "Average F1-Score until fold 2 is : 0.9849\n",
      "\n",
      "Fold 3\n",
      "Train label distribution: [9989 9989 9989]\n",
      "Test label distribution: [2505  320  235]\n",
      "epoch 1, loss: 0.1024\n",
      "epoch 2, loss: 0.0352\n",
      "epoch 3, loss: 0.0268\n",
      "epoch 4, loss: 0.0173\n",
      "epoch 5, loss: 0.0252\n",
      "epoch 6, loss: 0.1395\n",
      "epoch 7, loss: 0.1031\n",
      "epoch 8, loss: 0.0746\n",
      "epoch 9, loss: 0.0603\n",
      "epoch 10, loss: 0.0542\n",
      "epoch 11, loss: 0.0509\n",
      "epoch 12, loss: 0.0470\n",
      "epoch 13, loss: 0.0455\n",
      "epoch 14, loss: 0.0444\n",
      "epoch 15, loss: 0.0426\n",
      "epoch 16, loss: 0.0413\n",
      "epoch 17, loss: 0.0426\n",
      "epoch 18, loss: 0.0409\n",
      "epoch 19, loss: 0.0404\n",
      "epoch 20, loss: 0.0402\n",
      "\n",
      "fold 3 test accuracy: 0.9775\n",
      "\n",
      "Per-class accuracy:\n",
      "  Class 0: 2437/2505 correct (97.29%)\n",
      "  Class 1: 319/320 correct (99.69%)\n",
      "  Class 2: 235/235 correct (100.00%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9996    0.9729    0.9860      2505\n",
      "           1     0.8243    0.9969    0.9024       320\n",
      "           2     1.0000    1.0000    1.0000       235\n",
      "\n",
      "    accuracy                         0.9775      3060\n",
      "   macro avg     0.9413    0.9899    0.9628      3060\n",
      "weighted avg     0.9813    0.9775    0.9784      3060\n",
      "\n",
      "Macro F1-Score: 0.9628\n",
      "\n",
      "Average accuracy until fold 3 is : 0.9866\n",
      "Average F1-Score until fold 3 is : 0.9775\n",
      "\n",
      "Fold 4\n",
      "Train label distribution: [9988 9988 9988]\n",
      "Test label distribution: [2506  349  205]\n",
      "epoch 1, loss: 0.1125\n",
      "epoch 2, loss: 0.0258\n",
      "epoch 3, loss: 0.0247\n",
      "epoch 4, loss: 0.0436\n",
      "epoch 5, loss: 0.1103\n",
      "epoch 6, loss: 0.1172\n",
      "epoch 7, loss: 0.0830\n",
      "epoch 8, loss: 0.0666\n",
      "epoch 9, loss: 0.0570\n",
      "epoch 10, loss: 0.0518\n",
      "epoch 11, loss: 0.0478\n",
      "epoch 12, loss: 0.0458\n",
      "epoch 13, loss: 0.0457\n",
      "epoch 14, loss: 0.0428\n",
      "epoch 15, loss: 0.0419\n",
      "epoch 16, loss: 0.0410\n",
      "epoch 17, loss: 0.0404\n",
      "epoch 18, loss: 0.0391\n",
      "epoch 19, loss: 0.0396\n",
      "epoch 20, loss: 0.0386\n",
      "\n",
      "fold 4 test accuracy: 0.9758\n",
      "\n",
      "Per-class accuracy:\n",
      "  Class 0: 2436/2506 correct (97.21%)\n",
      "  Class 1: 345/349 correct (98.85%)\n",
      "  Class 2: 205/205 correct (100.00%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9984    0.9721    0.9850      2506\n",
      "           1     0.8313    0.9885    0.9031       349\n",
      "           2     1.0000    1.0000    1.0000       205\n",
      "\n",
      "    accuracy                         0.9758      3060\n",
      "   macro avg     0.9432    0.9869    0.9627      3060\n",
      "weighted avg     0.9794    0.9758    0.9767      3060\n",
      "\n",
      "Macro F1-Score: 0.9627\n",
      "\n",
      "Average accuracy until fold 4 is : 0.9839\n",
      "Average F1-Score until fold 4 is : 0.9738\n",
      "\n",
      "Fold 5\n",
      "Train label distribution: [9967 9967 9967]\n",
      "Test label distribution: [2527  316  217]\n",
      "epoch 1, loss: 0.1273\n",
      "epoch 2, loss: 0.0202\n",
      "epoch 3, loss: 0.0650\n",
      "epoch 4, loss: 0.1358\n",
      "epoch 5, loss: 0.1061\n",
      "epoch 6, loss: 0.0807\n",
      "epoch 7, loss: 0.0656\n",
      "epoch 8, loss: 0.0578\n",
      "epoch 9, loss: 0.0539\n",
      "epoch 10, loss: 0.0496\n",
      "epoch 11, loss: 0.0453\n",
      "epoch 12, loss: 0.0421\n",
      "epoch 13, loss: 0.0407\n",
      "epoch 14, loss: 0.0403\n",
      "epoch 15, loss: 0.0382\n",
      "epoch 16, loss: 0.0371\n",
      "epoch 17, loss: 0.0373\n",
      "epoch 18, loss: 0.0354\n",
      "epoch 19, loss: 0.0361\n",
      "epoch 20, loss: 0.0367\n",
      "\n",
      "fold 5 test accuracy: 0.9722\n",
      "\n",
      "Per-class accuracy:\n",
      "  Class 0: 2450/2527 correct (96.95%)\n",
      "  Class 1: 308/316 correct (97.47%)\n",
      "  Class 2: 217/217 correct (100.00%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9967    0.9695    0.9829      2527\n",
      "           1     0.8000    0.9747    0.8787       316\n",
      "           2     1.0000    1.0000    1.0000       217\n",
      "\n",
      "    accuracy                         0.9722      3060\n",
      "   macro avg     0.9322    0.9814    0.9539      3060\n",
      "weighted avg     0.9767    0.9722    0.9734      3060\n",
      "\n",
      "Macro F1-Score: 0.9539\n",
      "\n",
      "Average accuracy until fold 5 is : 0.9816\n",
      "Average F1-Score until fold 5 is : 0.9698\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import os\n",
    "\n",
    "class HybridTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_seq, X_meta, y):\n",
    "        self.X_seq = X_seq\n",
    "        self.X_meta = X_meta\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_seq)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.X_seq[idx]                # shape: (55,)\n",
    "        meta = self.X_meta[idx]                   # shape: (9,)\n",
    "        label = self.y[idx]\n",
    "\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.float32).unsqueeze(1)  # (55, 1)\n",
    "        meta_tensor = torch.tensor(meta, dtype=torch.float32)                       # (9,)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return sequence_tensor, meta_tensor, label_tensor\n",
    "\n",
    "pir_columns = [f'PIR_{i}' for i in range(1, 110)]\n",
    "X_seq = df[pir_columns].values\n",
    "\n",
    "# Metadata features\n",
    "X_meta = df[meta_features].values\n",
    "\n",
    "# Labels (remapped as before)\n",
    "label_map = {0: 0, 1: 1, 3: 2}\n",
    "y_raw = df[\"Label\"].values\n",
    "y = np.array([label_map[label] for label in y_raw])\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "input_size = 1\n",
    "\n",
    "# Calculate class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# classes = np.unique(y)\n",
    "# class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y)\n",
    "# class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# # Use in CrossEntropyLoss\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "nb_epochs = 20\n",
    "batch_size = 64  # you can adjust this based on memory\n",
    "accs = []\n",
    "f1_scores = []\n",
    "\n",
    "for fold, (train_i, test_i) in enumerate(kf.split(X_seq), 1):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "\n",
    "    model = HybridTransformer(input_size=input_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    X_seq_train, X_seq_test = X_seq[train_i], X_seq[test_i]\n",
    "    X_meta_train, X_meta_test = X_meta[train_i], X_meta[test_i]\n",
    "    y_train, y_test = y[train_i], y[test_i]\n",
    "\n",
    "    X_seq_train = pd.DataFrame(X_seq_train, columns=PIR_columns)\n",
    "    X_meta_train = pd.DataFrame(X_meta_train, columns=meta_features)\n",
    "\n",
    "    X_dummy = pd.concat([X_seq_train, X_meta_train], axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "    #Random Oversampler\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    X_dummy, y_train = ros.fit_resample(X_dummy, y_train)\n",
    "\n",
    "    # #SMOTE\n",
    "    # # smote = SMOTE(random_state=42)\n",
    "    # # X_dummy, y_train = smote.fit_resample(X_dummy, y_train)\n",
    "\n",
    "    X_seq_train = X_dummy[PIR_columns].values\n",
    "    X_meta_train = X_dummy[meta_features].values\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Initialize scalers\n",
    "    seq_scaler = StandardScaler()\n",
    "    meta_scaler = StandardScaler()\n",
    "\n",
    "    # Fit and transform\n",
    "    X_seq_train_norm = seq_scaler.fit_transform(X_seq_train)\n",
    "    X_meta_train_norm = meta_scaler.fit_transform(X_meta_train)\n",
    "    # Transform test data using the same scalers (don't fit again!)\n",
    "    X_seq_test_norm = seq_scaler.transform(X_seq_test)\n",
    "    X_meta_test_norm = meta_scaler.transform(X_meta_test)\n",
    "\n",
    "\n",
    "    print(\"Train label distribution:\", np.bincount(y_train))\n",
    "    print(\"Test label distribution:\", np.bincount(y_test))\n",
    "\n",
    "    train_dataset = HybridTimeSeriesDataset(X_seq_train_norm, X_meta_train_norm, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = HybridTimeSeriesDataset(X_seq_test_norm, X_meta_test_norm, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for sequences, metas, labels in train_loader:\n",
    "            output = model(sequences, metas)  # Forward pass\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * sequences.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataset)\n",
    "        print(f\"epoch {epoch+1}, loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_counts = {0: 0, 1: 0, 2: 0}\n",
    "    class_correct = {0: 0, 1: 0, 2: 0}\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, metas, labels in test_loader:\n",
    "            outputs = model(sequences, metas)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(preds.tolist())\n",
    "\n",
    "            for label, pred in zip(labels.tolist(), preds.tolist()):\n",
    "                class_counts[label] += 1\n",
    "                if pred == label:\n",
    "                    correct += 1\n",
    "                    class_correct[label] += 1\n",
    "                total += 1\n",
    "\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"\\nfold {fold} test accuracy: {accuracy:.4f}\")\n",
    "    accs.append(accuracy)\n",
    "\n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for cls in class_counts:\n",
    "        total_cls = class_counts[cls]\n",
    "        correct_cls = class_correct[cls]\n",
    "        acc_cls = correct_cls / total_cls if total_cls > 0 else 0.0\n",
    "        print(f\"  Class {cls}: {correct_cls}/{total_cls} correct ({acc_cls * 100:.2f}%)\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_scores.append(macro_f1)\n",
    "    print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "\n",
    "    avg_acc = np.mean(accs)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "    print(f\"\\nAverage accuracy until fold {fold} is : {avg_acc:.4f}\")\n",
    "    print(f\"Average F1-Score until fold {fold} is : {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignmentenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
